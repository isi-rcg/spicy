
struct pyopencv_AKAZE_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_AKAZE_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".AKAZE",
    sizeof(pyopencv_AKAZE_t),
};

static void pyopencv_AKAZE_dealloc(PyObject* self)
{
    ((pyopencv_AKAZE_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::AKAZE>& r)
{
    pyopencv_AKAZE_t *m = PyObject_NEW(pyopencv_AKAZE_t, &pyopencv_AKAZE_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::AKAZE>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_AKAZE_Type))
    {
        failmsg("Expected cv::AKAZE for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_AKAZE_t*)src)->v.dynamicCast<cv::AKAZE>();
    return true;
}


struct pyopencv_AffineTransformer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_AffineTransformer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".AffineTransformer",
    sizeof(pyopencv_AffineTransformer_t),
};

static void pyopencv_AffineTransformer_dealloc(PyObject* self)
{
    ((pyopencv_AffineTransformer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::AffineTransformer>& r)
{
    pyopencv_AffineTransformer_t *m = PyObject_NEW(pyopencv_AffineTransformer_t, &pyopencv_AffineTransformer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::AffineTransformer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_AffineTransformer_Type))
    {
        failmsg("Expected cv::AffineTransformer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_AffineTransformer_t*)src)->v.dynamicCast<cv::AffineTransformer>();
    return true;
}


struct pyopencv_AgastFeatureDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_AgastFeatureDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".AgastFeatureDetector",
    sizeof(pyopencv_AgastFeatureDetector_t),
};

static void pyopencv_AgastFeatureDetector_dealloc(PyObject* self)
{
    ((pyopencv_AgastFeatureDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::AgastFeatureDetector>& r)
{
    pyopencv_AgastFeatureDetector_t *m = PyObject_NEW(pyopencv_AgastFeatureDetector_t, &pyopencv_AgastFeatureDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::AgastFeatureDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_AgastFeatureDetector_Type))
    {
        failmsg("Expected cv::AgastFeatureDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_AgastFeatureDetector_t*)src)->v.dynamicCast<cv::AgastFeatureDetector>();
    return true;
}


struct pyopencv_Algorithm_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_Algorithm_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".Algorithm",
    sizeof(pyopencv_Algorithm_t),
};

static void pyopencv_Algorithm_dealloc(PyObject* self)
{
    ((pyopencv_Algorithm_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::Algorithm>& r)
{
    pyopencv_Algorithm_t *m = PyObject_NEW(pyopencv_Algorithm_t, &pyopencv_Algorithm_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::Algorithm>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_Algorithm_Type))
    {
        failmsg("Expected cv::Algorithm for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_Algorithm_t*)src)->v.dynamicCast<cv::Algorithm>();
    return true;
}


struct pyopencv_AlignExposures_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_AlignExposures_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".AlignExposures",
    sizeof(pyopencv_AlignExposures_t),
};

static void pyopencv_AlignExposures_dealloc(PyObject* self)
{
    ((pyopencv_AlignExposures_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::AlignExposures>& r)
{
    pyopencv_AlignExposures_t *m = PyObject_NEW(pyopencv_AlignExposures_t, &pyopencv_AlignExposures_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::AlignExposures>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_AlignExposures_Type))
    {
        failmsg("Expected cv::AlignExposures for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_AlignExposures_t*)src)->v.dynamicCast<cv::AlignExposures>();
    return true;
}


struct pyopencv_AlignMTB_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_AlignMTB_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".AlignMTB",
    sizeof(pyopencv_AlignMTB_t),
};

static void pyopencv_AlignMTB_dealloc(PyObject* self)
{
    ((pyopencv_AlignMTB_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::AlignMTB>& r)
{
    pyopencv_AlignMTB_t *m = PyObject_NEW(pyopencv_AlignMTB_t, &pyopencv_AlignMTB_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::AlignMTB>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_AlignMTB_Type))
    {
        failmsg("Expected cv::AlignMTB for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_AlignMTB_t*)src)->v.dynamicCast<cv::AlignMTB>();
    return true;
}


struct pyopencv_BFMatcher_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_BFMatcher_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BFMatcher",
    sizeof(pyopencv_BFMatcher_t),
};

static void pyopencv_BFMatcher_dealloc(PyObject* self)
{
    ((pyopencv_BFMatcher_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BFMatcher>& r)
{
    pyopencv_BFMatcher_t *m = PyObject_NEW(pyopencv_BFMatcher_t, &pyopencv_BFMatcher_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BFMatcher>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BFMatcher_Type))
    {
        failmsg("Expected cv::BFMatcher for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BFMatcher_t*)src)->v.dynamicCast<cv::BFMatcher>();
    return true;
}


struct pyopencv_BOWImgDescriptorExtractor_t
{
    PyObject_HEAD
    Ptr<cv::BOWImgDescriptorExtractor> v;
};

static PyTypeObject pyopencv_BOWImgDescriptorExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BOWImgDescriptorExtractor",
    sizeof(pyopencv_BOWImgDescriptorExtractor_t),
};

static void pyopencv_BOWImgDescriptorExtractor_dealloc(PyObject* self)
{
    ((pyopencv_BOWImgDescriptorExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BOWImgDescriptorExtractor>& r)
{
    pyopencv_BOWImgDescriptorExtractor_t *m = PyObject_NEW(pyopencv_BOWImgDescriptorExtractor_t, &pyopencv_BOWImgDescriptorExtractor_Type);
    new (&(m->v)) Ptr<cv::BOWImgDescriptorExtractor>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BOWImgDescriptorExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BOWImgDescriptorExtractor_Type))
    {
        failmsg("Expected cv::BOWImgDescriptorExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BOWImgDescriptorExtractor_t*)src)->v.dynamicCast<cv::BOWImgDescriptorExtractor>();
    return true;
}


struct pyopencv_BOWKMeansTrainer_t
{
    PyObject_HEAD
    Ptr<cv::BOWKMeansTrainer> v;
};

static PyTypeObject pyopencv_BOWKMeansTrainer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BOWKMeansTrainer",
    sizeof(pyopencv_BOWKMeansTrainer_t),
};

static void pyopencv_BOWKMeansTrainer_dealloc(PyObject* self)
{
    ((pyopencv_BOWKMeansTrainer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BOWKMeansTrainer>& r)
{
    pyopencv_BOWKMeansTrainer_t *m = PyObject_NEW(pyopencv_BOWKMeansTrainer_t, &pyopencv_BOWKMeansTrainer_Type);
    new (&(m->v)) Ptr<cv::BOWKMeansTrainer>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BOWKMeansTrainer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BOWKMeansTrainer_Type))
    {
        failmsg("Expected cv::BOWKMeansTrainer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BOWKMeansTrainer_t*)src)->v.dynamicCast<cv::BOWKMeansTrainer>();
    return true;
}


struct pyopencv_BOWTrainer_t
{
    PyObject_HEAD
    Ptr<cv::BOWTrainer> v;
};

static PyTypeObject pyopencv_BOWTrainer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BOWTrainer",
    sizeof(pyopencv_BOWTrainer_t),
};

static void pyopencv_BOWTrainer_dealloc(PyObject* self)
{
    ((pyopencv_BOWTrainer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BOWTrainer>& r)
{
    pyopencv_BOWTrainer_t *m = PyObject_NEW(pyopencv_BOWTrainer_t, &pyopencv_BOWTrainer_Type);
    new (&(m->v)) Ptr<cv::BOWTrainer>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BOWTrainer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BOWTrainer_Type))
    {
        failmsg("Expected cv::BOWTrainer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BOWTrainer_t*)src)->v.dynamicCast<cv::BOWTrainer>();
    return true;
}


struct pyopencv_BRISK_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_BRISK_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BRISK",
    sizeof(pyopencv_BRISK_t),
};

static void pyopencv_BRISK_dealloc(PyObject* self)
{
    ((pyopencv_BRISK_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BRISK>& r)
{
    pyopencv_BRISK_t *m = PyObject_NEW(pyopencv_BRISK_t, &pyopencv_BRISK_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BRISK>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BRISK_Type))
    {
        failmsg("Expected cv::BRISK for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BRISK_t*)src)->v.dynamicCast<cv::BRISK>();
    return true;
}


struct pyopencv_BackgroundSubtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_BackgroundSubtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BackgroundSubtractor",
    sizeof(pyopencv_BackgroundSubtractor_t),
};

static void pyopencv_BackgroundSubtractor_dealloc(PyObject* self)
{
    ((pyopencv_BackgroundSubtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BackgroundSubtractor>& r)
{
    pyopencv_BackgroundSubtractor_t *m = PyObject_NEW(pyopencv_BackgroundSubtractor_t, &pyopencv_BackgroundSubtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BackgroundSubtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BackgroundSubtractor_Type))
    {
        failmsg("Expected cv::BackgroundSubtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BackgroundSubtractor_t*)src)->v.dynamicCast<cv::BackgroundSubtractor>();
    return true;
}


struct pyopencv_BackgroundSubtractorKNN_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_BackgroundSubtractorKNN_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BackgroundSubtractorKNN",
    sizeof(pyopencv_BackgroundSubtractorKNN_t),
};

static void pyopencv_BackgroundSubtractorKNN_dealloc(PyObject* self)
{
    ((pyopencv_BackgroundSubtractorKNN_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BackgroundSubtractorKNN>& r)
{
    pyopencv_BackgroundSubtractorKNN_t *m = PyObject_NEW(pyopencv_BackgroundSubtractorKNN_t, &pyopencv_BackgroundSubtractorKNN_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BackgroundSubtractorKNN>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BackgroundSubtractorKNN_Type))
    {
        failmsg("Expected cv::BackgroundSubtractorKNN for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BackgroundSubtractorKNN_t*)src)->v.dynamicCast<cv::BackgroundSubtractorKNN>();
    return true;
}


struct pyopencv_BackgroundSubtractorMOG2_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_BackgroundSubtractorMOG2_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BackgroundSubtractorMOG2",
    sizeof(pyopencv_BackgroundSubtractorMOG2_t),
};

static void pyopencv_BackgroundSubtractorMOG2_dealloc(PyObject* self)
{
    ((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BackgroundSubtractorMOG2>& r)
{
    pyopencv_BackgroundSubtractorMOG2_t *m = PyObject_NEW(pyopencv_BackgroundSubtractorMOG2_t, &pyopencv_BackgroundSubtractorMOG2_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BackgroundSubtractorMOG2>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BackgroundSubtractorMOG2_Type))
    {
        failmsg("Expected cv::BackgroundSubtractorMOG2 for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BackgroundSubtractorMOG2_t*)src)->v.dynamicCast<cv::BackgroundSubtractorMOG2>();
    return true;
}


struct pyopencv_BaseCascadeClassifier_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_BaseCascadeClassifier_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".BaseCascadeClassifier",
    sizeof(pyopencv_BaseCascadeClassifier_t),
};

static void pyopencv_BaseCascadeClassifier_dealloc(PyObject* self)
{
    ((pyopencv_BaseCascadeClassifier_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::BaseCascadeClassifier>& r)
{
    pyopencv_BaseCascadeClassifier_t *m = PyObject_NEW(pyopencv_BaseCascadeClassifier_t, &pyopencv_BaseCascadeClassifier_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::BaseCascadeClassifier>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_BaseCascadeClassifier_Type))
    {
        failmsg("Expected cv::BaseCascadeClassifier for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_BaseCascadeClassifier_t*)src)->v.dynamicCast<cv::BaseCascadeClassifier>();
    return true;
}


struct pyopencv_CLAHE_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_CLAHE_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CLAHE",
    sizeof(pyopencv_CLAHE_t),
};

static void pyopencv_CLAHE_dealloc(PyObject* self)
{
    ((pyopencv_CLAHE_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::CLAHE>& r)
{
    pyopencv_CLAHE_t *m = PyObject_NEW(pyopencv_CLAHE_t, &pyopencv_CLAHE_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::CLAHE>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CLAHE_Type))
    {
        failmsg("Expected cv::CLAHE for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CLAHE_t*)src)->v.dynamicCast<cv::CLAHE>();
    return true;
}


struct pyopencv_CalibrateCRF_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_CalibrateCRF_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CalibrateCRF",
    sizeof(pyopencv_CalibrateCRF_t),
};

static void pyopencv_CalibrateCRF_dealloc(PyObject* self)
{
    ((pyopencv_CalibrateCRF_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::CalibrateCRF>& r)
{
    pyopencv_CalibrateCRF_t *m = PyObject_NEW(pyopencv_CalibrateCRF_t, &pyopencv_CalibrateCRF_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::CalibrateCRF>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CalibrateCRF_Type))
    {
        failmsg("Expected cv::CalibrateCRF for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CalibrateCRF_t*)src)->v.dynamicCast<cv::CalibrateCRF>();
    return true;
}


struct pyopencv_CalibrateDebevec_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_CalibrateDebevec_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CalibrateDebevec",
    sizeof(pyopencv_CalibrateDebevec_t),
};

static void pyopencv_CalibrateDebevec_dealloc(PyObject* self)
{
    ((pyopencv_CalibrateDebevec_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::CalibrateDebevec>& r)
{
    pyopencv_CalibrateDebevec_t *m = PyObject_NEW(pyopencv_CalibrateDebevec_t, &pyopencv_CalibrateDebevec_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::CalibrateDebevec>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CalibrateDebevec_Type))
    {
        failmsg("Expected cv::CalibrateDebevec for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CalibrateDebevec_t*)src)->v.dynamicCast<cv::CalibrateDebevec>();
    return true;
}


struct pyopencv_CalibrateRobertson_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_CalibrateRobertson_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CalibrateRobertson",
    sizeof(pyopencv_CalibrateRobertson_t),
};

static void pyopencv_CalibrateRobertson_dealloc(PyObject* self)
{
    ((pyopencv_CalibrateRobertson_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::CalibrateRobertson>& r)
{
    pyopencv_CalibrateRobertson_t *m = PyObject_NEW(pyopencv_CalibrateRobertson_t, &pyopencv_CalibrateRobertson_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::CalibrateRobertson>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CalibrateRobertson_Type))
    {
        failmsg("Expected cv::CalibrateRobertson for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CalibrateRobertson_t*)src)->v.dynamicCast<cv::CalibrateRobertson>();
    return true;
}


struct pyopencv_CascadeClassifier_t
{
    PyObject_HEAD
    Ptr<cv::CascadeClassifier> v;
};

static PyTypeObject pyopencv_CascadeClassifier_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CascadeClassifier",
    sizeof(pyopencv_CascadeClassifier_t),
};

static void pyopencv_CascadeClassifier_dealloc(PyObject* self)
{
    ((pyopencv_CascadeClassifier_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::CascadeClassifier>& r)
{
    pyopencv_CascadeClassifier_t *m = PyObject_NEW(pyopencv_CascadeClassifier_t, &pyopencv_CascadeClassifier_Type);
    new (&(m->v)) Ptr<cv::CascadeClassifier>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::CascadeClassifier>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CascadeClassifier_Type))
    {
        failmsg("Expected cv::CascadeClassifier for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CascadeClassifier_t*)src)->v.dynamicCast<cv::CascadeClassifier>();
    return true;
}


struct pyopencv_ChiHistogramCostExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ChiHistogramCostExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ChiHistogramCostExtractor",
    sizeof(pyopencv_ChiHistogramCostExtractor_t),
};

static void pyopencv_ChiHistogramCostExtractor_dealloc(PyObject* self)
{
    ((pyopencv_ChiHistogramCostExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ChiHistogramCostExtractor>& r)
{
    pyopencv_ChiHistogramCostExtractor_t *m = PyObject_NEW(pyopencv_ChiHistogramCostExtractor_t, &pyopencv_ChiHistogramCostExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ChiHistogramCostExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ChiHistogramCostExtractor_Type))
    {
        failmsg("Expected cv::ChiHistogramCostExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ChiHistogramCostExtractor_t*)src)->v.dynamicCast<cv::ChiHistogramCostExtractor>();
    return true;
}


struct pyopencv_CirclesGridFinderParameters_t
{
    PyObject_HEAD
    cv::CirclesGridFinderParameters v;
};

static PyTypeObject pyopencv_CirclesGridFinderParameters_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CirclesGridFinderParameters",
    sizeof(pyopencv_CirclesGridFinderParameters_t),
};

static void pyopencv_CirclesGridFinderParameters_dealloc(PyObject* self)
{
    ((pyopencv_CirclesGridFinderParameters_t*)self)->v.cv::CirclesGridFinderParameters::~CirclesGridFinderParameters();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::CirclesGridFinderParameters& r)
{
    pyopencv_CirclesGridFinderParameters_t *m = PyObject_NEW(pyopencv_CirclesGridFinderParameters_t, &pyopencv_CirclesGridFinderParameters_Type);
    new (&m->v) cv::CirclesGridFinderParameters(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::CirclesGridFinderParameters& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CirclesGridFinderParameters_Type))
    {
        failmsg("Expected cv::CirclesGridFinderParameters for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CirclesGridFinderParameters_t*)src)->v;
    return true;
}

struct pyopencv_CirclesGridFinderParameters2_t
{
    PyObject_HEAD
    cv::CirclesGridFinderParameters2 v;
};

static PyTypeObject pyopencv_CirclesGridFinderParameters2_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".CirclesGridFinderParameters2",
    sizeof(pyopencv_CirclesGridFinderParameters2_t),
};

static void pyopencv_CirclesGridFinderParameters2_dealloc(PyObject* self)
{
    ((pyopencv_CirclesGridFinderParameters2_t*)self)->v.cv::CirclesGridFinderParameters2::~CirclesGridFinderParameters2();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::CirclesGridFinderParameters2& r)
{
    pyopencv_CirclesGridFinderParameters2_t *m = PyObject_NEW(pyopencv_CirclesGridFinderParameters2_t, &pyopencv_CirclesGridFinderParameters2_Type);
    new (&m->v) cv::CirclesGridFinderParameters2(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::CirclesGridFinderParameters2& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_CirclesGridFinderParameters2_Type))
    {
        failmsg("Expected cv::CirclesGridFinderParameters2 for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_CirclesGridFinderParameters2_t*)src)->v;
    return true;
}

struct pyopencv_DMatch_t
{
    PyObject_HEAD
    cv::DMatch v;
};

static PyTypeObject pyopencv_DMatch_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".DMatch",
    sizeof(pyopencv_DMatch_t),
};

static void pyopencv_DMatch_dealloc(PyObject* self)
{
    ((pyopencv_DMatch_t*)self)->v.cv::DMatch::~DMatch();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::DMatch& r)
{
    pyopencv_DMatch_t *m = PyObject_NEW(pyopencv_DMatch_t, &pyopencv_DMatch_Type);
    new (&m->v) cv::DMatch(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::DMatch& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_DMatch_Type))
    {
        failmsg("Expected cv::DMatch for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_DMatch_t*)src)->v;
    return true;
}

struct pyopencv_DenseOpticalFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_DenseOpticalFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".DenseOpticalFlow",
    sizeof(pyopencv_DenseOpticalFlow_t),
};

static void pyopencv_DenseOpticalFlow_dealloc(PyObject* self)
{
    ((pyopencv_DenseOpticalFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::DenseOpticalFlow>& r)
{
    pyopencv_DenseOpticalFlow_t *m = PyObject_NEW(pyopencv_DenseOpticalFlow_t, &pyopencv_DenseOpticalFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::DenseOpticalFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_DenseOpticalFlow_Type))
    {
        failmsg("Expected cv::DenseOpticalFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_DenseOpticalFlow_t*)src)->v.dynamicCast<cv::DenseOpticalFlow>();
    return true;
}


struct pyopencv_DescriptorMatcher_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_DescriptorMatcher_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".DescriptorMatcher",
    sizeof(pyopencv_DescriptorMatcher_t),
};

static void pyopencv_DescriptorMatcher_dealloc(PyObject* self)
{
    ((pyopencv_DescriptorMatcher_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::DescriptorMatcher>& r)
{
    pyopencv_DescriptorMatcher_t *m = PyObject_NEW(pyopencv_DescriptorMatcher_t, &pyopencv_DescriptorMatcher_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::DescriptorMatcher>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_DescriptorMatcher_Type))
    {
        failmsg("Expected cv::DescriptorMatcher for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_DescriptorMatcher_t*)src)->v.dynamicCast<cv::DescriptorMatcher>();
    return true;
}


struct pyopencv_DualTVL1OpticalFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_DualTVL1OpticalFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".DualTVL1OpticalFlow",
    sizeof(pyopencv_DualTVL1OpticalFlow_t),
};

static void pyopencv_DualTVL1OpticalFlow_dealloc(PyObject* self)
{
    ((pyopencv_DualTVL1OpticalFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::DualTVL1OpticalFlow>& r)
{
    pyopencv_DualTVL1OpticalFlow_t *m = PyObject_NEW(pyopencv_DualTVL1OpticalFlow_t, &pyopencv_DualTVL1OpticalFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::DualTVL1OpticalFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_DualTVL1OpticalFlow_Type))
    {
        failmsg("Expected cv::DualTVL1OpticalFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_DualTVL1OpticalFlow_t*)src)->v.dynamicCast<cv::DualTVL1OpticalFlow>();
    return true;
}


struct pyopencv_EMDHistogramCostExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_EMDHistogramCostExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".EMDHistogramCostExtractor",
    sizeof(pyopencv_EMDHistogramCostExtractor_t),
};

static void pyopencv_EMDHistogramCostExtractor_dealloc(PyObject* self)
{
    ((pyopencv_EMDHistogramCostExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::EMDHistogramCostExtractor>& r)
{
    pyopencv_EMDHistogramCostExtractor_t *m = PyObject_NEW(pyopencv_EMDHistogramCostExtractor_t, &pyopencv_EMDHistogramCostExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::EMDHistogramCostExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_EMDHistogramCostExtractor_Type))
    {
        failmsg("Expected cv::EMDHistogramCostExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_EMDHistogramCostExtractor_t*)src)->v.dynamicCast<cv::EMDHistogramCostExtractor>();
    return true;
}


struct pyopencv_EMDL1HistogramCostExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_EMDL1HistogramCostExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".EMDL1HistogramCostExtractor",
    sizeof(pyopencv_EMDL1HistogramCostExtractor_t),
};

static void pyopencv_EMDL1HistogramCostExtractor_dealloc(PyObject* self)
{
    ((pyopencv_EMDL1HistogramCostExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::EMDL1HistogramCostExtractor>& r)
{
    pyopencv_EMDL1HistogramCostExtractor_t *m = PyObject_NEW(pyopencv_EMDL1HistogramCostExtractor_t, &pyopencv_EMDL1HistogramCostExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::EMDL1HistogramCostExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_EMDL1HistogramCostExtractor_Type))
    {
        failmsg("Expected cv::EMDL1HistogramCostExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_EMDL1HistogramCostExtractor_t*)src)->v.dynamicCast<cv::EMDL1HistogramCostExtractor>();
    return true;
}


struct pyopencv_FarnebackOpticalFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_FarnebackOpticalFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".FarnebackOpticalFlow",
    sizeof(pyopencv_FarnebackOpticalFlow_t),
};

static void pyopencv_FarnebackOpticalFlow_dealloc(PyObject* self)
{
    ((pyopencv_FarnebackOpticalFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::FarnebackOpticalFlow>& r)
{
    pyopencv_FarnebackOpticalFlow_t *m = PyObject_NEW(pyopencv_FarnebackOpticalFlow_t, &pyopencv_FarnebackOpticalFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::FarnebackOpticalFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_FarnebackOpticalFlow_Type))
    {
        failmsg("Expected cv::FarnebackOpticalFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_FarnebackOpticalFlow_t*)src)->v.dynamicCast<cv::FarnebackOpticalFlow>();
    return true;
}


struct pyopencv_FastFeatureDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_FastFeatureDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".FastFeatureDetector",
    sizeof(pyopencv_FastFeatureDetector_t),
};

static void pyopencv_FastFeatureDetector_dealloc(PyObject* self)
{
    ((pyopencv_FastFeatureDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::FastFeatureDetector>& r)
{
    pyopencv_FastFeatureDetector_t *m = PyObject_NEW(pyopencv_FastFeatureDetector_t, &pyopencv_FastFeatureDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::FastFeatureDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_FastFeatureDetector_Type))
    {
        failmsg("Expected cv::FastFeatureDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_FastFeatureDetector_t*)src)->v.dynamicCast<cv::FastFeatureDetector>();
    return true;
}


struct pyopencv_Feature2D_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_Feature2D_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".Feature2D",
    sizeof(pyopencv_Feature2D_t),
};

static void pyopencv_Feature2D_dealloc(PyObject* self)
{
    ((pyopencv_Feature2D_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::Feature2D>& r)
{
    pyopencv_Feature2D_t *m = PyObject_NEW(pyopencv_Feature2D_t, &pyopencv_Feature2D_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::Feature2D>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_Feature2D_Type))
    {
        failmsg("Expected cv::Feature2D for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_Feature2D_t*)src)->v.dynamicCast<cv::Feature2D>();
    return true;
}


struct pyopencv_FileNode_t
{
    PyObject_HEAD
    cv::FileNode v;
};

static PyTypeObject pyopencv_FileNode_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".FileNode",
    sizeof(pyopencv_FileNode_t),
};

static void pyopencv_FileNode_dealloc(PyObject* self)
{
    ((pyopencv_FileNode_t*)self)->v.cv::FileNode::~FileNode();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::FileNode& r)
{
    pyopencv_FileNode_t *m = PyObject_NEW(pyopencv_FileNode_t, &pyopencv_FileNode_Type);
    new (&m->v) cv::FileNode(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::FileNode& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_FileNode_Type))
    {
        failmsg("Expected cv::FileNode for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_FileNode_t*)src)->v;
    return true;
}

struct pyopencv_FileStorage_t
{
    PyObject_HEAD
    Ptr<cv::FileStorage> v;
};

static PyTypeObject pyopencv_FileStorage_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".FileStorage",
    sizeof(pyopencv_FileStorage_t),
};

static void pyopencv_FileStorage_dealloc(PyObject* self)
{
    ((pyopencv_FileStorage_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::FileStorage>& r)
{
    pyopencv_FileStorage_t *m = PyObject_NEW(pyopencv_FileStorage_t, &pyopencv_FileStorage_Type);
    new (&(m->v)) Ptr<cv::FileStorage>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::FileStorage>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_FileStorage_Type))
    {
        failmsg("Expected cv::FileStorage for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_FileStorage_t*)src)->v.dynamicCast<cv::FileStorage>();
    return true;
}


struct pyopencv_FlannBasedMatcher_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_FlannBasedMatcher_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".FlannBasedMatcher",
    sizeof(pyopencv_FlannBasedMatcher_t),
};

static void pyopencv_FlannBasedMatcher_dealloc(PyObject* self)
{
    ((pyopencv_FlannBasedMatcher_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::FlannBasedMatcher>& r)
{
    pyopencv_FlannBasedMatcher_t *m = PyObject_NEW(pyopencv_FlannBasedMatcher_t, &pyopencv_FlannBasedMatcher_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::FlannBasedMatcher>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_FlannBasedMatcher_Type))
    {
        failmsg("Expected cv::FlannBasedMatcher for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_FlannBasedMatcher_t*)src)->v.dynamicCast<cv::FlannBasedMatcher>();
    return true;
}


struct pyopencv_GFTTDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_GFTTDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".GFTTDetector",
    sizeof(pyopencv_GFTTDetector_t),
};

static void pyopencv_GFTTDetector_dealloc(PyObject* self)
{
    ((pyopencv_GFTTDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::GFTTDetector>& r)
{
    pyopencv_GFTTDetector_t *m = PyObject_NEW(pyopencv_GFTTDetector_t, &pyopencv_GFTTDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::GFTTDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_GFTTDetector_Type))
    {
        failmsg("Expected cv::GFTTDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_GFTTDetector_t*)src)->v.dynamicCast<cv::GFTTDetector>();
    return true;
}


struct pyopencv_HOGDescriptor_t
{
    PyObject_HEAD
    Ptr<cv::HOGDescriptor> v;
};

static PyTypeObject pyopencv_HOGDescriptor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".HOGDescriptor",
    sizeof(pyopencv_HOGDescriptor_t),
};

static void pyopencv_HOGDescriptor_dealloc(PyObject* self)
{
    ((pyopencv_HOGDescriptor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::HOGDescriptor>& r)
{
    pyopencv_HOGDescriptor_t *m = PyObject_NEW(pyopencv_HOGDescriptor_t, &pyopencv_HOGDescriptor_Type);
    new (&(m->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::HOGDescriptor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_HOGDescriptor_Type))
    {
        failmsg("Expected cv::HOGDescriptor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_HOGDescriptor_t*)src)->v.dynamicCast<cv::HOGDescriptor>();
    return true;
}


struct pyopencv_HausdorffDistanceExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_HausdorffDistanceExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".HausdorffDistanceExtractor",
    sizeof(pyopencv_HausdorffDistanceExtractor_t),
};

static void pyopencv_HausdorffDistanceExtractor_dealloc(PyObject* self)
{
    ((pyopencv_HausdorffDistanceExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::HausdorffDistanceExtractor>& r)
{
    pyopencv_HausdorffDistanceExtractor_t *m = PyObject_NEW(pyopencv_HausdorffDistanceExtractor_t, &pyopencv_HausdorffDistanceExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::HausdorffDistanceExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_HausdorffDistanceExtractor_Type))
    {
        failmsg("Expected cv::HausdorffDistanceExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_HausdorffDistanceExtractor_t*)src)->v.dynamicCast<cv::HausdorffDistanceExtractor>();
    return true;
}


struct pyopencv_HistogramCostExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_HistogramCostExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".HistogramCostExtractor",
    sizeof(pyopencv_HistogramCostExtractor_t),
};

static void pyopencv_HistogramCostExtractor_dealloc(PyObject* self)
{
    ((pyopencv_HistogramCostExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::HistogramCostExtractor>& r)
{
    pyopencv_HistogramCostExtractor_t *m = PyObject_NEW(pyopencv_HistogramCostExtractor_t, &pyopencv_HistogramCostExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::HistogramCostExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_HistogramCostExtractor_Type))
    {
        failmsg("Expected cv::HistogramCostExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_HistogramCostExtractor_t*)src)->v.dynamicCast<cv::HistogramCostExtractor>();
    return true;
}


struct pyopencv_KAZE_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_KAZE_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".KAZE",
    sizeof(pyopencv_KAZE_t),
};

static void pyopencv_KAZE_dealloc(PyObject* self)
{
    ((pyopencv_KAZE_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::KAZE>& r)
{
    pyopencv_KAZE_t *m = PyObject_NEW(pyopencv_KAZE_t, &pyopencv_KAZE_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::KAZE>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_KAZE_Type))
    {
        failmsg("Expected cv::KAZE for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_KAZE_t*)src)->v.dynamicCast<cv::KAZE>();
    return true;
}


struct pyopencv_KalmanFilter_t
{
    PyObject_HEAD
    Ptr<cv::KalmanFilter> v;
};

static PyTypeObject pyopencv_KalmanFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".KalmanFilter",
    sizeof(pyopencv_KalmanFilter_t),
};

static void pyopencv_KalmanFilter_dealloc(PyObject* self)
{
    ((pyopencv_KalmanFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::KalmanFilter>& r)
{
    pyopencv_KalmanFilter_t *m = PyObject_NEW(pyopencv_KalmanFilter_t, &pyopencv_KalmanFilter_Type);
    new (&(m->v)) Ptr<cv::KalmanFilter>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::KalmanFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_KalmanFilter_Type))
    {
        failmsg("Expected cv::KalmanFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_KalmanFilter_t*)src)->v.dynamicCast<cv::KalmanFilter>();
    return true;
}


struct pyopencv_KeyPoint_t
{
    PyObject_HEAD
    cv::KeyPoint v;
};

static PyTypeObject pyopencv_KeyPoint_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".KeyPoint",
    sizeof(pyopencv_KeyPoint_t),
};

static void pyopencv_KeyPoint_dealloc(PyObject* self)
{
    ((pyopencv_KeyPoint_t*)self)->v.cv::KeyPoint::~KeyPoint();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::KeyPoint& r)
{
    pyopencv_KeyPoint_t *m = PyObject_NEW(pyopencv_KeyPoint_t, &pyopencv_KeyPoint_Type);
    new (&m->v) cv::KeyPoint(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::KeyPoint& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_KeyPoint_Type))
    {
        failmsg("Expected cv::KeyPoint for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_KeyPoint_t*)src)->v;
    return true;
}

struct pyopencv_LineSegmentDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_LineSegmentDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".LineSegmentDetector",
    sizeof(pyopencv_LineSegmentDetector_t),
};

static void pyopencv_LineSegmentDetector_dealloc(PyObject* self)
{
    ((pyopencv_LineSegmentDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::LineSegmentDetector>& r)
{
    pyopencv_LineSegmentDetector_t *m = PyObject_NEW(pyopencv_LineSegmentDetector_t, &pyopencv_LineSegmentDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::LineSegmentDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_LineSegmentDetector_Type))
    {
        failmsg("Expected cv::LineSegmentDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_LineSegmentDetector_t*)src)->v.dynamicCast<cv::LineSegmentDetector>();
    return true;
}


struct pyopencv_MSER_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_MSER_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".MSER",
    sizeof(pyopencv_MSER_t),
};

static void pyopencv_MSER_dealloc(PyObject* self)
{
    ((pyopencv_MSER_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::MSER>& r)
{
    pyopencv_MSER_t *m = PyObject_NEW(pyopencv_MSER_t, &pyopencv_MSER_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::MSER>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_MSER_Type))
    {
        failmsg("Expected cv::MSER for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_MSER_t*)src)->v.dynamicCast<cv::MSER>();
    return true;
}


struct pyopencv_MergeDebevec_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_MergeDebevec_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".MergeDebevec",
    sizeof(pyopencv_MergeDebevec_t),
};

static void pyopencv_MergeDebevec_dealloc(PyObject* self)
{
    ((pyopencv_MergeDebevec_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::MergeDebevec>& r)
{
    pyopencv_MergeDebevec_t *m = PyObject_NEW(pyopencv_MergeDebevec_t, &pyopencv_MergeDebevec_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::MergeDebevec>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_MergeDebevec_Type))
    {
        failmsg("Expected cv::MergeDebevec for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_MergeDebevec_t*)src)->v.dynamicCast<cv::MergeDebevec>();
    return true;
}


struct pyopencv_MergeExposures_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_MergeExposures_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".MergeExposures",
    sizeof(pyopencv_MergeExposures_t),
};

static void pyopencv_MergeExposures_dealloc(PyObject* self)
{
    ((pyopencv_MergeExposures_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::MergeExposures>& r)
{
    pyopencv_MergeExposures_t *m = PyObject_NEW(pyopencv_MergeExposures_t, &pyopencv_MergeExposures_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::MergeExposures>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_MergeExposures_Type))
    {
        failmsg("Expected cv::MergeExposures for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_MergeExposures_t*)src)->v.dynamicCast<cv::MergeExposures>();
    return true;
}


struct pyopencv_MergeMertens_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_MergeMertens_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".MergeMertens",
    sizeof(pyopencv_MergeMertens_t),
};

static void pyopencv_MergeMertens_dealloc(PyObject* self)
{
    ((pyopencv_MergeMertens_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::MergeMertens>& r)
{
    pyopencv_MergeMertens_t *m = PyObject_NEW(pyopencv_MergeMertens_t, &pyopencv_MergeMertens_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::MergeMertens>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_MergeMertens_Type))
    {
        failmsg("Expected cv::MergeMertens for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_MergeMertens_t*)src)->v.dynamicCast<cv::MergeMertens>();
    return true;
}


struct pyopencv_MergeRobertson_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_MergeRobertson_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".MergeRobertson",
    sizeof(pyopencv_MergeRobertson_t),
};

static void pyopencv_MergeRobertson_dealloc(PyObject* self)
{
    ((pyopencv_MergeRobertson_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::MergeRobertson>& r)
{
    pyopencv_MergeRobertson_t *m = PyObject_NEW(pyopencv_MergeRobertson_t, &pyopencv_MergeRobertson_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::MergeRobertson>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_MergeRobertson_Type))
    {
        failmsg("Expected cv::MergeRobertson for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_MergeRobertson_t*)src)->v.dynamicCast<cv::MergeRobertson>();
    return true;
}


template<> bool pyopencv_to(PyObject* src, cv::Moments& dst, const char* name);

struct pyopencv_MultiTracker_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_MultiTracker_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".MultiTracker",
    sizeof(pyopencv_MultiTracker_t),
};

static void pyopencv_MultiTracker_dealloc(PyObject* self)
{
    ((pyopencv_MultiTracker_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::MultiTracker>& r)
{
    pyopencv_MultiTracker_t *m = PyObject_NEW(pyopencv_MultiTracker_t, &pyopencv_MultiTracker_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::MultiTracker>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_MultiTracker_Type))
    {
        failmsg("Expected cv::MultiTracker for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_MultiTracker_t*)src)->v.dynamicCast<cv::MultiTracker>();
    return true;
}


struct pyopencv_NormHistogramCostExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_NormHistogramCostExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".NormHistogramCostExtractor",
    sizeof(pyopencv_NormHistogramCostExtractor_t),
};

static void pyopencv_NormHistogramCostExtractor_dealloc(PyObject* self)
{
    ((pyopencv_NormHistogramCostExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::NormHistogramCostExtractor>& r)
{
    pyopencv_NormHistogramCostExtractor_t *m = PyObject_NEW(pyopencv_NormHistogramCostExtractor_t, &pyopencv_NormHistogramCostExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::NormHistogramCostExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_NormHistogramCostExtractor_Type))
    {
        failmsg("Expected cv::NormHistogramCostExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_NormHistogramCostExtractor_t*)src)->v.dynamicCast<cv::NormHistogramCostExtractor>();
    return true;
}


struct pyopencv_ORB_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ORB_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ORB",
    sizeof(pyopencv_ORB_t),
};

static void pyopencv_ORB_dealloc(PyObject* self)
{
    ((pyopencv_ORB_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ORB>& r)
{
    pyopencv_ORB_t *m = PyObject_NEW(pyopencv_ORB_t, &pyopencv_ORB_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ORB>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ORB_Type))
    {
        failmsg("Expected cv::ORB for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ORB_t*)src)->v.dynamicCast<cv::ORB>();
    return true;
}


struct pyopencv_ShapeContextDistanceExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ShapeContextDistanceExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ShapeContextDistanceExtractor",
    sizeof(pyopencv_ShapeContextDistanceExtractor_t),
};

static void pyopencv_ShapeContextDistanceExtractor_dealloc(PyObject* self)
{
    ((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ShapeContextDistanceExtractor>& r)
{
    pyopencv_ShapeContextDistanceExtractor_t *m = PyObject_NEW(pyopencv_ShapeContextDistanceExtractor_t, &pyopencv_ShapeContextDistanceExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ShapeContextDistanceExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ShapeContextDistanceExtractor_Type))
    {
        failmsg("Expected cv::ShapeContextDistanceExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ShapeContextDistanceExtractor_t*)src)->v.dynamicCast<cv::ShapeContextDistanceExtractor>();
    return true;
}


struct pyopencv_ShapeDistanceExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ShapeDistanceExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ShapeDistanceExtractor",
    sizeof(pyopencv_ShapeDistanceExtractor_t),
};

static void pyopencv_ShapeDistanceExtractor_dealloc(PyObject* self)
{
    ((pyopencv_ShapeDistanceExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ShapeDistanceExtractor>& r)
{
    pyopencv_ShapeDistanceExtractor_t *m = PyObject_NEW(pyopencv_ShapeDistanceExtractor_t, &pyopencv_ShapeDistanceExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ShapeDistanceExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ShapeDistanceExtractor_Type))
    {
        failmsg("Expected cv::ShapeDistanceExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ShapeDistanceExtractor_t*)src)->v.dynamicCast<cv::ShapeDistanceExtractor>();
    return true;
}


struct pyopencv_ShapeTransformer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ShapeTransformer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ShapeTransformer",
    sizeof(pyopencv_ShapeTransformer_t),
};

static void pyopencv_ShapeTransformer_dealloc(PyObject* self)
{
    ((pyopencv_ShapeTransformer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ShapeTransformer>& r)
{
    pyopencv_ShapeTransformer_t *m = PyObject_NEW(pyopencv_ShapeTransformer_t, &pyopencv_ShapeTransformer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ShapeTransformer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ShapeTransformer_Type))
    {
        failmsg("Expected cv::ShapeTransformer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ShapeTransformer_t*)src)->v.dynamicCast<cv::ShapeTransformer>();
    return true;
}


struct pyopencv_SimpleBlobDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_SimpleBlobDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".SimpleBlobDetector",
    sizeof(pyopencv_SimpleBlobDetector_t),
};

static void pyopencv_SimpleBlobDetector_dealloc(PyObject* self)
{
    ((pyopencv_SimpleBlobDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::SimpleBlobDetector>& r)
{
    pyopencv_SimpleBlobDetector_t *m = PyObject_NEW(pyopencv_SimpleBlobDetector_t, &pyopencv_SimpleBlobDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::SimpleBlobDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_SimpleBlobDetector_Type))
    {
        failmsg("Expected cv::SimpleBlobDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_SimpleBlobDetector_t*)src)->v.dynamicCast<cv::SimpleBlobDetector>();
    return true;
}


struct pyopencv_SimpleBlobDetector_Params_t
{
    PyObject_HEAD
    cv::SimpleBlobDetector::Params v;
};

static PyTypeObject pyopencv_SimpleBlobDetector_Params_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".SimpleBlobDetector_Params",
    sizeof(pyopencv_SimpleBlobDetector_Params_t),
};

static void pyopencv_SimpleBlobDetector_Params_dealloc(PyObject* self)
{
    ((pyopencv_SimpleBlobDetector_Params_t*)self)->v.cv::SimpleBlobDetector::Params::~Params();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::SimpleBlobDetector::Params& r)
{
    pyopencv_SimpleBlobDetector_Params_t *m = PyObject_NEW(pyopencv_SimpleBlobDetector_Params_t, &pyopencv_SimpleBlobDetector_Params_Type);
    new (&m->v) cv::SimpleBlobDetector::Params(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::SimpleBlobDetector::Params& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_SimpleBlobDetector_Params_Type))
    {
        failmsg("Expected cv::SimpleBlobDetector::Params for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_SimpleBlobDetector_Params_t*)src)->v;
    return true;
}

struct pyopencv_SparseOpticalFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_SparseOpticalFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".SparseOpticalFlow",
    sizeof(pyopencv_SparseOpticalFlow_t),
};

static void pyopencv_SparseOpticalFlow_dealloc(PyObject* self)
{
    ((pyopencv_SparseOpticalFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::SparseOpticalFlow>& r)
{
    pyopencv_SparseOpticalFlow_t *m = PyObject_NEW(pyopencv_SparseOpticalFlow_t, &pyopencv_SparseOpticalFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::SparseOpticalFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_SparseOpticalFlow_Type))
    {
        failmsg("Expected cv::SparseOpticalFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_SparseOpticalFlow_t*)src)->v.dynamicCast<cv::SparseOpticalFlow>();
    return true;
}


struct pyopencv_SparsePyrLKOpticalFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_SparsePyrLKOpticalFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".SparsePyrLKOpticalFlow",
    sizeof(pyopencv_SparsePyrLKOpticalFlow_t),
};

static void pyopencv_SparsePyrLKOpticalFlow_dealloc(PyObject* self)
{
    ((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::SparsePyrLKOpticalFlow>& r)
{
    pyopencv_SparsePyrLKOpticalFlow_t *m = PyObject_NEW(pyopencv_SparsePyrLKOpticalFlow_t, &pyopencv_SparsePyrLKOpticalFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::SparsePyrLKOpticalFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_SparsePyrLKOpticalFlow_Type))
    {
        failmsg("Expected cv::SparsePyrLKOpticalFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_SparsePyrLKOpticalFlow_t*)src)->v.dynamicCast<cv::SparsePyrLKOpticalFlow>();
    return true;
}


struct pyopencv_StereoBM_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_StereoBM_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".StereoBM",
    sizeof(pyopencv_StereoBM_t),
};

static void pyopencv_StereoBM_dealloc(PyObject* self)
{
    ((pyopencv_StereoBM_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::StereoBM>& r)
{
    pyopencv_StereoBM_t *m = PyObject_NEW(pyopencv_StereoBM_t, &pyopencv_StereoBM_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::StereoBM>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_StereoBM_Type))
    {
        failmsg("Expected cv::StereoBM for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_StereoBM_t*)src)->v.dynamicCast<cv::StereoBM>();
    return true;
}


struct pyopencv_StereoMatcher_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_StereoMatcher_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".StereoMatcher",
    sizeof(pyopencv_StereoMatcher_t),
};

static void pyopencv_StereoMatcher_dealloc(PyObject* self)
{
    ((pyopencv_StereoMatcher_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::StereoMatcher>& r)
{
    pyopencv_StereoMatcher_t *m = PyObject_NEW(pyopencv_StereoMatcher_t, &pyopencv_StereoMatcher_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::StereoMatcher>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_StereoMatcher_Type))
    {
        failmsg("Expected cv::StereoMatcher for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_StereoMatcher_t*)src)->v.dynamicCast<cv::StereoMatcher>();
    return true;
}


struct pyopencv_StereoSGBM_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_StereoSGBM_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".StereoSGBM",
    sizeof(pyopencv_StereoSGBM_t),
};

static void pyopencv_StereoSGBM_dealloc(PyObject* self)
{
    ((pyopencv_StereoSGBM_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::StereoSGBM>& r)
{
    pyopencv_StereoSGBM_t *m = PyObject_NEW(pyopencv_StereoSGBM_t, &pyopencv_StereoSGBM_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::StereoSGBM>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_StereoSGBM_Type))
    {
        failmsg("Expected cv::StereoSGBM for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_StereoSGBM_t*)src)->v.dynamicCast<cv::StereoSGBM>();
    return true;
}


struct pyopencv_Stitcher_t
{
    PyObject_HEAD
    Ptr<cv::Stitcher> v;
};

static PyTypeObject pyopencv_Stitcher_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".Stitcher",
    sizeof(pyopencv_Stitcher_t),
};

static void pyopencv_Stitcher_dealloc(PyObject* self)
{
    ((pyopencv_Stitcher_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::Stitcher>& r)
{
    pyopencv_Stitcher_t *m = PyObject_NEW(pyopencv_Stitcher_t, &pyopencv_Stitcher_Type);
    new (&(m->v)) Ptr<cv::Stitcher>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::Stitcher>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_Stitcher_Type))
    {
        failmsg("Expected cv::Stitcher for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_Stitcher_t*)src)->v.dynamicCast<cv::Stitcher>();
    return true;
}


struct pyopencv_Subdiv2D_t
{
    PyObject_HEAD
    Ptr<cv::Subdiv2D> v;
};

static PyTypeObject pyopencv_Subdiv2D_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".Subdiv2D",
    sizeof(pyopencv_Subdiv2D_t),
};

static void pyopencv_Subdiv2D_dealloc(PyObject* self)
{
    ((pyopencv_Subdiv2D_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::Subdiv2D>& r)
{
    pyopencv_Subdiv2D_t *m = PyObject_NEW(pyopencv_Subdiv2D_t, &pyopencv_Subdiv2D_Type);
    new (&(m->v)) Ptr<cv::Subdiv2D>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::Subdiv2D>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_Subdiv2D_Type))
    {
        failmsg("Expected cv::Subdiv2D for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_Subdiv2D_t*)src)->v.dynamicCast<cv::Subdiv2D>();
    return true;
}


struct pyopencv_ThinPlateSplineShapeTransformer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ThinPlateSplineShapeTransformer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ThinPlateSplineShapeTransformer",
    sizeof(pyopencv_ThinPlateSplineShapeTransformer_t),
};

static void pyopencv_ThinPlateSplineShapeTransformer_dealloc(PyObject* self)
{
    ((pyopencv_ThinPlateSplineShapeTransformer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ThinPlateSplineShapeTransformer>& r)
{
    pyopencv_ThinPlateSplineShapeTransformer_t *m = PyObject_NEW(pyopencv_ThinPlateSplineShapeTransformer_t, &pyopencv_ThinPlateSplineShapeTransformer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ThinPlateSplineShapeTransformer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ThinPlateSplineShapeTransformer_Type))
    {
        failmsg("Expected cv::ThinPlateSplineShapeTransformer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ThinPlateSplineShapeTransformer_t*)src)->v.dynamicCast<cv::ThinPlateSplineShapeTransformer>();
    return true;
}


struct pyopencv_TickMeter_t
{
    PyObject_HEAD
    Ptr<cv::TickMeter> v;
};

static PyTypeObject pyopencv_TickMeter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TickMeter",
    sizeof(pyopencv_TickMeter_t),
};

static void pyopencv_TickMeter_dealloc(PyObject* self)
{
    ((pyopencv_TickMeter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TickMeter>& r)
{
    pyopencv_TickMeter_t *m = PyObject_NEW(pyopencv_TickMeter_t, &pyopencv_TickMeter_Type);
    new (&(m->v)) Ptr<cv::TickMeter>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TickMeter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TickMeter_Type))
    {
        failmsg("Expected cv::TickMeter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TickMeter_t*)src)->v.dynamicCast<cv::TickMeter>();
    return true;
}


struct pyopencv_Tonemap_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_Tonemap_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".Tonemap",
    sizeof(pyopencv_Tonemap_t),
};

static void pyopencv_Tonemap_dealloc(PyObject* self)
{
    ((pyopencv_Tonemap_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::Tonemap>& r)
{
    pyopencv_Tonemap_t *m = PyObject_NEW(pyopencv_Tonemap_t, &pyopencv_Tonemap_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::Tonemap>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_Tonemap_Type))
    {
        failmsg("Expected cv::Tonemap for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_Tonemap_t*)src)->v.dynamicCast<cv::Tonemap>();
    return true;
}


struct pyopencv_TonemapDrago_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TonemapDrago_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TonemapDrago",
    sizeof(pyopencv_TonemapDrago_t),
};

static void pyopencv_TonemapDrago_dealloc(PyObject* self)
{
    ((pyopencv_TonemapDrago_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TonemapDrago>& r)
{
    pyopencv_TonemapDrago_t *m = PyObject_NEW(pyopencv_TonemapDrago_t, &pyopencv_TonemapDrago_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TonemapDrago>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TonemapDrago_Type))
    {
        failmsg("Expected cv::TonemapDrago for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TonemapDrago_t*)src)->v.dynamicCast<cv::TonemapDrago>();
    return true;
}


struct pyopencv_TonemapDurand_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TonemapDurand_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TonemapDurand",
    sizeof(pyopencv_TonemapDurand_t),
};

static void pyopencv_TonemapDurand_dealloc(PyObject* self)
{
    ((pyopencv_TonemapDurand_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TonemapDurand>& r)
{
    pyopencv_TonemapDurand_t *m = PyObject_NEW(pyopencv_TonemapDurand_t, &pyopencv_TonemapDurand_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TonemapDurand>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TonemapDurand_Type))
    {
        failmsg("Expected cv::TonemapDurand for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TonemapDurand_t*)src)->v.dynamicCast<cv::TonemapDurand>();
    return true;
}


struct pyopencv_TonemapMantiuk_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TonemapMantiuk_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TonemapMantiuk",
    sizeof(pyopencv_TonemapMantiuk_t),
};

static void pyopencv_TonemapMantiuk_dealloc(PyObject* self)
{
    ((pyopencv_TonemapMantiuk_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TonemapMantiuk>& r)
{
    pyopencv_TonemapMantiuk_t *m = PyObject_NEW(pyopencv_TonemapMantiuk_t, &pyopencv_TonemapMantiuk_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TonemapMantiuk>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TonemapMantiuk_Type))
    {
        failmsg("Expected cv::TonemapMantiuk for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TonemapMantiuk_t*)src)->v.dynamicCast<cv::TonemapMantiuk>();
    return true;
}


struct pyopencv_TonemapReinhard_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TonemapReinhard_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TonemapReinhard",
    sizeof(pyopencv_TonemapReinhard_t),
};

static void pyopencv_TonemapReinhard_dealloc(PyObject* self)
{
    ((pyopencv_TonemapReinhard_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TonemapReinhard>& r)
{
    pyopencv_TonemapReinhard_t *m = PyObject_NEW(pyopencv_TonemapReinhard_t, &pyopencv_TonemapReinhard_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TonemapReinhard>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TonemapReinhard_Type))
    {
        failmsg("Expected cv::TonemapReinhard for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TonemapReinhard_t*)src)->v.dynamicCast<cv::TonemapReinhard>();
    return true;
}


struct pyopencv_Tracker_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_Tracker_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".Tracker",
    sizeof(pyopencv_Tracker_t),
};

static void pyopencv_Tracker_dealloc(PyObject* self)
{
    ((pyopencv_Tracker_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::Tracker>& r)
{
    pyopencv_Tracker_t *m = PyObject_NEW(pyopencv_Tracker_t, &pyopencv_Tracker_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::Tracker>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_Tracker_Type))
    {
        failmsg("Expected cv::Tracker for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_Tracker_t*)src)->v.dynamicCast<cv::Tracker>();
    return true;
}


struct pyopencv_TrackerBoosting_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerBoosting_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerBoosting",
    sizeof(pyopencv_TrackerBoosting_t),
};

static void pyopencv_TrackerBoosting_dealloc(PyObject* self)
{
    ((pyopencv_TrackerBoosting_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerBoosting>& r)
{
    pyopencv_TrackerBoosting_t *m = PyObject_NEW(pyopencv_TrackerBoosting_t, &pyopencv_TrackerBoosting_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerBoosting>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerBoosting_Type))
    {
        failmsg("Expected cv::TrackerBoosting for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerBoosting_t*)src)->v.dynamicCast<cv::TrackerBoosting>();
    return true;
}


struct pyopencv_TrackerCSRT_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerCSRT_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerCSRT",
    sizeof(pyopencv_TrackerCSRT_t),
};

static void pyopencv_TrackerCSRT_dealloc(PyObject* self)
{
    ((pyopencv_TrackerCSRT_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerCSRT>& r)
{
    pyopencv_TrackerCSRT_t *m = PyObject_NEW(pyopencv_TrackerCSRT_t, &pyopencv_TrackerCSRT_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerCSRT>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerCSRT_Type))
    {
        failmsg("Expected cv::TrackerCSRT for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerCSRT_t*)src)->v.dynamicCast<cv::TrackerCSRT>();
    return true;
}


struct pyopencv_TrackerGOTURN_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerGOTURN_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerGOTURN",
    sizeof(pyopencv_TrackerGOTURN_t),
};

static void pyopencv_TrackerGOTURN_dealloc(PyObject* self)
{
    ((pyopencv_TrackerGOTURN_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerGOTURN>& r)
{
    pyopencv_TrackerGOTURN_t *m = PyObject_NEW(pyopencv_TrackerGOTURN_t, &pyopencv_TrackerGOTURN_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerGOTURN>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerGOTURN_Type))
    {
        failmsg("Expected cv::TrackerGOTURN for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerGOTURN_t*)src)->v.dynamicCast<cv::TrackerGOTURN>();
    return true;
}


struct pyopencv_TrackerKCF_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerKCF_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerKCF",
    sizeof(pyopencv_TrackerKCF_t),
};

static void pyopencv_TrackerKCF_dealloc(PyObject* self)
{
    ((pyopencv_TrackerKCF_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerKCF>& r)
{
    pyopencv_TrackerKCF_t *m = PyObject_NEW(pyopencv_TrackerKCF_t, &pyopencv_TrackerKCF_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerKCF>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerKCF_Type))
    {
        failmsg("Expected cv::TrackerKCF for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerKCF_t*)src)->v.dynamicCast<cv::TrackerKCF>();
    return true;
}


struct pyopencv_TrackerMIL_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerMIL_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerMIL",
    sizeof(pyopencv_TrackerMIL_t),
};

static void pyopencv_TrackerMIL_dealloc(PyObject* self)
{
    ((pyopencv_TrackerMIL_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerMIL>& r)
{
    pyopencv_TrackerMIL_t *m = PyObject_NEW(pyopencv_TrackerMIL_t, &pyopencv_TrackerMIL_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerMIL>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerMIL_Type))
    {
        failmsg("Expected cv::TrackerMIL for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerMIL_t*)src)->v.dynamicCast<cv::TrackerMIL>();
    return true;
}


struct pyopencv_TrackerMOSSE_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerMOSSE_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerMOSSE",
    sizeof(pyopencv_TrackerMOSSE_t),
};

static void pyopencv_TrackerMOSSE_dealloc(PyObject* self)
{
    ((pyopencv_TrackerMOSSE_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerMOSSE>& r)
{
    pyopencv_TrackerMOSSE_t *m = PyObject_NEW(pyopencv_TrackerMOSSE_t, &pyopencv_TrackerMOSSE_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerMOSSE>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerMOSSE_Type))
    {
        failmsg("Expected cv::TrackerMOSSE for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerMOSSE_t*)src)->v.dynamicCast<cv::TrackerMOSSE>();
    return true;
}


struct pyopencv_TrackerMedianFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerMedianFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerMedianFlow",
    sizeof(pyopencv_TrackerMedianFlow_t),
};

static void pyopencv_TrackerMedianFlow_dealloc(PyObject* self)
{
    ((pyopencv_TrackerMedianFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerMedianFlow>& r)
{
    pyopencv_TrackerMedianFlow_t *m = PyObject_NEW(pyopencv_TrackerMedianFlow_t, &pyopencv_TrackerMedianFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerMedianFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerMedianFlow_Type))
    {
        failmsg("Expected cv::TrackerMedianFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerMedianFlow_t*)src)->v.dynamicCast<cv::TrackerMedianFlow>();
    return true;
}


struct pyopencv_TrackerTLD_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_TrackerTLD_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".TrackerTLD",
    sizeof(pyopencv_TrackerTLD_t),
};

static void pyopencv_TrackerTLD_dealloc(PyObject* self)
{
    ((pyopencv_TrackerTLD_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::TrackerTLD>& r)
{
    pyopencv_TrackerTLD_t *m = PyObject_NEW(pyopencv_TrackerTLD_t, &pyopencv_TrackerTLD_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::TrackerTLD>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_TrackerTLD_Type))
    {
        failmsg("Expected cv::TrackerTLD for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_TrackerTLD_t*)src)->v.dynamicCast<cv::TrackerTLD>();
    return true;
}


struct pyopencv_VideoCapture_t
{
    PyObject_HEAD
    Ptr<cv::VideoCapture> v;
};

static PyTypeObject pyopencv_VideoCapture_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".VideoCapture",
    sizeof(pyopencv_VideoCapture_t),
};

static void pyopencv_VideoCapture_dealloc(PyObject* self)
{
    ((pyopencv_VideoCapture_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::VideoCapture>& r)
{
    pyopencv_VideoCapture_t *m = PyObject_NEW(pyopencv_VideoCapture_t, &pyopencv_VideoCapture_Type);
    new (&(m->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::VideoCapture>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_VideoCapture_Type))
    {
        failmsg("Expected cv::VideoCapture for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_VideoCapture_t*)src)->v.dynamicCast<cv::VideoCapture>();
    return true;
}


struct pyopencv_VideoWriter_t
{
    PyObject_HEAD
    Ptr<cv::VideoWriter> v;
};

static PyTypeObject pyopencv_VideoWriter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".VideoWriter",
    sizeof(pyopencv_VideoWriter_t),
};

static void pyopencv_VideoWriter_dealloc(PyObject* self)
{
    ((pyopencv_VideoWriter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::VideoWriter>& r)
{
    pyopencv_VideoWriter_t *m = PyObject_NEW(pyopencv_VideoWriter_t, &pyopencv_VideoWriter_Type);
    new (&(m->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::VideoWriter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_VideoWriter_Type))
    {
        failmsg("Expected cv::VideoWriter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_VideoWriter_t*)src)->v.dynamicCast<cv::VideoWriter>();
    return true;
}


struct pyopencv_aruco_Board_t
{
    PyObject_HEAD
    Ptr<cv::aruco::Board> v;
};

static PyTypeObject pyopencv_aruco_Board_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".aruco_Board",
    sizeof(pyopencv_aruco_Board_t),
};

static void pyopencv_aruco_Board_dealloc(PyObject* self)
{
    ((pyopencv_aruco_Board_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::aruco::Board>& r)
{
    pyopencv_aruco_Board_t *m = PyObject_NEW(pyopencv_aruco_Board_t, &pyopencv_aruco_Board_Type);
    new (&(m->v)) Ptr<cv::aruco::Board>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::aruco::Board>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_aruco_Board_Type))
    {
        failmsg("Expected cv::aruco::Board for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_aruco_Board_t*)src)->v.dynamicCast<cv::aruco::Board>();
    return true;
}


struct pyopencv_aruco_CharucoBoard_t
{
    PyObject_HEAD
    Ptr<cv::aruco::CharucoBoard> v;
};

static PyTypeObject pyopencv_aruco_CharucoBoard_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".aruco_CharucoBoard",
    sizeof(pyopencv_aruco_CharucoBoard_t),
};

static void pyopencv_aruco_CharucoBoard_dealloc(PyObject* self)
{
    ((pyopencv_aruco_CharucoBoard_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::aruco::CharucoBoard>& r)
{
    pyopencv_aruco_CharucoBoard_t *m = PyObject_NEW(pyopencv_aruco_CharucoBoard_t, &pyopencv_aruco_CharucoBoard_Type);
    new (&(m->v)) Ptr<cv::aruco::CharucoBoard>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::aruco::CharucoBoard>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_aruco_CharucoBoard_Type))
    {
        failmsg("Expected cv::aruco::CharucoBoard for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_aruco_CharucoBoard_t*)src)->v.dynamicCast<cv::aruco::CharucoBoard>();
    return true;
}


struct pyopencv_aruco_DetectorParameters_t
{
    PyObject_HEAD
    Ptr<cv::aruco::DetectorParameters> v;
};

static PyTypeObject pyopencv_aruco_DetectorParameters_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".aruco_DetectorParameters",
    sizeof(pyopencv_aruco_DetectorParameters_t),
};

static void pyopencv_aruco_DetectorParameters_dealloc(PyObject* self)
{
    ((pyopencv_aruco_DetectorParameters_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::aruco::DetectorParameters>& r)
{
    pyopencv_aruco_DetectorParameters_t *m = PyObject_NEW(pyopencv_aruco_DetectorParameters_t, &pyopencv_aruco_DetectorParameters_Type);
    new (&(m->v)) Ptr<cv::aruco::DetectorParameters>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::aruco::DetectorParameters>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_aruco_DetectorParameters_Type))
    {
        failmsg("Expected cv::aruco::DetectorParameters for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_aruco_DetectorParameters_t*)src)->v.dynamicCast<cv::aruco::DetectorParameters>();
    return true;
}


struct pyopencv_aruco_Dictionary_t
{
    PyObject_HEAD
    Ptr<cv::aruco::Dictionary> v;
};

static PyTypeObject pyopencv_aruco_Dictionary_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".aruco_Dictionary",
    sizeof(pyopencv_aruco_Dictionary_t),
};

static void pyopencv_aruco_Dictionary_dealloc(PyObject* self)
{
    ((pyopencv_aruco_Dictionary_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::aruco::Dictionary>& r)
{
    pyopencv_aruco_Dictionary_t *m = PyObject_NEW(pyopencv_aruco_Dictionary_t, &pyopencv_aruco_Dictionary_Type);
    new (&(m->v)) Ptr<cv::aruco::Dictionary>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::aruco::Dictionary>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_aruco_Dictionary_Type))
    {
        failmsg("Expected cv::aruco::Dictionary for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_aruco_Dictionary_t*)src)->v.dynamicCast<cv::aruco::Dictionary>();
    return true;
}


struct pyopencv_aruco_GridBoard_t
{
    PyObject_HEAD
    Ptr<cv::aruco::GridBoard> v;
};

static PyTypeObject pyopencv_aruco_GridBoard_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".aruco_GridBoard",
    sizeof(pyopencv_aruco_GridBoard_t),
};

static void pyopencv_aruco_GridBoard_dealloc(PyObject* self)
{
    ((pyopencv_aruco_GridBoard_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::aruco::GridBoard>& r)
{
    pyopencv_aruco_GridBoard_t *m = PyObject_NEW(pyopencv_aruco_GridBoard_t, &pyopencv_aruco_GridBoard_Type);
    new (&(m->v)) Ptr<cv::aruco::GridBoard>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::aruco::GridBoard>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_aruco_GridBoard_Type))
    {
        failmsg("Expected cv::aruco::GridBoard for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_aruco_GridBoard_t*)src)->v.dynamicCast<cv::aruco::GridBoard>();
    return true;
}


struct pyopencv_bgsegm_BackgroundSubtractorCNT_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bgsegm_BackgroundSubtractorCNT_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_BackgroundSubtractorCNT",
    sizeof(pyopencv_bgsegm_BackgroundSubtractorCNT_t),
};

static void pyopencv_bgsegm_BackgroundSubtractorCNT_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::BackgroundSubtractorCNT>& r)
{
    pyopencv_bgsegm_BackgroundSubtractorCNT_t *m = PyObject_NEW(pyopencv_bgsegm_BackgroundSubtractorCNT_t, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::BackgroundSubtractorCNT>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
    {
        failmsg("Expected cv::bgsegm::BackgroundSubtractorCNT for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)src)->v.dynamicCast<cv::bgsegm::BackgroundSubtractorCNT>();
    return true;
}


struct pyopencv_bgsegm_BackgroundSubtractorGMG_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bgsegm_BackgroundSubtractorGMG_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_BackgroundSubtractorGMG",
    sizeof(pyopencv_bgsegm_BackgroundSubtractorGMG_t),
};

static void pyopencv_bgsegm_BackgroundSubtractorGMG_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::BackgroundSubtractorGMG>& r)
{
    pyopencv_bgsegm_BackgroundSubtractorGMG_t *m = PyObject_NEW(pyopencv_bgsegm_BackgroundSubtractorGMG_t, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::BackgroundSubtractorGMG>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
    {
        failmsg("Expected cv::bgsegm::BackgroundSubtractorGMG for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)src)->v.dynamicCast<cv::bgsegm::BackgroundSubtractorGMG>();
    return true;
}


struct pyopencv_bgsegm_BackgroundSubtractorGSOC_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bgsegm_BackgroundSubtractorGSOC_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_BackgroundSubtractorGSOC",
    sizeof(pyopencv_bgsegm_BackgroundSubtractorGSOC_t),
};

static void pyopencv_bgsegm_BackgroundSubtractorGSOC_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_BackgroundSubtractorGSOC_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::BackgroundSubtractorGSOC>& r)
{
    pyopencv_bgsegm_BackgroundSubtractorGSOC_t *m = PyObject_NEW(pyopencv_bgsegm_BackgroundSubtractorGSOC_t, &pyopencv_bgsegm_BackgroundSubtractorGSOC_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::BackgroundSubtractorGSOC>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_BackgroundSubtractorGSOC_Type))
    {
        failmsg("Expected cv::bgsegm::BackgroundSubtractorGSOC for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_BackgroundSubtractorGSOC_t*)src)->v.dynamicCast<cv::bgsegm::BackgroundSubtractorGSOC>();
    return true;
}


struct pyopencv_bgsegm_BackgroundSubtractorLSBP_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bgsegm_BackgroundSubtractorLSBP_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_BackgroundSubtractorLSBP",
    sizeof(pyopencv_bgsegm_BackgroundSubtractorLSBP_t),
};

static void pyopencv_bgsegm_BackgroundSubtractorLSBP_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_BackgroundSubtractorLSBP_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::BackgroundSubtractorLSBP>& r)
{
    pyopencv_bgsegm_BackgroundSubtractorLSBP_t *m = PyObject_NEW(pyopencv_bgsegm_BackgroundSubtractorLSBP_t, &pyopencv_bgsegm_BackgroundSubtractorLSBP_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::BackgroundSubtractorLSBP>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_BackgroundSubtractorLSBP_Type))
    {
        failmsg("Expected cv::bgsegm::BackgroundSubtractorLSBP for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_BackgroundSubtractorLSBP_t*)src)->v.dynamicCast<cv::bgsegm::BackgroundSubtractorLSBP>();
    return true;
}


struct pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_t
{
    PyObject_HEAD
    Ptr<cv::bgsegm::BackgroundSubtractorLSBPDesc> v;
};

static PyTypeObject pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_BackgroundSubtractorLSBPDesc",
    sizeof(pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_t),
};

static void pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::BackgroundSubtractorLSBPDesc>& r)
{
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_t *m = PyObject_NEW(pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_t, &pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type);
    new (&(m->v)) Ptr<cv::bgsegm::BackgroundSubtractorLSBPDesc>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::BackgroundSubtractorLSBPDesc>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type))
    {
        failmsg("Expected cv::bgsegm::BackgroundSubtractorLSBPDesc for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_t*)src)->v.dynamicCast<cv::bgsegm::BackgroundSubtractorLSBPDesc>();
    return true;
}


struct pyopencv_bgsegm_BackgroundSubtractorMOG_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bgsegm_BackgroundSubtractorMOG_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_BackgroundSubtractorMOG",
    sizeof(pyopencv_bgsegm_BackgroundSubtractorMOG_t),
};

static void pyopencv_bgsegm_BackgroundSubtractorMOG_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::BackgroundSubtractorMOG>& r)
{
    pyopencv_bgsegm_BackgroundSubtractorMOG_t *m = PyObject_NEW(pyopencv_bgsegm_BackgroundSubtractorMOG_t, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::BackgroundSubtractorMOG>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
    {
        failmsg("Expected cv::bgsegm::BackgroundSubtractorMOG for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)src)->v.dynamicCast<cv::bgsegm::BackgroundSubtractorMOG>();
    return true;
}


struct pyopencv_bgsegm_SyntheticSequenceGenerator_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bgsegm_SyntheticSequenceGenerator_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bgsegm_SyntheticSequenceGenerator",
    sizeof(pyopencv_bgsegm_SyntheticSequenceGenerator_t),
};

static void pyopencv_bgsegm_SyntheticSequenceGenerator_dealloc(PyObject* self)
{
    ((pyopencv_bgsegm_SyntheticSequenceGenerator_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bgsegm::SyntheticSequenceGenerator>& r)
{
    pyopencv_bgsegm_SyntheticSequenceGenerator_t *m = PyObject_NEW(pyopencv_bgsegm_SyntheticSequenceGenerator_t, &pyopencv_bgsegm_SyntheticSequenceGenerator_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bgsegm::SyntheticSequenceGenerator>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bgsegm_SyntheticSequenceGenerator_Type))
    {
        failmsg("Expected cv::bgsegm::SyntheticSequenceGenerator for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bgsegm_SyntheticSequenceGenerator_t*)src)->v.dynamicCast<cv::bgsegm::SyntheticSequenceGenerator>();
    return true;
}


struct pyopencv_bioinspired_Retina_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bioinspired_Retina_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bioinspired_Retina",
    sizeof(pyopencv_bioinspired_Retina_t),
};

static void pyopencv_bioinspired_Retina_dealloc(PyObject* self)
{
    ((pyopencv_bioinspired_Retina_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bioinspired::Retina>& r)
{
    pyopencv_bioinspired_Retina_t *m = PyObject_NEW(pyopencv_bioinspired_Retina_t, &pyopencv_bioinspired_Retina_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bioinspired::Retina>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bioinspired_Retina_Type))
    {
        failmsg("Expected cv::bioinspired::Retina for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bioinspired_Retina_t*)src)->v.dynamicCast<cv::bioinspired::Retina>();
    return true;
}


struct pyopencv_bioinspired_RetinaFastToneMapping_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bioinspired_RetinaFastToneMapping_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bioinspired_RetinaFastToneMapping",
    sizeof(pyopencv_bioinspired_RetinaFastToneMapping_t),
};

static void pyopencv_bioinspired_RetinaFastToneMapping_dealloc(PyObject* self)
{
    ((pyopencv_bioinspired_RetinaFastToneMapping_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bioinspired::RetinaFastToneMapping>& r)
{
    pyopencv_bioinspired_RetinaFastToneMapping_t *m = PyObject_NEW(pyopencv_bioinspired_RetinaFastToneMapping_t, &pyopencv_bioinspired_RetinaFastToneMapping_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bioinspired::RetinaFastToneMapping>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bioinspired_RetinaFastToneMapping_Type))
    {
        failmsg("Expected cv::bioinspired::RetinaFastToneMapping for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bioinspired_RetinaFastToneMapping_t*)src)->v.dynamicCast<cv::bioinspired::RetinaFastToneMapping>();
    return true;
}


struct pyopencv_bioinspired_TransientAreasSegmentationModule_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_bioinspired_TransientAreasSegmentationModule_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".bioinspired_TransientAreasSegmentationModule",
    sizeof(pyopencv_bioinspired_TransientAreasSegmentationModule_t),
};

static void pyopencv_bioinspired_TransientAreasSegmentationModule_dealloc(PyObject* self)
{
    ((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::bioinspired::TransientAreasSegmentationModule>& r)
{
    pyopencv_bioinspired_TransientAreasSegmentationModule_t *m = PyObject_NEW(pyopencv_bioinspired_TransientAreasSegmentationModule_t, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::bioinspired::TransientAreasSegmentationModule>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
    {
        failmsg("Expected cv::bioinspired::TransientAreasSegmentationModule for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)src)->v.dynamicCast<cv::bioinspired::TransientAreasSegmentationModule>();
    return true;
}


struct pyopencv_dnn_DictValue_t
{
    PyObject_HEAD
    Ptr<cv::dnn::DictValue> v;
};

static PyTypeObject pyopencv_dnn_DictValue_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".dnn_DictValue",
    sizeof(pyopencv_dnn_DictValue_t),
};

static void pyopencv_dnn_DictValue_dealloc(PyObject* self)
{
    ((pyopencv_dnn_DictValue_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::dnn::DictValue>& r)
{
    pyopencv_dnn_DictValue_t *m = PyObject_NEW(pyopencv_dnn_DictValue_t, &pyopencv_dnn_DictValue_Type);
    new (&(m->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::dnn::DictValue>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_dnn_DictValue_Type))
    {
        failmsg("Expected cv::dnn::DictValue for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_dnn_DictValue_t*)src)->v.dynamicCast<cv::dnn::DictValue>();
    return true;
}


struct pyopencv_dnn_Layer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_dnn_Layer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".dnn_Layer",
    sizeof(pyopencv_dnn_Layer_t),
};

static void pyopencv_dnn_Layer_dealloc(PyObject* self)
{
    ((pyopencv_dnn_Layer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::dnn::Layer>& r)
{
    pyopencv_dnn_Layer_t *m = PyObject_NEW(pyopencv_dnn_Layer_t, &pyopencv_dnn_Layer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::dnn::Layer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_dnn_Layer_Type))
    {
        failmsg("Expected cv::dnn::Layer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_dnn_Layer_t*)src)->v.dynamicCast<cv::dnn::Layer>();
    return true;
}


struct pyopencv_dnn_Net_t
{
    PyObject_HEAD
    cv::dnn::Net v;
};

static PyTypeObject pyopencv_dnn_Net_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".dnn_Net",
    sizeof(pyopencv_dnn_Net_t),
};

static void pyopencv_dnn_Net_dealloc(PyObject* self)
{
    ((pyopencv_dnn_Net_t*)self)->v.cv::dnn::Net::~Net();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::dnn::Net& r)
{
    pyopencv_dnn_Net_t *m = PyObject_NEW(pyopencv_dnn_Net_t, &pyopencv_dnn_Net_Type);
    new (&m->v) cv::dnn::Net(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::dnn::Net& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_dnn_Net_Type))
    {
        failmsg("Expected cv::dnn::Net for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_dnn_Net_t*)src)->v;
    return true;
}

struct pyopencv_dpm_DPMDetector_t
{
    PyObject_HEAD
    Ptr<cv::dpm::DPMDetector> v;
};

static PyTypeObject pyopencv_dpm_DPMDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".dpm_DPMDetector",
    sizeof(pyopencv_dpm_DPMDetector_t),
};

static void pyopencv_dpm_DPMDetector_dealloc(PyObject* self)
{
    ((pyopencv_dpm_DPMDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::dpm::DPMDetector>& r)
{
    pyopencv_dpm_DPMDetector_t *m = PyObject_NEW(pyopencv_dpm_DPMDetector_t, &pyopencv_dpm_DPMDetector_Type);
    new (&(m->v)) Ptr<cv::dpm::DPMDetector>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::dpm::DPMDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_dpm_DPMDetector_Type))
    {
        failmsg("Expected cv::dpm::DPMDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_dpm_DPMDetector_t*)src)->v.dynamicCast<cv::dpm::DPMDetector>();
    return true;
}


struct pyopencv_dpm_DPMDetector_ObjectDetection_t
{
    PyObject_HEAD
    Ptr<cv::dpm::DPMDetector::ObjectDetection> v;
};

static PyTypeObject pyopencv_dpm_DPMDetector_ObjectDetection_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".dpm_DPMDetector_ObjectDetection",
    sizeof(pyopencv_dpm_DPMDetector_ObjectDetection_t),
};

static void pyopencv_dpm_DPMDetector_ObjectDetection_dealloc(PyObject* self)
{
    ((pyopencv_dpm_DPMDetector_ObjectDetection_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::dpm::DPMDetector::ObjectDetection>& r)
{
    pyopencv_dpm_DPMDetector_ObjectDetection_t *m = PyObject_NEW(pyopencv_dpm_DPMDetector_ObjectDetection_t, &pyopencv_dpm_DPMDetector_ObjectDetection_Type);
    new (&(m->v)) Ptr<cv::dpm::DPMDetector::ObjectDetection>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::dpm::DPMDetector::ObjectDetection>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_dpm_DPMDetector_ObjectDetection_Type))
    {
        failmsg("Expected cv::dpm::DPMDetector::ObjectDetection for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_dpm_DPMDetector_ObjectDetection_t*)src)->v.dynamicCast<cv::dpm::DPMDetector::ObjectDetection>();
    return true;
}


struct pyopencv_face_BIF_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_BIF_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_BIF",
    sizeof(pyopencv_face_BIF_t),
};

static void pyopencv_face_BIF_dealloc(PyObject* self)
{
    ((pyopencv_face_BIF_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::BIF>& r)
{
    pyopencv_face_BIF_t *m = PyObject_NEW(pyopencv_face_BIF_t, &pyopencv_face_BIF_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::BIF>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_BIF_Type))
    {
        failmsg("Expected cv::face::BIF for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_BIF_t*)src)->v.dynamicCast<cv::face::BIF>();
    return true;
}


struct pyopencv_face_BasicFaceRecognizer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_BasicFaceRecognizer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_BasicFaceRecognizer",
    sizeof(pyopencv_face_BasicFaceRecognizer_t),
};

static void pyopencv_face_BasicFaceRecognizer_dealloc(PyObject* self)
{
    ((pyopencv_face_BasicFaceRecognizer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::BasicFaceRecognizer>& r)
{
    pyopencv_face_BasicFaceRecognizer_t *m = PyObject_NEW(pyopencv_face_BasicFaceRecognizer_t, &pyopencv_face_BasicFaceRecognizer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::BasicFaceRecognizer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_BasicFaceRecognizer_Type))
    {
        failmsg("Expected cv::face::BasicFaceRecognizer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_BasicFaceRecognizer_t*)src)->v.dynamicCast<cv::face::BasicFaceRecognizer>();
    return true;
}


struct pyopencv_face_EigenFaceRecognizer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_EigenFaceRecognizer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_EigenFaceRecognizer",
    sizeof(pyopencv_face_EigenFaceRecognizer_t),
};

static void pyopencv_face_EigenFaceRecognizer_dealloc(PyObject* self)
{
    ((pyopencv_face_EigenFaceRecognizer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::EigenFaceRecognizer>& r)
{
    pyopencv_face_EigenFaceRecognizer_t *m = PyObject_NEW(pyopencv_face_EigenFaceRecognizer_t, &pyopencv_face_EigenFaceRecognizer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::EigenFaceRecognizer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_EigenFaceRecognizer_Type))
    {
        failmsg("Expected cv::face::EigenFaceRecognizer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_EigenFaceRecognizer_t*)src)->v.dynamicCast<cv::face::EigenFaceRecognizer>();
    return true;
}


struct pyopencv_face_FaceRecognizer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_FaceRecognizer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_FaceRecognizer",
    sizeof(pyopencv_face_FaceRecognizer_t),
};

static void pyopencv_face_FaceRecognizer_dealloc(PyObject* self)
{
    ((pyopencv_face_FaceRecognizer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::FaceRecognizer>& r)
{
    pyopencv_face_FaceRecognizer_t *m = PyObject_NEW(pyopencv_face_FaceRecognizer_t, &pyopencv_face_FaceRecognizer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::FaceRecognizer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_FaceRecognizer_Type))
    {
        failmsg("Expected cv::face::FaceRecognizer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_FaceRecognizer_t*)src)->v.dynamicCast<cv::face::FaceRecognizer>();
    return true;
}


struct pyopencv_face_Facemark_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_Facemark_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_Facemark",
    sizeof(pyopencv_face_Facemark_t),
};

static void pyopencv_face_Facemark_dealloc(PyObject* self)
{
    ((pyopencv_face_Facemark_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::Facemark>& r)
{
    pyopencv_face_Facemark_t *m = PyObject_NEW(pyopencv_face_Facemark_t, &pyopencv_face_Facemark_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::Facemark>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_Facemark_Type))
    {
        failmsg("Expected cv::face::Facemark for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_Facemark_t*)src)->v.dynamicCast<cv::face::Facemark>();
    return true;
}


struct pyopencv_face_FacemarkAAM_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_FacemarkAAM_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_FacemarkAAM",
    sizeof(pyopencv_face_FacemarkAAM_t),
};

static void pyopencv_face_FacemarkAAM_dealloc(PyObject* self)
{
    ((pyopencv_face_FacemarkAAM_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::FacemarkAAM>& r)
{
    pyopencv_face_FacemarkAAM_t *m = PyObject_NEW(pyopencv_face_FacemarkAAM_t, &pyopencv_face_FacemarkAAM_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::FacemarkAAM>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_FacemarkAAM_Type))
    {
        failmsg("Expected cv::face::FacemarkAAM for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_FacemarkAAM_t*)src)->v.dynamicCast<cv::face::FacemarkAAM>();
    return true;
}


struct pyopencv_face_FacemarkKazemi_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_FacemarkKazemi_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_FacemarkKazemi",
    sizeof(pyopencv_face_FacemarkKazemi_t),
};

static void pyopencv_face_FacemarkKazemi_dealloc(PyObject* self)
{
    ((pyopencv_face_FacemarkKazemi_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::FacemarkKazemi>& r)
{
    pyopencv_face_FacemarkKazemi_t *m = PyObject_NEW(pyopencv_face_FacemarkKazemi_t, &pyopencv_face_FacemarkKazemi_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::FacemarkKazemi>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_FacemarkKazemi_Type))
    {
        failmsg("Expected cv::face::FacemarkKazemi for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_FacemarkKazemi_t*)src)->v.dynamicCast<cv::face::FacemarkKazemi>();
    return true;
}


struct pyopencv_face_FacemarkLBF_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_FacemarkLBF_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_FacemarkLBF",
    sizeof(pyopencv_face_FacemarkLBF_t),
};

static void pyopencv_face_FacemarkLBF_dealloc(PyObject* self)
{
    ((pyopencv_face_FacemarkLBF_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::FacemarkLBF>& r)
{
    pyopencv_face_FacemarkLBF_t *m = PyObject_NEW(pyopencv_face_FacemarkLBF_t, &pyopencv_face_FacemarkLBF_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::FacemarkLBF>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_FacemarkLBF_Type))
    {
        failmsg("Expected cv::face::FacemarkLBF for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_FacemarkLBF_t*)src)->v.dynamicCast<cv::face::FacemarkLBF>();
    return true;
}


struct pyopencv_face_FacemarkTrain_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_FacemarkTrain_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_FacemarkTrain",
    sizeof(pyopencv_face_FacemarkTrain_t),
};

static void pyopencv_face_FacemarkTrain_dealloc(PyObject* self)
{
    ((pyopencv_face_FacemarkTrain_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::FacemarkTrain>& r)
{
    pyopencv_face_FacemarkTrain_t *m = PyObject_NEW(pyopencv_face_FacemarkTrain_t, &pyopencv_face_FacemarkTrain_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::FacemarkTrain>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_FacemarkTrain_Type))
    {
        failmsg("Expected cv::face::FacemarkTrain for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_FacemarkTrain_t*)src)->v.dynamicCast<cv::face::FacemarkTrain>();
    return true;
}


struct pyopencv_face_FisherFaceRecognizer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_FisherFaceRecognizer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_FisherFaceRecognizer",
    sizeof(pyopencv_face_FisherFaceRecognizer_t),
};

static void pyopencv_face_FisherFaceRecognizer_dealloc(PyObject* self)
{
    ((pyopencv_face_FisherFaceRecognizer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::FisherFaceRecognizer>& r)
{
    pyopencv_face_FisherFaceRecognizer_t *m = PyObject_NEW(pyopencv_face_FisherFaceRecognizer_t, &pyopencv_face_FisherFaceRecognizer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::FisherFaceRecognizer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_FisherFaceRecognizer_Type))
    {
        failmsg("Expected cv::face::FisherFaceRecognizer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_FisherFaceRecognizer_t*)src)->v.dynamicCast<cv::face::FisherFaceRecognizer>();
    return true;
}


struct pyopencv_face_LBPHFaceRecognizer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_LBPHFaceRecognizer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_LBPHFaceRecognizer",
    sizeof(pyopencv_face_LBPHFaceRecognizer_t),
};

static void pyopencv_face_LBPHFaceRecognizer_dealloc(PyObject* self)
{
    ((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::LBPHFaceRecognizer>& r)
{
    pyopencv_face_LBPHFaceRecognizer_t *m = PyObject_NEW(pyopencv_face_LBPHFaceRecognizer_t, &pyopencv_face_LBPHFaceRecognizer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::LBPHFaceRecognizer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_LBPHFaceRecognizer_Type))
    {
        failmsg("Expected cv::face::LBPHFaceRecognizer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_LBPHFaceRecognizer_t*)src)->v.dynamicCast<cv::face::LBPHFaceRecognizer>();
    return true;
}


struct pyopencv_face_MACE_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_face_MACE_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_MACE",
    sizeof(pyopencv_face_MACE_t),
};

static void pyopencv_face_MACE_dealloc(PyObject* self)
{
    ((pyopencv_face_MACE_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::MACE>& r)
{
    pyopencv_face_MACE_t *m = PyObject_NEW(pyopencv_face_MACE_t, &pyopencv_face_MACE_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::MACE>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_MACE_Type))
    {
        failmsg("Expected cv::face::MACE for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_MACE_t*)src)->v.dynamicCast<cv::face::MACE>();
    return true;
}


struct pyopencv_face_PredictCollector_t
{
    PyObject_HEAD
    Ptr<cv::face::PredictCollector> v;
};

static PyTypeObject pyopencv_face_PredictCollector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_PredictCollector",
    sizeof(pyopencv_face_PredictCollector_t),
};

static void pyopencv_face_PredictCollector_dealloc(PyObject* self)
{
    ((pyopencv_face_PredictCollector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::PredictCollector>& r)
{
    pyopencv_face_PredictCollector_t *m = PyObject_NEW(pyopencv_face_PredictCollector_t, &pyopencv_face_PredictCollector_Type);
    new (&(m->v)) Ptr<cv::face::PredictCollector>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::PredictCollector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_PredictCollector_Type))
    {
        failmsg("Expected cv::face::PredictCollector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_PredictCollector_t*)src)->v.dynamicCast<cv::face::PredictCollector>();
    return true;
}


struct pyopencv_face_StandardCollector_t
{
    PyObject_HEAD
    Ptr<cv::face::StandardCollector> v;
};

static PyTypeObject pyopencv_face_StandardCollector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".face_StandardCollector",
    sizeof(pyopencv_face_StandardCollector_t),
};

static void pyopencv_face_StandardCollector_dealloc(PyObject* self)
{
    ((pyopencv_face_StandardCollector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::face::StandardCollector>& r)
{
    pyopencv_face_StandardCollector_t *m = PyObject_NEW(pyopencv_face_StandardCollector_t, &pyopencv_face_StandardCollector_Type);
    new (&(m->v)) Ptr<cv::face::StandardCollector>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::face::StandardCollector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_face_StandardCollector_Type))
    {
        failmsg("Expected cv::face::StandardCollector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_face_StandardCollector_t*)src)->v.dynamicCast<cv::face::StandardCollector>();
    return true;
}


struct pyopencv_flann_Index_t
{
    PyObject_HEAD
    Ptr<cv::flann::Index> v;
};

static PyTypeObject pyopencv_flann_Index_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".flann_Index",
    sizeof(pyopencv_flann_Index_t),
};

static void pyopencv_flann_Index_dealloc(PyObject* self)
{
    ((pyopencv_flann_Index_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::flann::Index>& r)
{
    pyopencv_flann_Index_t *m = PyObject_NEW(pyopencv_flann_Index_t, &pyopencv_flann_Index_Type);
    new (&(m->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::flann::Index>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_flann_Index_Type))
    {
        failmsg("Expected cv::flann::Index for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_flann_Index_t*)src)->v.dynamicCast<cv::flann::Index>();
    return true;
}


struct pyopencv_hfs_HfsSegment_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_hfs_HfsSegment_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".hfs_HfsSegment",
    sizeof(pyopencv_hfs_HfsSegment_t),
};

static void pyopencv_hfs_HfsSegment_dealloc(PyObject* self)
{
    ((pyopencv_hfs_HfsSegment_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::hfs::HfsSegment>& r)
{
    pyopencv_hfs_HfsSegment_t *m = PyObject_NEW(pyopencv_hfs_HfsSegment_t, &pyopencv_hfs_HfsSegment_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::hfs::HfsSegment>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_hfs_HfsSegment_Type))
    {
        failmsg("Expected cv::hfs::HfsSegment for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_hfs_HfsSegment_t*)src)->v.dynamicCast<cv::hfs::HfsSegment>();
    return true;
}


struct pyopencv_img_hash_AverageHash_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_AverageHash_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_AverageHash",
    sizeof(pyopencv_img_hash_AverageHash_t),
};

static void pyopencv_img_hash_AverageHash_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_AverageHash_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::AverageHash>& r)
{
    pyopencv_img_hash_AverageHash_t *m = PyObject_NEW(pyopencv_img_hash_AverageHash_t, &pyopencv_img_hash_AverageHash_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::AverageHash>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_AverageHash_Type))
    {
        failmsg("Expected cv::img_hash::AverageHash for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_AverageHash_t*)src)->v.dynamicCast<cv::img_hash::AverageHash>();
    return true;
}


struct pyopencv_img_hash_BlockMeanHash_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_BlockMeanHash_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_BlockMeanHash",
    sizeof(pyopencv_img_hash_BlockMeanHash_t),
};

static void pyopencv_img_hash_BlockMeanHash_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_BlockMeanHash_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::BlockMeanHash>& r)
{
    pyopencv_img_hash_BlockMeanHash_t *m = PyObject_NEW(pyopencv_img_hash_BlockMeanHash_t, &pyopencv_img_hash_BlockMeanHash_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::BlockMeanHash>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_BlockMeanHash_Type))
    {
        failmsg("Expected cv::img_hash::BlockMeanHash for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_BlockMeanHash_t*)src)->v.dynamicCast<cv::img_hash::BlockMeanHash>();
    return true;
}


struct pyopencv_img_hash_ColorMomentHash_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_ColorMomentHash_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_ColorMomentHash",
    sizeof(pyopencv_img_hash_ColorMomentHash_t),
};

static void pyopencv_img_hash_ColorMomentHash_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_ColorMomentHash_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::ColorMomentHash>& r)
{
    pyopencv_img_hash_ColorMomentHash_t *m = PyObject_NEW(pyopencv_img_hash_ColorMomentHash_t, &pyopencv_img_hash_ColorMomentHash_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::ColorMomentHash>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_ColorMomentHash_Type))
    {
        failmsg("Expected cv::img_hash::ColorMomentHash for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_ColorMomentHash_t*)src)->v.dynamicCast<cv::img_hash::ColorMomentHash>();
    return true;
}


struct pyopencv_img_hash_ImgHashBase_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_ImgHashBase_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_ImgHashBase",
    sizeof(pyopencv_img_hash_ImgHashBase_t),
};

static void pyopencv_img_hash_ImgHashBase_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_ImgHashBase_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::ImgHashBase>& r)
{
    pyopencv_img_hash_ImgHashBase_t *m = PyObject_NEW(pyopencv_img_hash_ImgHashBase_t, &pyopencv_img_hash_ImgHashBase_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::ImgHashBase>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_ImgHashBase_Type))
    {
        failmsg("Expected cv::img_hash::ImgHashBase for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_ImgHashBase_t*)src)->v.dynamicCast<cv::img_hash::ImgHashBase>();
    return true;
}


struct pyopencv_img_hash_MarrHildrethHash_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_MarrHildrethHash_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_MarrHildrethHash",
    sizeof(pyopencv_img_hash_MarrHildrethHash_t),
};

static void pyopencv_img_hash_MarrHildrethHash_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_MarrHildrethHash_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::MarrHildrethHash>& r)
{
    pyopencv_img_hash_MarrHildrethHash_t *m = PyObject_NEW(pyopencv_img_hash_MarrHildrethHash_t, &pyopencv_img_hash_MarrHildrethHash_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::MarrHildrethHash>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_MarrHildrethHash_Type))
    {
        failmsg("Expected cv::img_hash::MarrHildrethHash for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_MarrHildrethHash_t*)src)->v.dynamicCast<cv::img_hash::MarrHildrethHash>();
    return true;
}


struct pyopencv_img_hash_PHash_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_PHash_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_PHash",
    sizeof(pyopencv_img_hash_PHash_t),
};

static void pyopencv_img_hash_PHash_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_PHash_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::PHash>& r)
{
    pyopencv_img_hash_PHash_t *m = PyObject_NEW(pyopencv_img_hash_PHash_t, &pyopencv_img_hash_PHash_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::PHash>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_PHash_Type))
    {
        failmsg("Expected cv::img_hash::PHash for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_PHash_t*)src)->v.dynamicCast<cv::img_hash::PHash>();
    return true;
}


struct pyopencv_img_hash_RadialVarianceHash_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_img_hash_RadialVarianceHash_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".img_hash_RadialVarianceHash",
    sizeof(pyopencv_img_hash_RadialVarianceHash_t),
};

static void pyopencv_img_hash_RadialVarianceHash_dealloc(PyObject* self)
{
    ((pyopencv_img_hash_RadialVarianceHash_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::img_hash::RadialVarianceHash>& r)
{
    pyopencv_img_hash_RadialVarianceHash_t *m = PyObject_NEW(pyopencv_img_hash_RadialVarianceHash_t, &pyopencv_img_hash_RadialVarianceHash_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::img_hash::RadialVarianceHash>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_img_hash_RadialVarianceHash_Type))
    {
        failmsg("Expected cv::img_hash::RadialVarianceHash for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_img_hash_RadialVarianceHash_t*)src)->v.dynamicCast<cv::img_hash::RadialVarianceHash>();
    return true;
}


struct pyopencv_line_descriptor_KeyLine_t
{
    PyObject_HEAD
    cv::line_descriptor::KeyLine v;
};

static PyTypeObject pyopencv_line_descriptor_KeyLine_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".line_descriptor_KeyLine",
    sizeof(pyopencv_line_descriptor_KeyLine_t),
};

static void pyopencv_line_descriptor_KeyLine_dealloc(PyObject* self)
{
    ((pyopencv_line_descriptor_KeyLine_t*)self)->v.cv::line_descriptor::KeyLine::~KeyLine();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::line_descriptor::KeyLine& r)
{
    pyopencv_line_descriptor_KeyLine_t *m = PyObject_NEW(pyopencv_line_descriptor_KeyLine_t, &pyopencv_line_descriptor_KeyLine_Type);
    new (&m->v) cv::line_descriptor::KeyLine(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::line_descriptor::KeyLine& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_line_descriptor_KeyLine_Type))
    {
        failmsg("Expected cv::line_descriptor::KeyLine for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_line_descriptor_KeyLine_t*)src)->v;
    return true;
}

struct pyopencv_line_descriptor_LSDDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_line_descriptor_LSDDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".line_descriptor_LSDDetector",
    sizeof(pyopencv_line_descriptor_LSDDetector_t),
};

static void pyopencv_line_descriptor_LSDDetector_dealloc(PyObject* self)
{
    ((pyopencv_line_descriptor_LSDDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::line_descriptor::LSDDetector>& r)
{
    pyopencv_line_descriptor_LSDDetector_t *m = PyObject_NEW(pyopencv_line_descriptor_LSDDetector_t, &pyopencv_line_descriptor_LSDDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::line_descriptor::LSDDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_line_descriptor_LSDDetector_Type))
    {
        failmsg("Expected cv::line_descriptor::LSDDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_line_descriptor_LSDDetector_t*)src)->v.dynamicCast<cv::line_descriptor::LSDDetector>();
    return true;
}


struct pyopencv_linemod_Detector_t
{
    PyObject_HEAD
    Ptr<cv::linemod::Detector> v;
};

static PyTypeObject pyopencv_linemod_Detector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".linemod_Detector",
    sizeof(pyopencv_linemod_Detector_t),
};

static void pyopencv_linemod_Detector_dealloc(PyObject* self)
{
    ((pyopencv_linemod_Detector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::linemod::Detector>& r)
{
    pyopencv_linemod_Detector_t *m = PyObject_NEW(pyopencv_linemod_Detector_t, &pyopencv_linemod_Detector_Type);
    new (&(m->v)) Ptr<cv::linemod::Detector>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::linemod::Detector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_linemod_Detector_Type))
    {
        failmsg("Expected cv::linemod::Detector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_linemod_Detector_t*)src)->v.dynamicCast<cv::linemod::Detector>();
    return true;
}


struct pyopencv_linemod_Feature_t
{
    PyObject_HEAD
    cv::linemod::Feature v;
};

static PyTypeObject pyopencv_linemod_Feature_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".linemod_Feature",
    sizeof(pyopencv_linemod_Feature_t),
};

static void pyopencv_linemod_Feature_dealloc(PyObject* self)
{
    ((pyopencv_linemod_Feature_t*)self)->v.cv::linemod::Feature::~Feature();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::linemod::Feature& r)
{
    pyopencv_linemod_Feature_t *m = PyObject_NEW(pyopencv_linemod_Feature_t, &pyopencv_linemod_Feature_Type);
    new (&m->v) cv::linemod::Feature(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::linemod::Feature& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_linemod_Feature_Type))
    {
        failmsg("Expected cv::linemod::Feature for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_linemod_Feature_t*)src)->v;
    return true;
}

struct pyopencv_linemod_Match_t
{
    PyObject_HEAD
    cv::linemod::Match v;
};

static PyTypeObject pyopencv_linemod_Match_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".linemod_Match",
    sizeof(pyopencv_linemod_Match_t),
};

static void pyopencv_linemod_Match_dealloc(PyObject* self)
{
    ((pyopencv_linemod_Match_t*)self)->v.cv::linemod::Match::~Match();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::linemod::Match& r)
{
    pyopencv_linemod_Match_t *m = PyObject_NEW(pyopencv_linemod_Match_t, &pyopencv_linemod_Match_Type);
    new (&m->v) cv::linemod::Match(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::linemod::Match& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_linemod_Match_Type))
    {
        failmsg("Expected cv::linemod::Match for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_linemod_Match_t*)src)->v;
    return true;
}

struct pyopencv_linemod_Modality_t
{
    PyObject_HEAD
    Ptr<cv::linemod::Modality> v;
};

static PyTypeObject pyopencv_linemod_Modality_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".linemod_Modality",
    sizeof(pyopencv_linemod_Modality_t),
};

static void pyopencv_linemod_Modality_dealloc(PyObject* self)
{
    ((pyopencv_linemod_Modality_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::linemod::Modality>& r)
{
    pyopencv_linemod_Modality_t *m = PyObject_NEW(pyopencv_linemod_Modality_t, &pyopencv_linemod_Modality_Type);
    new (&(m->v)) Ptr<cv::linemod::Modality>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::linemod::Modality>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_linemod_Modality_Type))
    {
        failmsg("Expected cv::linemod::Modality for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_linemod_Modality_t*)src)->v.dynamicCast<cv::linemod::Modality>();
    return true;
}


struct pyopencv_linemod_QuantizedPyramid_t
{
    PyObject_HEAD
    Ptr<cv::linemod::QuantizedPyramid> v;
};

static PyTypeObject pyopencv_linemod_QuantizedPyramid_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".linemod_QuantizedPyramid",
    sizeof(pyopencv_linemod_QuantizedPyramid_t),
};

static void pyopencv_linemod_QuantizedPyramid_dealloc(PyObject* self)
{
    ((pyopencv_linemod_QuantizedPyramid_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::linemod::QuantizedPyramid>& r)
{
    pyopencv_linemod_QuantizedPyramid_t *m = PyObject_NEW(pyopencv_linemod_QuantizedPyramid_t, &pyopencv_linemod_QuantizedPyramid_Type);
    new (&(m->v)) Ptr<cv::linemod::QuantizedPyramid>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::linemod::QuantizedPyramid>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_linemod_QuantizedPyramid_Type))
    {
        failmsg("Expected cv::linemod::QuantizedPyramid for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_linemod_QuantizedPyramid_t*)src)->v.dynamicCast<cv::linemod::QuantizedPyramid>();
    return true;
}


struct pyopencv_linemod_Template_t
{
    PyObject_HEAD
    cv::linemod::Template v;
};

static PyTypeObject pyopencv_linemod_Template_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".linemod_Template",
    sizeof(pyopencv_linemod_Template_t),
};

static void pyopencv_linemod_Template_dealloc(PyObject* self)
{
    ((pyopencv_linemod_Template_t*)self)->v.cv::linemod::Template::~Template();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const cv::linemod::Template& r)
{
    pyopencv_linemod_Template_t *m = PyObject_NEW(pyopencv_linemod_Template_t, &pyopencv_linemod_Template_Type);
    new (&m->v) cv::linemod::Template(r); //Copy constructor
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, cv::linemod::Template& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_linemod_Template_Type))
    {
        failmsg("Expected cv::linemod::Template for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_linemod_Template_t*)src)->v;
    return true;
}

struct pyopencv_ml_ANN_MLP_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_ANN_MLP_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_ANN_MLP",
    sizeof(pyopencv_ml_ANN_MLP_t),
};

static void pyopencv_ml_ANN_MLP_dealloc(PyObject* self)
{
    ((pyopencv_ml_ANN_MLP_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::ANN_MLP>& r)
{
    pyopencv_ml_ANN_MLP_t *m = PyObject_NEW(pyopencv_ml_ANN_MLP_t, &pyopencv_ml_ANN_MLP_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::ANN_MLP>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_ANN_MLP_Type))
    {
        failmsg("Expected cv::ml::ANN_MLP for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_ANN_MLP_t*)src)->v.dynamicCast<cv::ml::ANN_MLP>();
    return true;
}


struct pyopencv_ml_ANN_MLP_ANNEAL_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_ANN_MLP_ANNEAL_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_ANN_MLP_ANNEAL",
    sizeof(pyopencv_ml_ANN_MLP_ANNEAL_t),
};

static void pyopencv_ml_ANN_MLP_ANNEAL_dealloc(PyObject* self)
{
    ((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::ANN_MLP_ANNEAL>& r)
{
    pyopencv_ml_ANN_MLP_ANNEAL_t *m = PyObject_NEW(pyopencv_ml_ANN_MLP_ANNEAL_t, &pyopencv_ml_ANN_MLP_ANNEAL_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::ANN_MLP_ANNEAL>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
    {
        failmsg("Expected cv::ml::ANN_MLP_ANNEAL for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_ANN_MLP_ANNEAL_t*)src)->v.dynamicCast<cv::ml::ANN_MLP_ANNEAL>();
    return true;
}


struct pyopencv_ml_Boost_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_Boost_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_Boost",
    sizeof(pyopencv_ml_Boost_t),
};

static void pyopencv_ml_Boost_dealloc(PyObject* self)
{
    ((pyopencv_ml_Boost_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::Boost>& r)
{
    pyopencv_ml_Boost_t *m = PyObject_NEW(pyopencv_ml_Boost_t, &pyopencv_ml_Boost_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::Boost>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_Boost_Type))
    {
        failmsg("Expected cv::ml::Boost for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_Boost_t*)src)->v.dynamicCast<cv::ml::Boost>();
    return true;
}


struct pyopencv_ml_DTrees_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_DTrees_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_DTrees",
    sizeof(pyopencv_ml_DTrees_t),
};

static void pyopencv_ml_DTrees_dealloc(PyObject* self)
{
    ((pyopencv_ml_DTrees_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::DTrees>& r)
{
    pyopencv_ml_DTrees_t *m = PyObject_NEW(pyopencv_ml_DTrees_t, &pyopencv_ml_DTrees_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::DTrees>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_DTrees_Type))
    {
        failmsg("Expected cv::ml::DTrees for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_DTrees_t*)src)->v.dynamicCast<cv::ml::DTrees>();
    return true;
}


struct pyopencv_ml_EM_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_EM_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_EM",
    sizeof(pyopencv_ml_EM_t),
};

static void pyopencv_ml_EM_dealloc(PyObject* self)
{
    ((pyopencv_ml_EM_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::EM>& r)
{
    pyopencv_ml_EM_t *m = PyObject_NEW(pyopencv_ml_EM_t, &pyopencv_ml_EM_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::EM>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_EM_Type))
    {
        failmsg("Expected cv::ml::EM for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_EM_t*)src)->v.dynamicCast<cv::ml::EM>();
    return true;
}


struct pyopencv_ml_KNearest_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_KNearest_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_KNearest",
    sizeof(pyopencv_ml_KNearest_t),
};

static void pyopencv_ml_KNearest_dealloc(PyObject* self)
{
    ((pyopencv_ml_KNearest_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::KNearest>& r)
{
    pyopencv_ml_KNearest_t *m = PyObject_NEW(pyopencv_ml_KNearest_t, &pyopencv_ml_KNearest_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::KNearest>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_KNearest_Type))
    {
        failmsg("Expected cv::ml::KNearest for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_KNearest_t*)src)->v.dynamicCast<cv::ml::KNearest>();
    return true;
}


struct pyopencv_ml_LogisticRegression_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_LogisticRegression_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_LogisticRegression",
    sizeof(pyopencv_ml_LogisticRegression_t),
};

static void pyopencv_ml_LogisticRegression_dealloc(PyObject* self)
{
    ((pyopencv_ml_LogisticRegression_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::LogisticRegression>& r)
{
    pyopencv_ml_LogisticRegression_t *m = PyObject_NEW(pyopencv_ml_LogisticRegression_t, &pyopencv_ml_LogisticRegression_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::LogisticRegression>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_LogisticRegression_Type))
    {
        failmsg("Expected cv::ml::LogisticRegression for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_LogisticRegression_t*)src)->v.dynamicCast<cv::ml::LogisticRegression>();
    return true;
}


struct pyopencv_ml_NormalBayesClassifier_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_NormalBayesClassifier_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_NormalBayesClassifier",
    sizeof(pyopencv_ml_NormalBayesClassifier_t),
};

static void pyopencv_ml_NormalBayesClassifier_dealloc(PyObject* self)
{
    ((pyopencv_ml_NormalBayesClassifier_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::NormalBayesClassifier>& r)
{
    pyopencv_ml_NormalBayesClassifier_t *m = PyObject_NEW(pyopencv_ml_NormalBayesClassifier_t, &pyopencv_ml_NormalBayesClassifier_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::NormalBayesClassifier>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_NormalBayesClassifier_Type))
    {
        failmsg("Expected cv::ml::NormalBayesClassifier for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_NormalBayesClassifier_t*)src)->v.dynamicCast<cv::ml::NormalBayesClassifier>();
    return true;
}


struct pyopencv_ml_ParamGrid_t
{
    PyObject_HEAD
    Ptr<cv::ml::ParamGrid> v;
};

static PyTypeObject pyopencv_ml_ParamGrid_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_ParamGrid",
    sizeof(pyopencv_ml_ParamGrid_t),
};

static void pyopencv_ml_ParamGrid_dealloc(PyObject* self)
{
    ((pyopencv_ml_ParamGrid_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::ParamGrid>& r)
{
    pyopencv_ml_ParamGrid_t *m = PyObject_NEW(pyopencv_ml_ParamGrid_t, &pyopencv_ml_ParamGrid_Type);
    new (&(m->v)) Ptr<cv::ml::ParamGrid>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::ParamGrid>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_ParamGrid_Type))
    {
        failmsg("Expected cv::ml::ParamGrid for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_ParamGrid_t*)src)->v.dynamicCast<cv::ml::ParamGrid>();
    return true;
}


struct pyopencv_ml_RTrees_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_RTrees_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_RTrees",
    sizeof(pyopencv_ml_RTrees_t),
};

static void pyopencv_ml_RTrees_dealloc(PyObject* self)
{
    ((pyopencv_ml_RTrees_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::RTrees>& r)
{
    pyopencv_ml_RTrees_t *m = PyObject_NEW(pyopencv_ml_RTrees_t, &pyopencv_ml_RTrees_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::RTrees>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_RTrees_Type))
    {
        failmsg("Expected cv::ml::RTrees for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_RTrees_t*)src)->v.dynamicCast<cv::ml::RTrees>();
    return true;
}


struct pyopencv_ml_SVM_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_SVM_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_SVM",
    sizeof(pyopencv_ml_SVM_t),
};

static void pyopencv_ml_SVM_dealloc(PyObject* self)
{
    ((pyopencv_ml_SVM_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::SVM>& r)
{
    pyopencv_ml_SVM_t *m = PyObject_NEW(pyopencv_ml_SVM_t, &pyopencv_ml_SVM_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::SVM>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_SVM_Type))
    {
        failmsg("Expected cv::ml::SVM for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_SVM_t*)src)->v.dynamicCast<cv::ml::SVM>();
    return true;
}


struct pyopencv_ml_SVMSGD_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_SVMSGD_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_SVMSGD",
    sizeof(pyopencv_ml_SVMSGD_t),
};

static void pyopencv_ml_SVMSGD_dealloc(PyObject* self)
{
    ((pyopencv_ml_SVMSGD_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::SVMSGD>& r)
{
    pyopencv_ml_SVMSGD_t *m = PyObject_NEW(pyopencv_ml_SVMSGD_t, &pyopencv_ml_SVMSGD_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::SVMSGD>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_SVMSGD_Type))
    {
        failmsg("Expected cv::ml::SVMSGD for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_SVMSGD_t*)src)->v.dynamicCast<cv::ml::SVMSGD>();
    return true;
}


struct pyopencv_ml_StatModel_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ml_StatModel_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_StatModel",
    sizeof(pyopencv_ml_StatModel_t),
};

static void pyopencv_ml_StatModel_dealloc(PyObject* self)
{
    ((pyopencv_ml_StatModel_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::StatModel>& r)
{
    pyopencv_ml_StatModel_t *m = PyObject_NEW(pyopencv_ml_StatModel_t, &pyopencv_ml_StatModel_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::StatModel>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_StatModel_Type))
    {
        failmsg("Expected cv::ml::StatModel for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_StatModel_t*)src)->v.dynamicCast<cv::ml::StatModel>();
    return true;
}


struct pyopencv_ml_TrainData_t
{
    PyObject_HEAD
    Ptr<cv::ml::TrainData> v;
};

static PyTypeObject pyopencv_ml_TrainData_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ml_TrainData",
    sizeof(pyopencv_ml_TrainData_t),
};

static void pyopencv_ml_TrainData_dealloc(PyObject* self)
{
    ((pyopencv_ml_TrainData_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ml::TrainData>& r)
{
    pyopencv_ml_TrainData_t *m = PyObject_NEW(pyopencv_ml_TrainData_t, &pyopencv_ml_TrainData_Type);
    new (&(m->v)) Ptr<cv::ml::TrainData>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ml::TrainData>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ml_TrainData_Type))
    {
        failmsg("Expected cv::ml::TrainData for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ml_TrainData_t*)src)->v.dynamicCast<cv::ml::TrainData>();
    return true;
}


struct pyopencv_optflow_DISOpticalFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_optflow_DISOpticalFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_DISOpticalFlow",
    sizeof(pyopencv_optflow_DISOpticalFlow_t),
};

static void pyopencv_optflow_DISOpticalFlow_dealloc(PyObject* self)
{
    ((pyopencv_optflow_DISOpticalFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::DISOpticalFlow>& r)
{
    pyopencv_optflow_DISOpticalFlow_t *m = PyObject_NEW(pyopencv_optflow_DISOpticalFlow_t, &pyopencv_optflow_DISOpticalFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::DISOpticalFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_DISOpticalFlow_Type))
    {
        failmsg("Expected cv::optflow::DISOpticalFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_DISOpticalFlow_t*)src)->v.dynamicCast<cv::optflow::DISOpticalFlow>();
    return true;
}


struct pyopencv_optflow_GPCDetails_t
{
    PyObject_HEAD
    Ptr<cv::optflow::GPCDetails> v;
};

static PyTypeObject pyopencv_optflow_GPCDetails_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_GPCDetails",
    sizeof(pyopencv_optflow_GPCDetails_t),
};

static void pyopencv_optflow_GPCDetails_dealloc(PyObject* self)
{
    ((pyopencv_optflow_GPCDetails_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::GPCDetails>& r)
{
    pyopencv_optflow_GPCDetails_t *m = PyObject_NEW(pyopencv_optflow_GPCDetails_t, &pyopencv_optflow_GPCDetails_Type);
    new (&(m->v)) Ptr<cv::optflow::GPCDetails>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::GPCDetails>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_GPCDetails_Type))
    {
        failmsg("Expected cv::optflow::GPCDetails for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_GPCDetails_t*)src)->v.dynamicCast<cv::optflow::GPCDetails>();
    return true;
}


struct pyopencv_optflow_GPCPatchDescriptor_t
{
    PyObject_HEAD
    Ptr<cv::optflow::GPCPatchDescriptor> v;
};

static PyTypeObject pyopencv_optflow_GPCPatchDescriptor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_GPCPatchDescriptor",
    sizeof(pyopencv_optflow_GPCPatchDescriptor_t),
};

static void pyopencv_optflow_GPCPatchDescriptor_dealloc(PyObject* self)
{
    ((pyopencv_optflow_GPCPatchDescriptor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::GPCPatchDescriptor>& r)
{
    pyopencv_optflow_GPCPatchDescriptor_t *m = PyObject_NEW(pyopencv_optflow_GPCPatchDescriptor_t, &pyopencv_optflow_GPCPatchDescriptor_Type);
    new (&(m->v)) Ptr<cv::optflow::GPCPatchDescriptor>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::GPCPatchDescriptor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_GPCPatchDescriptor_Type))
    {
        failmsg("Expected cv::optflow::GPCPatchDescriptor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_GPCPatchDescriptor_t*)src)->v.dynamicCast<cv::optflow::GPCPatchDescriptor>();
    return true;
}


struct pyopencv_optflow_GPCPatchSample_t
{
    PyObject_HEAD
    Ptr<cv::optflow::GPCPatchSample> v;
};

static PyTypeObject pyopencv_optflow_GPCPatchSample_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_GPCPatchSample",
    sizeof(pyopencv_optflow_GPCPatchSample_t),
};

static void pyopencv_optflow_GPCPatchSample_dealloc(PyObject* self)
{
    ((pyopencv_optflow_GPCPatchSample_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::GPCPatchSample>& r)
{
    pyopencv_optflow_GPCPatchSample_t *m = PyObject_NEW(pyopencv_optflow_GPCPatchSample_t, &pyopencv_optflow_GPCPatchSample_Type);
    new (&(m->v)) Ptr<cv::optflow::GPCPatchSample>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::GPCPatchSample>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_GPCPatchSample_Type))
    {
        failmsg("Expected cv::optflow::GPCPatchSample for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_GPCPatchSample_t*)src)->v.dynamicCast<cv::optflow::GPCPatchSample>();
    return true;
}


struct pyopencv_optflow_GPCTrainingSamples_t
{
    PyObject_HEAD
    Ptr<cv::optflow::GPCTrainingSamples> v;
};

static PyTypeObject pyopencv_optflow_GPCTrainingSamples_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_GPCTrainingSamples",
    sizeof(pyopencv_optflow_GPCTrainingSamples_t),
};

static void pyopencv_optflow_GPCTrainingSamples_dealloc(PyObject* self)
{
    ((pyopencv_optflow_GPCTrainingSamples_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::GPCTrainingSamples>& r)
{
    pyopencv_optflow_GPCTrainingSamples_t *m = PyObject_NEW(pyopencv_optflow_GPCTrainingSamples_t, &pyopencv_optflow_GPCTrainingSamples_Type);
    new (&(m->v)) Ptr<cv::optflow::GPCTrainingSamples>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::GPCTrainingSamples>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_GPCTrainingSamples_Type))
    {
        failmsg("Expected cv::optflow::GPCTrainingSamples for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_GPCTrainingSamples_t*)src)->v.dynamicCast<cv::optflow::GPCTrainingSamples>();
    return true;
}


struct pyopencv_optflow_GPCTree_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_optflow_GPCTree_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_GPCTree",
    sizeof(pyopencv_optflow_GPCTree_t),
};

static void pyopencv_optflow_GPCTree_dealloc(PyObject* self)
{
    ((pyopencv_optflow_GPCTree_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::GPCTree>& r)
{
    pyopencv_optflow_GPCTree_t *m = PyObject_NEW(pyopencv_optflow_GPCTree_t, &pyopencv_optflow_GPCTree_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::GPCTree>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_GPCTree_Type))
    {
        failmsg("Expected cv::optflow::GPCTree for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_GPCTree_t*)src)->v.dynamicCast<cv::optflow::GPCTree>();
    return true;
}


struct pyopencv_optflow_OpticalFlowPCAFlow_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_optflow_OpticalFlowPCAFlow_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_OpticalFlowPCAFlow",
    sizeof(pyopencv_optflow_OpticalFlowPCAFlow_t),
};

static void pyopencv_optflow_OpticalFlowPCAFlow_dealloc(PyObject* self)
{
    ((pyopencv_optflow_OpticalFlowPCAFlow_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::OpticalFlowPCAFlow>& r)
{
    pyopencv_optflow_OpticalFlowPCAFlow_t *m = PyObject_NEW(pyopencv_optflow_OpticalFlowPCAFlow_t, &pyopencv_optflow_OpticalFlowPCAFlow_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::OpticalFlowPCAFlow>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_OpticalFlowPCAFlow_Type))
    {
        failmsg("Expected cv::optflow::OpticalFlowPCAFlow for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_OpticalFlowPCAFlow_t*)src)->v.dynamicCast<cv::optflow::OpticalFlowPCAFlow>();
    return true;
}


struct pyopencv_optflow_PCAPrior_t
{
    PyObject_HEAD
    Ptr<cv::optflow::PCAPrior> v;
};

static PyTypeObject pyopencv_optflow_PCAPrior_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_PCAPrior",
    sizeof(pyopencv_optflow_PCAPrior_t),
};

static void pyopencv_optflow_PCAPrior_dealloc(PyObject* self)
{
    ((pyopencv_optflow_PCAPrior_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::PCAPrior>& r)
{
    pyopencv_optflow_PCAPrior_t *m = PyObject_NEW(pyopencv_optflow_PCAPrior_t, &pyopencv_optflow_PCAPrior_Type);
    new (&(m->v)) Ptr<cv::optflow::PCAPrior>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::PCAPrior>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_PCAPrior_Type))
    {
        failmsg("Expected cv::optflow::PCAPrior for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_PCAPrior_t*)src)->v.dynamicCast<cv::optflow::PCAPrior>();
    return true;
}


struct pyopencv_optflow_VariationalRefinement_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_optflow_VariationalRefinement_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".optflow_VariationalRefinement",
    sizeof(pyopencv_optflow_VariationalRefinement_t),
};

static void pyopencv_optflow_VariationalRefinement_dealloc(PyObject* self)
{
    ((pyopencv_optflow_VariationalRefinement_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::optflow::VariationalRefinement>& r)
{
    pyopencv_optflow_VariationalRefinement_t *m = PyObject_NEW(pyopencv_optflow_VariationalRefinement_t, &pyopencv_optflow_VariationalRefinement_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::optflow::VariationalRefinement>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_optflow_VariationalRefinement_Type))
    {
        failmsg("Expected cv::optflow::VariationalRefinement for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_optflow_VariationalRefinement_t*)src)->v.dynamicCast<cv::optflow::VariationalRefinement>();
    return true;
}


struct pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".phase_unwrapping_HistogramPhaseUnwrapping",
    sizeof(pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t),
};

static void pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_dealloc(PyObject* self)
{
    ((pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::phase_unwrapping::HistogramPhaseUnwrapping>& r)
{
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t *m = PyObject_NEW(pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t, &pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::phase_unwrapping::HistogramPhaseUnwrapping>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type))
    {
        failmsg("Expected cv::phase_unwrapping::HistogramPhaseUnwrapping for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t*)src)->v.dynamicCast<cv::phase_unwrapping::HistogramPhaseUnwrapping>();
    return true;
}


struct pyopencv_phase_unwrapping_PhaseUnwrapping_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_phase_unwrapping_PhaseUnwrapping_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".phase_unwrapping_PhaseUnwrapping",
    sizeof(pyopencv_phase_unwrapping_PhaseUnwrapping_t),
};

static void pyopencv_phase_unwrapping_PhaseUnwrapping_dealloc(PyObject* self)
{
    ((pyopencv_phase_unwrapping_PhaseUnwrapping_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::phase_unwrapping::PhaseUnwrapping>& r)
{
    pyopencv_phase_unwrapping_PhaseUnwrapping_t *m = PyObject_NEW(pyopencv_phase_unwrapping_PhaseUnwrapping_t, &pyopencv_phase_unwrapping_PhaseUnwrapping_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::phase_unwrapping::PhaseUnwrapping>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_phase_unwrapping_PhaseUnwrapping_Type))
    {
        failmsg("Expected cv::phase_unwrapping::PhaseUnwrapping for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_phase_unwrapping_PhaseUnwrapping_t*)src)->v.dynamicCast<cv::phase_unwrapping::PhaseUnwrapping>();
    return true;
}


struct pyopencv_plot_Plot2d_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_plot_Plot2d_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".plot_Plot2d",
    sizeof(pyopencv_plot_Plot2d_t),
};

static void pyopencv_plot_Plot2d_dealloc(PyObject* self)
{
    ((pyopencv_plot_Plot2d_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::plot::Plot2d>& r)
{
    pyopencv_plot_Plot2d_t *m = PyObject_NEW(pyopencv_plot_Plot2d_t, &pyopencv_plot_Plot2d_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::plot::Plot2d>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_plot_Plot2d_Type))
    {
        failmsg("Expected cv::plot::Plot2d for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_plot_Plot2d_t*)src)->v.dynamicCast<cv::plot::Plot2d>();
    return true;
}


struct pyopencv_ppf_match_3d_ICP_t
{
    PyObject_HEAD
    Ptr<cv::ppf_match_3d::ICP> v;
};

static PyTypeObject pyopencv_ppf_match_3d_ICP_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ppf_match_3d_ICP",
    sizeof(pyopencv_ppf_match_3d_ICP_t),
};

static void pyopencv_ppf_match_3d_ICP_dealloc(PyObject* self)
{
    ((pyopencv_ppf_match_3d_ICP_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ppf_match_3d::ICP>& r)
{
    pyopencv_ppf_match_3d_ICP_t *m = PyObject_NEW(pyopencv_ppf_match_3d_ICP_t, &pyopencv_ppf_match_3d_ICP_Type);
    new (&(m->v)) Ptr<cv::ppf_match_3d::ICP>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ppf_match_3d::ICP>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ppf_match_3d_ICP_Type))
    {
        failmsg("Expected cv::ppf_match_3d::ICP for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ppf_match_3d_ICP_t*)src)->v.dynamicCast<cv::ppf_match_3d::ICP>();
    return true;
}


struct pyopencv_ppf_match_3d_PPF3DDetector_t
{
    PyObject_HEAD
    Ptr<cv::ppf_match_3d::PPF3DDetector> v;
};

static PyTypeObject pyopencv_ppf_match_3d_PPF3DDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ppf_match_3d_PPF3DDetector",
    sizeof(pyopencv_ppf_match_3d_PPF3DDetector_t),
};

static void pyopencv_ppf_match_3d_PPF3DDetector_dealloc(PyObject* self)
{
    ((pyopencv_ppf_match_3d_PPF3DDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ppf_match_3d::PPF3DDetector>& r)
{
    pyopencv_ppf_match_3d_PPF3DDetector_t *m = PyObject_NEW(pyopencv_ppf_match_3d_PPF3DDetector_t, &pyopencv_ppf_match_3d_PPF3DDetector_Type);
    new (&(m->v)) Ptr<cv::ppf_match_3d::PPF3DDetector>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ppf_match_3d::PPF3DDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ppf_match_3d_PPF3DDetector_Type))
    {
        failmsg("Expected cv::ppf_match_3d::PPF3DDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ppf_match_3d_PPF3DDetector_t*)src)->v.dynamicCast<cv::ppf_match_3d::PPF3DDetector>();
    return true;
}


struct pyopencv_ppf_match_3d_PoseCluster3D_t
{
    PyObject_HEAD
    Ptr<cv::ppf_match_3d::PoseCluster3D> v;
};

static PyTypeObject pyopencv_ppf_match_3d_PoseCluster3D_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ppf_match_3d_PoseCluster3D",
    sizeof(pyopencv_ppf_match_3d_PoseCluster3D_t),
};

static void pyopencv_ppf_match_3d_PoseCluster3D_dealloc(PyObject* self)
{
    ((pyopencv_ppf_match_3d_PoseCluster3D_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ppf_match_3d::PoseCluster3D>& r)
{
    pyopencv_ppf_match_3d_PoseCluster3D_t *m = PyObject_NEW(pyopencv_ppf_match_3d_PoseCluster3D_t, &pyopencv_ppf_match_3d_PoseCluster3D_Type);
    new (&(m->v)) Ptr<cv::ppf_match_3d::PoseCluster3D>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ppf_match_3d::PoseCluster3D>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ppf_match_3d_PoseCluster3D_Type))
    {
        failmsg("Expected cv::ppf_match_3d::PoseCluster3D for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ppf_match_3d_PoseCluster3D_t*)src)->v.dynamicCast<cv::ppf_match_3d::PoseCluster3D>();
    return true;
}


struct pyopencv_reg_Map_t
{
    PyObject_HEAD
    Ptr<cv::reg::Map> v;
};

static PyTypeObject pyopencv_reg_Map_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_Map",
    sizeof(pyopencv_reg_Map_t),
};

static void pyopencv_reg_Map_dealloc(PyObject* self)
{
    ((pyopencv_reg_Map_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::Map>& r)
{
    pyopencv_reg_Map_t *m = PyObject_NEW(pyopencv_reg_Map_t, &pyopencv_reg_Map_Type);
    new (&(m->v)) Ptr<cv::reg::Map>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::Map>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_Map_Type))
    {
        failmsg("Expected cv::reg::Map for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_Map_t*)src)->v.dynamicCast<cv::reg::Map>();
    return true;
}


struct pyopencv_reg_MapAffine_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapAffine> v;
};

static PyTypeObject pyopencv_reg_MapAffine_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapAffine",
    sizeof(pyopencv_reg_MapAffine_t),
};

static void pyopencv_reg_MapAffine_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapAffine_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapAffine>& r)
{
    pyopencv_reg_MapAffine_t *m = PyObject_NEW(pyopencv_reg_MapAffine_t, &pyopencv_reg_MapAffine_Type);
    new (&(m->v)) Ptr<cv::reg::MapAffine>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapAffine>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapAffine_Type))
    {
        failmsg("Expected cv::reg::MapAffine for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapAffine_t*)src)->v.dynamicCast<cv::reg::MapAffine>();
    return true;
}


struct pyopencv_reg_MapProjec_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapProjec> v;
};

static PyTypeObject pyopencv_reg_MapProjec_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapProjec",
    sizeof(pyopencv_reg_MapProjec_t),
};

static void pyopencv_reg_MapProjec_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapProjec_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapProjec>& r)
{
    pyopencv_reg_MapProjec_t *m = PyObject_NEW(pyopencv_reg_MapProjec_t, &pyopencv_reg_MapProjec_Type);
    new (&(m->v)) Ptr<cv::reg::MapProjec>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapProjec>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapProjec_Type))
    {
        failmsg("Expected cv::reg::MapProjec for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapProjec_t*)src)->v.dynamicCast<cv::reg::MapProjec>();
    return true;
}


struct pyopencv_reg_MapShift_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapShift> v;
};

static PyTypeObject pyopencv_reg_MapShift_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapShift",
    sizeof(pyopencv_reg_MapShift_t),
};

static void pyopencv_reg_MapShift_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapShift_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapShift>& r)
{
    pyopencv_reg_MapShift_t *m = PyObject_NEW(pyopencv_reg_MapShift_t, &pyopencv_reg_MapShift_Type);
    new (&(m->v)) Ptr<cv::reg::MapShift>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapShift>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapShift_Type))
    {
        failmsg("Expected cv::reg::MapShift for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapShift_t*)src)->v.dynamicCast<cv::reg::MapShift>();
    return true;
}


struct pyopencv_reg_MapTypeCaster_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapTypeCaster> v;
};

static PyTypeObject pyopencv_reg_MapTypeCaster_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapTypeCaster",
    sizeof(pyopencv_reg_MapTypeCaster_t),
};

static void pyopencv_reg_MapTypeCaster_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapTypeCaster_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapTypeCaster>& r)
{
    pyopencv_reg_MapTypeCaster_t *m = PyObject_NEW(pyopencv_reg_MapTypeCaster_t, &pyopencv_reg_MapTypeCaster_Type);
    new (&(m->v)) Ptr<cv::reg::MapTypeCaster>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapTypeCaster>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapTypeCaster_Type))
    {
        failmsg("Expected cv::reg::MapTypeCaster for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapTypeCaster_t*)src)->v.dynamicCast<cv::reg::MapTypeCaster>();
    return true;
}


struct pyopencv_reg_Mapper_t
{
    PyObject_HEAD
    Ptr<cv::reg::Mapper> v;
};

static PyTypeObject pyopencv_reg_Mapper_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_Mapper",
    sizeof(pyopencv_reg_Mapper_t),
};

static void pyopencv_reg_Mapper_dealloc(PyObject* self)
{
    ((pyopencv_reg_Mapper_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::Mapper>& r)
{
    pyopencv_reg_Mapper_t *m = PyObject_NEW(pyopencv_reg_Mapper_t, &pyopencv_reg_Mapper_Type);
    new (&(m->v)) Ptr<cv::reg::Mapper>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::Mapper>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_Mapper_Type))
    {
        failmsg("Expected cv::reg::Mapper for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_Mapper_t*)src)->v.dynamicCast<cv::reg::Mapper>();
    return true;
}


struct pyopencv_reg_MapperGradAffine_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapperGradAffine> v;
};

static PyTypeObject pyopencv_reg_MapperGradAffine_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapperGradAffine",
    sizeof(pyopencv_reg_MapperGradAffine_t),
};

static void pyopencv_reg_MapperGradAffine_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapperGradAffine_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapperGradAffine>& r)
{
    pyopencv_reg_MapperGradAffine_t *m = PyObject_NEW(pyopencv_reg_MapperGradAffine_t, &pyopencv_reg_MapperGradAffine_Type);
    new (&(m->v)) Ptr<cv::reg::MapperGradAffine>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapperGradAffine>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapperGradAffine_Type))
    {
        failmsg("Expected cv::reg::MapperGradAffine for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapperGradAffine_t*)src)->v.dynamicCast<cv::reg::MapperGradAffine>();
    return true;
}


struct pyopencv_reg_MapperGradEuclid_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapperGradEuclid> v;
};

static PyTypeObject pyopencv_reg_MapperGradEuclid_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapperGradEuclid",
    sizeof(pyopencv_reg_MapperGradEuclid_t),
};

static void pyopencv_reg_MapperGradEuclid_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapperGradEuclid_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapperGradEuclid>& r)
{
    pyopencv_reg_MapperGradEuclid_t *m = PyObject_NEW(pyopencv_reg_MapperGradEuclid_t, &pyopencv_reg_MapperGradEuclid_Type);
    new (&(m->v)) Ptr<cv::reg::MapperGradEuclid>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapperGradEuclid>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapperGradEuclid_Type))
    {
        failmsg("Expected cv::reg::MapperGradEuclid for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapperGradEuclid_t*)src)->v.dynamicCast<cv::reg::MapperGradEuclid>();
    return true;
}


struct pyopencv_reg_MapperGradProj_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapperGradProj> v;
};

static PyTypeObject pyopencv_reg_MapperGradProj_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapperGradProj",
    sizeof(pyopencv_reg_MapperGradProj_t),
};

static void pyopencv_reg_MapperGradProj_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapperGradProj_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapperGradProj>& r)
{
    pyopencv_reg_MapperGradProj_t *m = PyObject_NEW(pyopencv_reg_MapperGradProj_t, &pyopencv_reg_MapperGradProj_Type);
    new (&(m->v)) Ptr<cv::reg::MapperGradProj>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapperGradProj>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapperGradProj_Type))
    {
        failmsg("Expected cv::reg::MapperGradProj for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapperGradProj_t*)src)->v.dynamicCast<cv::reg::MapperGradProj>();
    return true;
}


struct pyopencv_reg_MapperGradShift_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapperGradShift> v;
};

static PyTypeObject pyopencv_reg_MapperGradShift_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapperGradShift",
    sizeof(pyopencv_reg_MapperGradShift_t),
};

static void pyopencv_reg_MapperGradShift_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapperGradShift_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapperGradShift>& r)
{
    pyopencv_reg_MapperGradShift_t *m = PyObject_NEW(pyopencv_reg_MapperGradShift_t, &pyopencv_reg_MapperGradShift_Type);
    new (&(m->v)) Ptr<cv::reg::MapperGradShift>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapperGradShift>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapperGradShift_Type))
    {
        failmsg("Expected cv::reg::MapperGradShift for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapperGradShift_t*)src)->v.dynamicCast<cv::reg::MapperGradShift>();
    return true;
}


struct pyopencv_reg_MapperGradSimilar_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapperGradSimilar> v;
};

static PyTypeObject pyopencv_reg_MapperGradSimilar_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapperGradSimilar",
    sizeof(pyopencv_reg_MapperGradSimilar_t),
};

static void pyopencv_reg_MapperGradSimilar_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapperGradSimilar_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapperGradSimilar>& r)
{
    pyopencv_reg_MapperGradSimilar_t *m = PyObject_NEW(pyopencv_reg_MapperGradSimilar_t, &pyopencv_reg_MapperGradSimilar_Type);
    new (&(m->v)) Ptr<cv::reg::MapperGradSimilar>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapperGradSimilar>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapperGradSimilar_Type))
    {
        failmsg("Expected cv::reg::MapperGradSimilar for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapperGradSimilar_t*)src)->v.dynamicCast<cv::reg::MapperGradSimilar>();
    return true;
}


struct pyopencv_reg_MapperPyramid_t
{
    PyObject_HEAD
    Ptr<cv::reg::MapperPyramid> v;
};

static PyTypeObject pyopencv_reg_MapperPyramid_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".reg_MapperPyramid",
    sizeof(pyopencv_reg_MapperPyramid_t),
};

static void pyopencv_reg_MapperPyramid_dealloc(PyObject* self)
{
    ((pyopencv_reg_MapperPyramid_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::reg::MapperPyramid>& r)
{
    pyopencv_reg_MapperPyramid_t *m = PyObject_NEW(pyopencv_reg_MapperPyramid_t, &pyopencv_reg_MapperPyramid_Type);
    new (&(m->v)) Ptr<cv::reg::MapperPyramid>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::reg::MapperPyramid>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_reg_MapperPyramid_Type))
    {
        failmsg("Expected cv::reg::MapperPyramid for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_reg_MapperPyramid_t*)src)->v.dynamicCast<cv::reg::MapperPyramid>();
    return true;
}


struct pyopencv_rgbd_DepthCleaner_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_DepthCleaner_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_DepthCleaner",
    sizeof(pyopencv_rgbd_DepthCleaner_t),
};

static void pyopencv_rgbd_DepthCleaner_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_DepthCleaner_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::DepthCleaner>& r)
{
    pyopencv_rgbd_DepthCleaner_t *m = PyObject_NEW(pyopencv_rgbd_DepthCleaner_t, &pyopencv_rgbd_DepthCleaner_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::DepthCleaner>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_DepthCleaner_Type))
    {
        failmsg("Expected cv::rgbd::DepthCleaner for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_DepthCleaner_t*)src)->v.dynamicCast<cv::rgbd::DepthCleaner>();
    return true;
}


struct pyopencv_rgbd_ICPOdometry_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_ICPOdometry_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_ICPOdometry",
    sizeof(pyopencv_rgbd_ICPOdometry_t),
};

static void pyopencv_rgbd_ICPOdometry_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_ICPOdometry_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::ICPOdometry>& r)
{
    pyopencv_rgbd_ICPOdometry_t *m = PyObject_NEW(pyopencv_rgbd_ICPOdometry_t, &pyopencv_rgbd_ICPOdometry_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::ICPOdometry>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_ICPOdometry_Type))
    {
        failmsg("Expected cv::rgbd::ICPOdometry for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_ICPOdometry_t*)src)->v.dynamicCast<cv::rgbd::ICPOdometry>();
    return true;
}


struct pyopencv_rgbd_Odometry_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_Odometry_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_Odometry",
    sizeof(pyopencv_rgbd_Odometry_t),
};

static void pyopencv_rgbd_Odometry_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_Odometry_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::Odometry>& r)
{
    pyopencv_rgbd_Odometry_t *m = PyObject_NEW(pyopencv_rgbd_Odometry_t, &pyopencv_rgbd_Odometry_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::Odometry>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_Odometry_Type))
    {
        failmsg("Expected cv::rgbd::Odometry for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_Odometry_t*)src)->v.dynamicCast<cv::rgbd::Odometry>();
    return true;
}


struct pyopencv_rgbd_OdometryFrame_t
{
    PyObject_HEAD
    Ptr<cv::rgbd::OdometryFrame> v;
};

static PyTypeObject pyopencv_rgbd_OdometryFrame_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_OdometryFrame",
    sizeof(pyopencv_rgbd_OdometryFrame_t),
};

static void pyopencv_rgbd_OdometryFrame_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_OdometryFrame_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::OdometryFrame>& r)
{
    pyopencv_rgbd_OdometryFrame_t *m = PyObject_NEW(pyopencv_rgbd_OdometryFrame_t, &pyopencv_rgbd_OdometryFrame_Type);
    new (&(m->v)) Ptr<cv::rgbd::OdometryFrame>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::OdometryFrame>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_OdometryFrame_Type))
    {
        failmsg("Expected cv::rgbd::OdometryFrame for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_OdometryFrame_t*)src)->v.dynamicCast<cv::rgbd::OdometryFrame>();
    return true;
}


struct pyopencv_rgbd_RgbdFrame_t
{
    PyObject_HEAD
    Ptr<cv::rgbd::RgbdFrame> v;
};

static PyTypeObject pyopencv_rgbd_RgbdFrame_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_RgbdFrame",
    sizeof(pyopencv_rgbd_RgbdFrame_t),
};

static void pyopencv_rgbd_RgbdFrame_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_RgbdFrame_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::RgbdFrame>& r)
{
    pyopencv_rgbd_RgbdFrame_t *m = PyObject_NEW(pyopencv_rgbd_RgbdFrame_t, &pyopencv_rgbd_RgbdFrame_Type);
    new (&(m->v)) Ptr<cv::rgbd::RgbdFrame>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::RgbdFrame>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_RgbdFrame_Type))
    {
        failmsg("Expected cv::rgbd::RgbdFrame for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_RgbdFrame_t*)src)->v.dynamicCast<cv::rgbd::RgbdFrame>();
    return true;
}


struct pyopencv_rgbd_RgbdICPOdometry_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_RgbdICPOdometry_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_RgbdICPOdometry",
    sizeof(pyopencv_rgbd_RgbdICPOdometry_t),
};

static void pyopencv_rgbd_RgbdICPOdometry_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::RgbdICPOdometry>& r)
{
    pyopencv_rgbd_RgbdICPOdometry_t *m = PyObject_NEW(pyopencv_rgbd_RgbdICPOdometry_t, &pyopencv_rgbd_RgbdICPOdometry_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::RgbdICPOdometry>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_RgbdICPOdometry_Type))
    {
        failmsg("Expected cv::rgbd::RgbdICPOdometry for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_RgbdICPOdometry_t*)src)->v.dynamicCast<cv::rgbd::RgbdICPOdometry>();
    return true;
}


struct pyopencv_rgbd_RgbdNormals_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_RgbdNormals_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_RgbdNormals",
    sizeof(pyopencv_rgbd_RgbdNormals_t),
};

static void pyopencv_rgbd_RgbdNormals_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_RgbdNormals_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::RgbdNormals>& r)
{
    pyopencv_rgbd_RgbdNormals_t *m = PyObject_NEW(pyopencv_rgbd_RgbdNormals_t, &pyopencv_rgbd_RgbdNormals_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::RgbdNormals>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_RgbdNormals_Type))
    {
        failmsg("Expected cv::rgbd::RgbdNormals for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_RgbdNormals_t*)src)->v.dynamicCast<cv::rgbd::RgbdNormals>();
    return true;
}


struct pyopencv_rgbd_RgbdOdometry_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_RgbdOdometry_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_RgbdOdometry",
    sizeof(pyopencv_rgbd_RgbdOdometry_t),
};

static void pyopencv_rgbd_RgbdOdometry_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_RgbdOdometry_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::RgbdOdometry>& r)
{
    pyopencv_rgbd_RgbdOdometry_t *m = PyObject_NEW(pyopencv_rgbd_RgbdOdometry_t, &pyopencv_rgbd_RgbdOdometry_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::RgbdOdometry>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_RgbdOdometry_Type))
    {
        failmsg("Expected cv::rgbd::RgbdOdometry for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_RgbdOdometry_t*)src)->v.dynamicCast<cv::rgbd::RgbdOdometry>();
    return true;
}


struct pyopencv_rgbd_RgbdPlane_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_rgbd_RgbdPlane_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".rgbd_RgbdPlane",
    sizeof(pyopencv_rgbd_RgbdPlane_t),
};

static void pyopencv_rgbd_RgbdPlane_dealloc(PyObject* self)
{
    ((pyopencv_rgbd_RgbdPlane_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::rgbd::RgbdPlane>& r)
{
    pyopencv_rgbd_RgbdPlane_t *m = PyObject_NEW(pyopencv_rgbd_RgbdPlane_t, &pyopencv_rgbd_RgbdPlane_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::rgbd::RgbdPlane>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_rgbd_RgbdPlane_Type))
    {
        failmsg("Expected cv::rgbd::RgbdPlane for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_rgbd_RgbdPlane_t*)src)->v.dynamicCast<cv::rgbd::RgbdPlane>();
    return true;
}


struct pyopencv_saliency_MotionSaliency_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_MotionSaliency_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_MotionSaliency",
    sizeof(pyopencv_saliency_MotionSaliency_t),
};

static void pyopencv_saliency_MotionSaliency_dealloc(PyObject* self)
{
    ((pyopencv_saliency_MotionSaliency_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::MotionSaliency>& r)
{
    pyopencv_saliency_MotionSaliency_t *m = PyObject_NEW(pyopencv_saliency_MotionSaliency_t, &pyopencv_saliency_MotionSaliency_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::MotionSaliency>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_MotionSaliency_Type))
    {
        failmsg("Expected cv::saliency::MotionSaliency for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_MotionSaliency_t*)src)->v.dynamicCast<cv::saliency::MotionSaliency>();
    return true;
}


struct pyopencv_saliency_MotionSaliencyBinWangApr2014_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_MotionSaliencyBinWangApr2014_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_MotionSaliencyBinWangApr2014",
    sizeof(pyopencv_saliency_MotionSaliencyBinWangApr2014_t),
};

static void pyopencv_saliency_MotionSaliencyBinWangApr2014_dealloc(PyObject* self)
{
    ((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::MotionSaliencyBinWangApr2014>& r)
{
    pyopencv_saliency_MotionSaliencyBinWangApr2014_t *m = PyObject_NEW(pyopencv_saliency_MotionSaliencyBinWangApr2014_t, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::MotionSaliencyBinWangApr2014>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
    {
        failmsg("Expected cv::saliency::MotionSaliencyBinWangApr2014 for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)src)->v.dynamicCast<cv::saliency::MotionSaliencyBinWangApr2014>();
    return true;
}


struct pyopencv_saliency_Objectness_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_Objectness_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_Objectness",
    sizeof(pyopencv_saliency_Objectness_t),
};

static void pyopencv_saliency_Objectness_dealloc(PyObject* self)
{
    ((pyopencv_saliency_Objectness_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::Objectness>& r)
{
    pyopencv_saliency_Objectness_t *m = PyObject_NEW(pyopencv_saliency_Objectness_t, &pyopencv_saliency_Objectness_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::Objectness>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_Objectness_Type))
    {
        failmsg("Expected cv::saliency::Objectness for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_Objectness_t*)src)->v.dynamicCast<cv::saliency::Objectness>();
    return true;
}


struct pyopencv_saliency_ObjectnessBING_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_ObjectnessBING_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_ObjectnessBING",
    sizeof(pyopencv_saliency_ObjectnessBING_t),
};

static void pyopencv_saliency_ObjectnessBING_dealloc(PyObject* self)
{
    ((pyopencv_saliency_ObjectnessBING_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::ObjectnessBING>& r)
{
    pyopencv_saliency_ObjectnessBING_t *m = PyObject_NEW(pyopencv_saliency_ObjectnessBING_t, &pyopencv_saliency_ObjectnessBING_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::ObjectnessBING>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_ObjectnessBING_Type))
    {
        failmsg("Expected cv::saliency::ObjectnessBING for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_ObjectnessBING_t*)src)->v.dynamicCast<cv::saliency::ObjectnessBING>();
    return true;
}


struct pyopencv_saliency_Saliency_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_Saliency_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_Saliency",
    sizeof(pyopencv_saliency_Saliency_t),
};

static void pyopencv_saliency_Saliency_dealloc(PyObject* self)
{
    ((pyopencv_saliency_Saliency_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::Saliency>& r)
{
    pyopencv_saliency_Saliency_t *m = PyObject_NEW(pyopencv_saliency_Saliency_t, &pyopencv_saliency_Saliency_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::Saliency>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_Saliency_Type))
    {
        failmsg("Expected cv::saliency::Saliency for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_Saliency_t*)src)->v.dynamicCast<cv::saliency::Saliency>();
    return true;
}


struct pyopencv_saliency_StaticSaliency_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_StaticSaliency_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_StaticSaliency",
    sizeof(pyopencv_saliency_StaticSaliency_t),
};

static void pyopencv_saliency_StaticSaliency_dealloc(PyObject* self)
{
    ((pyopencv_saliency_StaticSaliency_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::StaticSaliency>& r)
{
    pyopencv_saliency_StaticSaliency_t *m = PyObject_NEW(pyopencv_saliency_StaticSaliency_t, &pyopencv_saliency_StaticSaliency_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::StaticSaliency>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_StaticSaliency_Type))
    {
        failmsg("Expected cv::saliency::StaticSaliency for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_StaticSaliency_t*)src)->v.dynamicCast<cv::saliency::StaticSaliency>();
    return true;
}


struct pyopencv_saliency_StaticSaliencyFineGrained_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_StaticSaliencyFineGrained_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_StaticSaliencyFineGrained",
    sizeof(pyopencv_saliency_StaticSaliencyFineGrained_t),
};

static void pyopencv_saliency_StaticSaliencyFineGrained_dealloc(PyObject* self)
{
    ((pyopencv_saliency_StaticSaliencyFineGrained_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::StaticSaliencyFineGrained>& r)
{
    pyopencv_saliency_StaticSaliencyFineGrained_t *m = PyObject_NEW(pyopencv_saliency_StaticSaliencyFineGrained_t, &pyopencv_saliency_StaticSaliencyFineGrained_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::StaticSaliencyFineGrained>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_StaticSaliencyFineGrained_Type))
    {
        failmsg("Expected cv::saliency::StaticSaliencyFineGrained for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_StaticSaliencyFineGrained_t*)src)->v.dynamicCast<cv::saliency::StaticSaliencyFineGrained>();
    return true;
}


struct pyopencv_saliency_StaticSaliencySpectralResidual_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_saliency_StaticSaliencySpectralResidual_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".saliency_StaticSaliencySpectralResidual",
    sizeof(pyopencv_saliency_StaticSaliencySpectralResidual_t),
};

static void pyopencv_saliency_StaticSaliencySpectralResidual_dealloc(PyObject* self)
{
    ((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::saliency::StaticSaliencySpectralResidual>& r)
{
    pyopencv_saliency_StaticSaliencySpectralResidual_t *m = PyObject_NEW(pyopencv_saliency_StaticSaliencySpectralResidual_t, &pyopencv_saliency_StaticSaliencySpectralResidual_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::saliency::StaticSaliencySpectralResidual>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
    {
        failmsg("Expected cv::saliency::StaticSaliencySpectralResidual for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_saliency_StaticSaliencySpectralResidual_t*)src)->v.dynamicCast<cv::saliency::StaticSaliencySpectralResidual>();
    return true;
}


struct pyopencv_structured_light_GrayCodePattern_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_structured_light_GrayCodePattern_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".structured_light_GrayCodePattern",
    sizeof(pyopencv_structured_light_GrayCodePattern_t),
};

static void pyopencv_structured_light_GrayCodePattern_dealloc(PyObject* self)
{
    ((pyopencv_structured_light_GrayCodePattern_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::structured_light::GrayCodePattern>& r)
{
    pyopencv_structured_light_GrayCodePattern_t *m = PyObject_NEW(pyopencv_structured_light_GrayCodePattern_t, &pyopencv_structured_light_GrayCodePattern_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::structured_light::GrayCodePattern>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_structured_light_GrayCodePattern_Type))
    {
        failmsg("Expected cv::structured_light::GrayCodePattern for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_structured_light_GrayCodePattern_t*)src)->v.dynamicCast<cv::structured_light::GrayCodePattern>();
    return true;
}


struct pyopencv_structured_light_SinusoidalPattern_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_structured_light_SinusoidalPattern_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".structured_light_SinusoidalPattern",
    sizeof(pyopencv_structured_light_SinusoidalPattern_t),
};

static void pyopencv_structured_light_SinusoidalPattern_dealloc(PyObject* self)
{
    ((pyopencv_structured_light_SinusoidalPattern_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::structured_light::SinusoidalPattern>& r)
{
    pyopencv_structured_light_SinusoidalPattern_t *m = PyObject_NEW(pyopencv_structured_light_SinusoidalPattern_t, &pyopencv_structured_light_SinusoidalPattern_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::structured_light::SinusoidalPattern>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_structured_light_SinusoidalPattern_Type))
    {
        failmsg("Expected cv::structured_light::SinusoidalPattern for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_structured_light_SinusoidalPattern_t*)src)->v.dynamicCast<cv::structured_light::SinusoidalPattern>();
    return true;
}


struct pyopencv_structured_light_SinusoidalPattern_Params_t
{
    PyObject_HEAD
    Ptr<cv::structured_light::SinusoidalPattern::Params> v;
};

static PyTypeObject pyopencv_structured_light_SinusoidalPattern_Params_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".structured_light_SinusoidalPattern_Params",
    sizeof(pyopencv_structured_light_SinusoidalPattern_Params_t),
};

static void pyopencv_structured_light_SinusoidalPattern_Params_dealloc(PyObject* self)
{
    ((pyopencv_structured_light_SinusoidalPattern_Params_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::structured_light::SinusoidalPattern::Params>& r)
{
    pyopencv_structured_light_SinusoidalPattern_Params_t *m = PyObject_NEW(pyopencv_structured_light_SinusoidalPattern_Params_t, &pyopencv_structured_light_SinusoidalPattern_Params_Type);
    new (&(m->v)) Ptr<cv::structured_light::SinusoidalPattern::Params>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::structured_light::SinusoidalPattern::Params>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_structured_light_SinusoidalPattern_Params_Type))
    {
        failmsg("Expected cv::structured_light::SinusoidalPattern::Params for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_structured_light_SinusoidalPattern_Params_t*)src)->v.dynamicCast<cv::structured_light::SinusoidalPattern::Params>();
    return true;
}


struct pyopencv_structured_light_StructuredLightPattern_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_structured_light_StructuredLightPattern_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".structured_light_StructuredLightPattern",
    sizeof(pyopencv_structured_light_StructuredLightPattern_t),
};

static void pyopencv_structured_light_StructuredLightPattern_dealloc(PyObject* self)
{
    ((pyopencv_structured_light_StructuredLightPattern_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::structured_light::StructuredLightPattern>& r)
{
    pyopencv_structured_light_StructuredLightPattern_t *m = PyObject_NEW(pyopencv_structured_light_StructuredLightPattern_t, &pyopencv_structured_light_StructuredLightPattern_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::structured_light::StructuredLightPattern>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_structured_light_StructuredLightPattern_Type))
    {
        failmsg("Expected cv::structured_light::StructuredLightPattern for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_structured_light_StructuredLightPattern_t*)src)->v.dynamicCast<cv::structured_light::StructuredLightPattern>();
    return true;
}


struct pyopencv_xfeatures2d_BoostDesc_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_BoostDesc_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_BoostDesc",
    sizeof(pyopencv_xfeatures2d_BoostDesc_t),
};

static void pyopencv_xfeatures2d_BoostDesc_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_BoostDesc_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::BoostDesc>& r)
{
    pyopencv_xfeatures2d_BoostDesc_t *m = PyObject_NEW(pyopencv_xfeatures2d_BoostDesc_t, &pyopencv_xfeatures2d_BoostDesc_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::BoostDesc>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_BoostDesc_Type))
    {
        failmsg("Expected cv::xfeatures2d::BoostDesc for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_BoostDesc_t*)src)->v.dynamicCast<cv::xfeatures2d::BoostDesc>();
    return true;
}


struct pyopencv_xfeatures2d_BriefDescriptorExtractor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_BriefDescriptorExtractor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_BriefDescriptorExtractor",
    sizeof(pyopencv_xfeatures2d_BriefDescriptorExtractor_t),
};

static void pyopencv_xfeatures2d_BriefDescriptorExtractor_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_BriefDescriptorExtractor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::BriefDescriptorExtractor>& r)
{
    pyopencv_xfeatures2d_BriefDescriptorExtractor_t *m = PyObject_NEW(pyopencv_xfeatures2d_BriefDescriptorExtractor_t, &pyopencv_xfeatures2d_BriefDescriptorExtractor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::BriefDescriptorExtractor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_BriefDescriptorExtractor_Type))
    {
        failmsg("Expected cv::xfeatures2d::BriefDescriptorExtractor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_BriefDescriptorExtractor_t*)src)->v.dynamicCast<cv::xfeatures2d::BriefDescriptorExtractor>();
    return true;
}


struct pyopencv_xfeatures2d_DAISY_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_DAISY_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_DAISY",
    sizeof(pyopencv_xfeatures2d_DAISY_t),
};

static void pyopencv_xfeatures2d_DAISY_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_DAISY_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::DAISY>& r)
{
    pyopencv_xfeatures2d_DAISY_t *m = PyObject_NEW(pyopencv_xfeatures2d_DAISY_t, &pyopencv_xfeatures2d_DAISY_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::DAISY>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_DAISY_Type))
    {
        failmsg("Expected cv::xfeatures2d::DAISY for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_DAISY_t*)src)->v.dynamicCast<cv::xfeatures2d::DAISY>();
    return true;
}


struct pyopencv_xfeatures2d_FREAK_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_FREAK_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_FREAK",
    sizeof(pyopencv_xfeatures2d_FREAK_t),
};

static void pyopencv_xfeatures2d_FREAK_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_FREAK_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::FREAK>& r)
{
    pyopencv_xfeatures2d_FREAK_t *m = PyObject_NEW(pyopencv_xfeatures2d_FREAK_t, &pyopencv_xfeatures2d_FREAK_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::FREAK>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_FREAK_Type))
    {
        failmsg("Expected cv::xfeatures2d::FREAK for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_FREAK_t*)src)->v.dynamicCast<cv::xfeatures2d::FREAK>();
    return true;
}


struct pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_HarrisLaplaceFeatureDetector",
    sizeof(pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_t),
};

static void pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector>& r)
{
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_t *m = PyObject_NEW(pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_t, &pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::HarrisLaplaceFeatureDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type))
    {
        failmsg("Expected cv::xfeatures2d::HarrisLaplaceFeatureDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_t*)src)->v.dynamicCast<cv::xfeatures2d::HarrisLaplaceFeatureDetector>();
    return true;
}


struct pyopencv_xfeatures2d_LATCH_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_LATCH_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_LATCH",
    sizeof(pyopencv_xfeatures2d_LATCH_t),
};

static void pyopencv_xfeatures2d_LATCH_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_LATCH_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::LATCH>& r)
{
    pyopencv_xfeatures2d_LATCH_t *m = PyObject_NEW(pyopencv_xfeatures2d_LATCH_t, &pyopencv_xfeatures2d_LATCH_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::LATCH>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_LATCH_Type))
    {
        failmsg("Expected cv::xfeatures2d::LATCH for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_LATCH_t*)src)->v.dynamicCast<cv::xfeatures2d::LATCH>();
    return true;
}


struct pyopencv_xfeatures2d_LUCID_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_LUCID_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_LUCID",
    sizeof(pyopencv_xfeatures2d_LUCID_t),
};

static void pyopencv_xfeatures2d_LUCID_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_LUCID_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::LUCID>& r)
{
    pyopencv_xfeatures2d_LUCID_t *m = PyObject_NEW(pyopencv_xfeatures2d_LUCID_t, &pyopencv_xfeatures2d_LUCID_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::LUCID>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_LUCID_Type))
    {
        failmsg("Expected cv::xfeatures2d::LUCID for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_LUCID_t*)src)->v.dynamicCast<cv::xfeatures2d::LUCID>();
    return true;
}


struct pyopencv_xfeatures2d_MSDDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_MSDDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_MSDDetector",
    sizeof(pyopencv_xfeatures2d_MSDDetector_t),
};

static void pyopencv_xfeatures2d_MSDDetector_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_MSDDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::MSDDetector>& r)
{
    pyopencv_xfeatures2d_MSDDetector_t *m = PyObject_NEW(pyopencv_xfeatures2d_MSDDetector_t, &pyopencv_xfeatures2d_MSDDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::MSDDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_MSDDetector_Type))
    {
        failmsg("Expected cv::xfeatures2d::MSDDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_MSDDetector_t*)src)->v.dynamicCast<cv::xfeatures2d::MSDDetector>();
    return true;
}


struct pyopencv_xfeatures2d_PCTSignatures_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_PCTSignatures_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_PCTSignatures",
    sizeof(pyopencv_xfeatures2d_PCTSignatures_t),
};

static void pyopencv_xfeatures2d_PCTSignatures_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::PCTSignatures>& r)
{
    pyopencv_xfeatures2d_PCTSignatures_t *m = PyObject_NEW(pyopencv_xfeatures2d_PCTSignatures_t, &pyopencv_xfeatures2d_PCTSignatures_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::PCTSignatures>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_PCTSignatures_Type))
    {
        failmsg("Expected cv::xfeatures2d::PCTSignatures for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_PCTSignatures_t*)src)->v.dynamicCast<cv::xfeatures2d::PCTSignatures>();
    return true;
}


struct pyopencv_xfeatures2d_PCTSignaturesSQFD_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_PCTSignaturesSQFD_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_PCTSignaturesSQFD",
    sizeof(pyopencv_xfeatures2d_PCTSignaturesSQFD_t),
};

static void pyopencv_xfeatures2d_PCTSignaturesSQFD_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_PCTSignaturesSQFD_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::PCTSignaturesSQFD>& r)
{
    pyopencv_xfeatures2d_PCTSignaturesSQFD_t *m = PyObject_NEW(pyopencv_xfeatures2d_PCTSignaturesSQFD_t, &pyopencv_xfeatures2d_PCTSignaturesSQFD_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::PCTSignaturesSQFD>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_PCTSignaturesSQFD_Type))
    {
        failmsg("Expected cv::xfeatures2d::PCTSignaturesSQFD for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_PCTSignaturesSQFD_t*)src)->v.dynamicCast<cv::xfeatures2d::PCTSignaturesSQFD>();
    return true;
}


struct pyopencv_xfeatures2d_SIFT_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_SIFT_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_SIFT",
    sizeof(pyopencv_xfeatures2d_SIFT_t),
};

static void pyopencv_xfeatures2d_SIFT_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_SIFT_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::SIFT>& r)
{
    pyopencv_xfeatures2d_SIFT_t *m = PyObject_NEW(pyopencv_xfeatures2d_SIFT_t, &pyopencv_xfeatures2d_SIFT_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::SIFT>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_SIFT_Type))
    {
        failmsg("Expected cv::xfeatures2d::SIFT for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_SIFT_t*)src)->v.dynamicCast<cv::xfeatures2d::SIFT>();
    return true;
}


struct pyopencv_xfeatures2d_SURF_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_SURF_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_SURF",
    sizeof(pyopencv_xfeatures2d_SURF_t),
};

static void pyopencv_xfeatures2d_SURF_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_SURF_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::SURF>& r)
{
    pyopencv_xfeatures2d_SURF_t *m = PyObject_NEW(pyopencv_xfeatures2d_SURF_t, &pyopencv_xfeatures2d_SURF_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::SURF>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_SURF_Type))
    {
        failmsg("Expected cv::xfeatures2d::SURF for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_SURF_t*)src)->v.dynamicCast<cv::xfeatures2d::SURF>();
    return true;
}


struct pyopencv_xfeatures2d_StarDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_StarDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_StarDetector",
    sizeof(pyopencv_xfeatures2d_StarDetector_t),
};

static void pyopencv_xfeatures2d_StarDetector_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_StarDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::StarDetector>& r)
{
    pyopencv_xfeatures2d_StarDetector_t *m = PyObject_NEW(pyopencv_xfeatures2d_StarDetector_t, &pyopencv_xfeatures2d_StarDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::StarDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_StarDetector_Type))
    {
        failmsg("Expected cv::xfeatures2d::StarDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_StarDetector_t*)src)->v.dynamicCast<cv::xfeatures2d::StarDetector>();
    return true;
}


struct pyopencv_xfeatures2d_VGG_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xfeatures2d_VGG_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xfeatures2d_VGG",
    sizeof(pyopencv_xfeatures2d_VGG_t),
};

static void pyopencv_xfeatures2d_VGG_dealloc(PyObject* self)
{
    ((pyopencv_xfeatures2d_VGG_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xfeatures2d::VGG>& r)
{
    pyopencv_xfeatures2d_VGG_t *m = PyObject_NEW(pyopencv_xfeatures2d_VGG_t, &pyopencv_xfeatures2d_VGG_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xfeatures2d::VGG>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xfeatures2d_VGG_Type))
    {
        failmsg("Expected cv::xfeatures2d::VGG for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xfeatures2d_VGG_t*)src)->v.dynamicCast<cv::xfeatures2d::VGG>();
    return true;
}


struct pyopencv_ximgproc_AdaptiveManifoldFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_AdaptiveManifoldFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_AdaptiveManifoldFilter",
    sizeof(pyopencv_ximgproc_AdaptiveManifoldFilter_t),
};

static void pyopencv_ximgproc_AdaptiveManifoldFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_AdaptiveManifoldFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::AdaptiveManifoldFilter>& r)
{
    pyopencv_ximgproc_AdaptiveManifoldFilter_t *m = PyObject_NEW(pyopencv_ximgproc_AdaptiveManifoldFilter_t, &pyopencv_ximgproc_AdaptiveManifoldFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::AdaptiveManifoldFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_AdaptiveManifoldFilter_Type))
    {
        failmsg("Expected cv::ximgproc::AdaptiveManifoldFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_AdaptiveManifoldFilter_t*)src)->v.dynamicCast<cv::ximgproc::AdaptiveManifoldFilter>();
    return true;
}


struct pyopencv_ximgproc_ContourFitting_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_ContourFitting_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_ContourFitting",
    sizeof(pyopencv_ximgproc_ContourFitting_t),
};

static void pyopencv_ximgproc_ContourFitting_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_ContourFitting_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::ContourFitting>& r)
{
    pyopencv_ximgproc_ContourFitting_t *m = PyObject_NEW(pyopencv_ximgproc_ContourFitting_t, &pyopencv_ximgproc_ContourFitting_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::ContourFitting>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_ContourFitting_Type))
    {
        failmsg("Expected cv::ximgproc::ContourFitting for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_ContourFitting_t*)src)->v.dynamicCast<cv::ximgproc::ContourFitting>();
    return true;
}


struct pyopencv_ximgproc_DTFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_DTFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_DTFilter",
    sizeof(pyopencv_ximgproc_DTFilter_t),
};

static void pyopencv_ximgproc_DTFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_DTFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::DTFilter>& r)
{
    pyopencv_ximgproc_DTFilter_t *m = PyObject_NEW(pyopencv_ximgproc_DTFilter_t, &pyopencv_ximgproc_DTFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::DTFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_DTFilter_Type))
    {
        failmsg("Expected cv::ximgproc::DTFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_DTFilter_t*)src)->v.dynamicCast<cv::ximgproc::DTFilter>();
    return true;
}


struct pyopencv_ximgproc_DisparityFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_DisparityFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_DisparityFilter",
    sizeof(pyopencv_ximgproc_DisparityFilter_t),
};

static void pyopencv_ximgproc_DisparityFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_DisparityFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::DisparityFilter>& r)
{
    pyopencv_ximgproc_DisparityFilter_t *m = PyObject_NEW(pyopencv_ximgproc_DisparityFilter_t, &pyopencv_ximgproc_DisparityFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::DisparityFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_DisparityFilter_Type))
    {
        failmsg("Expected cv::ximgproc::DisparityFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_DisparityFilter_t*)src)->v.dynamicCast<cv::ximgproc::DisparityFilter>();
    return true;
}


struct pyopencv_ximgproc_DisparityWLSFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_DisparityWLSFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_DisparityWLSFilter",
    sizeof(pyopencv_ximgproc_DisparityWLSFilter_t),
};

static void pyopencv_ximgproc_DisparityWLSFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::DisparityWLSFilter>& r)
{
    pyopencv_ximgproc_DisparityWLSFilter_t *m = PyObject_NEW(pyopencv_ximgproc_DisparityWLSFilter_t, &pyopencv_ximgproc_DisparityWLSFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::DisparityWLSFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_DisparityWLSFilter_Type))
    {
        failmsg("Expected cv::ximgproc::DisparityWLSFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_DisparityWLSFilter_t*)src)->v.dynamicCast<cv::ximgproc::DisparityWLSFilter>();
    return true;
}


struct pyopencv_ximgproc_EdgeAwareInterpolator_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_EdgeAwareInterpolator_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_EdgeAwareInterpolator",
    sizeof(pyopencv_ximgproc_EdgeAwareInterpolator_t),
};

static void pyopencv_ximgproc_EdgeAwareInterpolator_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::EdgeAwareInterpolator>& r)
{
    pyopencv_ximgproc_EdgeAwareInterpolator_t *m = PyObject_NEW(pyopencv_ximgproc_EdgeAwareInterpolator_t, &pyopencv_ximgproc_EdgeAwareInterpolator_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::EdgeAwareInterpolator>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
    {
        failmsg("Expected cv::ximgproc::EdgeAwareInterpolator for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_EdgeAwareInterpolator_t*)src)->v.dynamicCast<cv::ximgproc::EdgeAwareInterpolator>();
    return true;
}


struct pyopencv_ximgproc_EdgeBoxes_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_EdgeBoxes_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_EdgeBoxes",
    sizeof(pyopencv_ximgproc_EdgeBoxes_t),
};

static void pyopencv_ximgproc_EdgeBoxes_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::EdgeBoxes>& r)
{
    pyopencv_ximgproc_EdgeBoxes_t *m = PyObject_NEW(pyopencv_ximgproc_EdgeBoxes_t, &pyopencv_ximgproc_EdgeBoxes_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::EdgeBoxes>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_EdgeBoxes_Type))
    {
        failmsg("Expected cv::ximgproc::EdgeBoxes for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_EdgeBoxes_t*)src)->v.dynamicCast<cv::ximgproc::EdgeBoxes>();
    return true;
}


struct pyopencv_ximgproc_FastGlobalSmootherFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_FastGlobalSmootherFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_FastGlobalSmootherFilter",
    sizeof(pyopencv_ximgproc_FastGlobalSmootherFilter_t),
};

static void pyopencv_ximgproc_FastGlobalSmootherFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_FastGlobalSmootherFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::FastGlobalSmootherFilter>& r)
{
    pyopencv_ximgproc_FastGlobalSmootherFilter_t *m = PyObject_NEW(pyopencv_ximgproc_FastGlobalSmootherFilter_t, &pyopencv_ximgproc_FastGlobalSmootherFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::FastGlobalSmootherFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_FastGlobalSmootherFilter_Type))
    {
        failmsg("Expected cv::ximgproc::FastGlobalSmootherFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_FastGlobalSmootherFilter_t*)src)->v.dynamicCast<cv::ximgproc::FastGlobalSmootherFilter>();
    return true;
}


struct pyopencv_ximgproc_FastLineDetector_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_FastLineDetector_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_FastLineDetector",
    sizeof(pyopencv_ximgproc_FastLineDetector_t),
};

static void pyopencv_ximgproc_FastLineDetector_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_FastLineDetector_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::FastLineDetector>& r)
{
    pyopencv_ximgproc_FastLineDetector_t *m = PyObject_NEW(pyopencv_ximgproc_FastLineDetector_t, &pyopencv_ximgproc_FastLineDetector_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::FastLineDetector>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_FastLineDetector_Type))
    {
        failmsg("Expected cv::ximgproc::FastLineDetector for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_FastLineDetector_t*)src)->v.dynamicCast<cv::ximgproc::FastLineDetector>();
    return true;
}


struct pyopencv_ximgproc_GuidedFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_GuidedFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_GuidedFilter",
    sizeof(pyopencv_ximgproc_GuidedFilter_t),
};

static void pyopencv_ximgproc_GuidedFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_GuidedFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::GuidedFilter>& r)
{
    pyopencv_ximgproc_GuidedFilter_t *m = PyObject_NEW(pyopencv_ximgproc_GuidedFilter_t, &pyopencv_ximgproc_GuidedFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::GuidedFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_GuidedFilter_Type))
    {
        failmsg("Expected cv::ximgproc::GuidedFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_GuidedFilter_t*)src)->v.dynamicCast<cv::ximgproc::GuidedFilter>();
    return true;
}


struct pyopencv_ximgproc_RFFeatureGetter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_RFFeatureGetter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_RFFeatureGetter",
    sizeof(pyopencv_ximgproc_RFFeatureGetter_t),
};

static void pyopencv_ximgproc_RFFeatureGetter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_RFFeatureGetter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::RFFeatureGetter>& r)
{
    pyopencv_ximgproc_RFFeatureGetter_t *m = PyObject_NEW(pyopencv_ximgproc_RFFeatureGetter_t, &pyopencv_ximgproc_RFFeatureGetter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::RFFeatureGetter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_RFFeatureGetter_Type))
    {
        failmsg("Expected cv::ximgproc::RFFeatureGetter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_RFFeatureGetter_t*)src)->v.dynamicCast<cv::ximgproc::RFFeatureGetter>();
    return true;
}


struct pyopencv_ximgproc_RidgeDetectionFilter_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_RidgeDetectionFilter_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_RidgeDetectionFilter",
    sizeof(pyopencv_ximgproc_RidgeDetectionFilter_t),
};

static void pyopencv_ximgproc_RidgeDetectionFilter_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_RidgeDetectionFilter_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::RidgeDetectionFilter>& r)
{
    pyopencv_ximgproc_RidgeDetectionFilter_t *m = PyObject_NEW(pyopencv_ximgproc_RidgeDetectionFilter_t, &pyopencv_ximgproc_RidgeDetectionFilter_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::RidgeDetectionFilter>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_RidgeDetectionFilter_Type))
    {
        failmsg("Expected cv::ximgproc::RidgeDetectionFilter for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_RidgeDetectionFilter_t*)src)->v.dynamicCast<cv::ximgproc::RidgeDetectionFilter>();
    return true;
}


struct pyopencv_ximgproc_SparseMatchInterpolator_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_SparseMatchInterpolator_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_SparseMatchInterpolator",
    sizeof(pyopencv_ximgproc_SparseMatchInterpolator_t),
};

static void pyopencv_ximgproc_SparseMatchInterpolator_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_SparseMatchInterpolator_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::SparseMatchInterpolator>& r)
{
    pyopencv_ximgproc_SparseMatchInterpolator_t *m = PyObject_NEW(pyopencv_ximgproc_SparseMatchInterpolator_t, &pyopencv_ximgproc_SparseMatchInterpolator_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::SparseMatchInterpolator>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_SparseMatchInterpolator_Type))
    {
        failmsg("Expected cv::ximgproc::SparseMatchInterpolator for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_SparseMatchInterpolator_t*)src)->v.dynamicCast<cv::ximgproc::SparseMatchInterpolator>();
    return true;
}


struct pyopencv_ximgproc_StructuredEdgeDetection_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_StructuredEdgeDetection_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_StructuredEdgeDetection",
    sizeof(pyopencv_ximgproc_StructuredEdgeDetection_t),
};

static void pyopencv_ximgproc_StructuredEdgeDetection_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_StructuredEdgeDetection_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::StructuredEdgeDetection>& r)
{
    pyopencv_ximgproc_StructuredEdgeDetection_t *m = PyObject_NEW(pyopencv_ximgproc_StructuredEdgeDetection_t, &pyopencv_ximgproc_StructuredEdgeDetection_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::StructuredEdgeDetection>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_StructuredEdgeDetection_Type))
    {
        failmsg("Expected cv::ximgproc::StructuredEdgeDetection for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_StructuredEdgeDetection_t*)src)->v.dynamicCast<cv::ximgproc::StructuredEdgeDetection>();
    return true;
}


struct pyopencv_ximgproc_SuperpixelLSC_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_SuperpixelLSC_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_SuperpixelLSC",
    sizeof(pyopencv_ximgproc_SuperpixelLSC_t),
};

static void pyopencv_ximgproc_SuperpixelLSC_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_SuperpixelLSC_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::SuperpixelLSC>& r)
{
    pyopencv_ximgproc_SuperpixelLSC_t *m = PyObject_NEW(pyopencv_ximgproc_SuperpixelLSC_t, &pyopencv_ximgproc_SuperpixelLSC_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::SuperpixelLSC>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_SuperpixelLSC_Type))
    {
        failmsg("Expected cv::ximgproc::SuperpixelLSC for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_SuperpixelLSC_t*)src)->v.dynamicCast<cv::ximgproc::SuperpixelLSC>();
    return true;
}


struct pyopencv_ximgproc_SuperpixelSEEDS_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_SuperpixelSEEDS_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_SuperpixelSEEDS",
    sizeof(pyopencv_ximgproc_SuperpixelSEEDS_t),
};

static void pyopencv_ximgproc_SuperpixelSEEDS_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_SuperpixelSEEDS_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::SuperpixelSEEDS>& r)
{
    pyopencv_ximgproc_SuperpixelSEEDS_t *m = PyObject_NEW(pyopencv_ximgproc_SuperpixelSEEDS_t, &pyopencv_ximgproc_SuperpixelSEEDS_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::SuperpixelSEEDS>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_SuperpixelSEEDS_Type))
    {
        failmsg("Expected cv::ximgproc::SuperpixelSEEDS for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_SuperpixelSEEDS_t*)src)->v.dynamicCast<cv::ximgproc::SuperpixelSEEDS>();
    return true;
}


struct pyopencv_ximgproc_SuperpixelSLIC_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_SuperpixelSLIC_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_SuperpixelSLIC",
    sizeof(pyopencv_ximgproc_SuperpixelSLIC_t),
};

static void pyopencv_ximgproc_SuperpixelSLIC_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_SuperpixelSLIC_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::SuperpixelSLIC>& r)
{
    pyopencv_ximgproc_SuperpixelSLIC_t *m = PyObject_NEW(pyopencv_ximgproc_SuperpixelSLIC_t, &pyopencv_ximgproc_SuperpixelSLIC_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::SuperpixelSLIC>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_SuperpixelSLIC_Type))
    {
        failmsg("Expected cv::ximgproc::SuperpixelSLIC for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_SuperpixelSLIC_t*)src)->v.dynamicCast<cv::ximgproc::SuperpixelSLIC>();
    return true;
}


struct pyopencv_ximgproc_segmentation_GraphSegmentation_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_GraphSegmentation_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_GraphSegmentation",
    sizeof(pyopencv_ximgproc_segmentation_GraphSegmentation_t),
};

static void pyopencv_ximgproc_segmentation_GraphSegmentation_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::GraphSegmentation>& r)
{
    pyopencv_ximgproc_segmentation_GraphSegmentation_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_GraphSegmentation_t, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::GraphSegmentation>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::GraphSegmentation for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::GraphSegmentation>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentation",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentation>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentation for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentation>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentationStrategy",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyColor>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyFill>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentationStrategySize",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategySize>();
    return true;
}


struct pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture",
    sizeof(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_t),
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_dealloc(PyObject* self)
{
    ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture>& r)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_t *m = PyObject_NEW(pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_t, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type))
    {
        failmsg("Expected cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_t*)src)->v.dynamicCast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyTexture>();
    return true;
}


struct pyopencv_xphoto_GrayworldWB_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xphoto_GrayworldWB_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xphoto_GrayworldWB",
    sizeof(pyopencv_xphoto_GrayworldWB_t),
};

static void pyopencv_xphoto_GrayworldWB_dealloc(PyObject* self)
{
    ((pyopencv_xphoto_GrayworldWB_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xphoto::GrayworldWB>& r)
{
    pyopencv_xphoto_GrayworldWB_t *m = PyObject_NEW(pyopencv_xphoto_GrayworldWB_t, &pyopencv_xphoto_GrayworldWB_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xphoto::GrayworldWB>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xphoto_GrayworldWB_Type))
    {
        failmsg("Expected cv::xphoto::GrayworldWB for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xphoto_GrayworldWB_t*)src)->v.dynamicCast<cv::xphoto::GrayworldWB>();
    return true;
}


struct pyopencv_xphoto_LearningBasedWB_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xphoto_LearningBasedWB_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xphoto_LearningBasedWB",
    sizeof(pyopencv_xphoto_LearningBasedWB_t),
};

static void pyopencv_xphoto_LearningBasedWB_dealloc(PyObject* self)
{
    ((pyopencv_xphoto_LearningBasedWB_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xphoto::LearningBasedWB>& r)
{
    pyopencv_xphoto_LearningBasedWB_t *m = PyObject_NEW(pyopencv_xphoto_LearningBasedWB_t, &pyopencv_xphoto_LearningBasedWB_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xphoto::LearningBasedWB>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xphoto_LearningBasedWB_Type))
    {
        failmsg("Expected cv::xphoto::LearningBasedWB for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xphoto_LearningBasedWB_t*)src)->v.dynamicCast<cv::xphoto::LearningBasedWB>();
    return true;
}


struct pyopencv_xphoto_SimpleWB_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xphoto_SimpleWB_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xphoto_SimpleWB",
    sizeof(pyopencv_xphoto_SimpleWB_t),
};

static void pyopencv_xphoto_SimpleWB_dealloc(PyObject* self)
{
    ((pyopencv_xphoto_SimpleWB_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xphoto::SimpleWB>& r)
{
    pyopencv_xphoto_SimpleWB_t *m = PyObject_NEW(pyopencv_xphoto_SimpleWB_t, &pyopencv_xphoto_SimpleWB_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xphoto::SimpleWB>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xphoto_SimpleWB_Type))
    {
        failmsg("Expected cv::xphoto::SimpleWB for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xphoto_SimpleWB_t*)src)->v.dynamicCast<cv::xphoto::SimpleWB>();
    return true;
}


struct pyopencv_xphoto_WhiteBalancer_t
{
    PyObject_HEAD
    Ptr<cv::Algorithm> v;
};

static PyTypeObject pyopencv_xphoto_WhiteBalancer_Type =
{
    CV_PYTHON_TYPE_HEAD_INIT()
    MODULESTR".xphoto_WhiteBalancer",
    sizeof(pyopencv_xphoto_WhiteBalancer_t),
};

static void pyopencv_xphoto_WhiteBalancer_dealloc(PyObject* self)
{
    ((pyopencv_xphoto_WhiteBalancer_t*)self)->v.release();
    PyObject_Del(self);
}

template<> PyObject* pyopencv_from(const Ptr<cv::xphoto::WhiteBalancer>& r)
{
    pyopencv_xphoto_WhiteBalancer_t *m = PyObject_NEW(pyopencv_xphoto_WhiteBalancer_t, &pyopencv_xphoto_WhiteBalancer_Type);
    new (&(m->v)) Ptr<cv::Algorithm>(); // init Ptr with placement new
    m->v = r;
    return (PyObject*)m;
}

template<> bool pyopencv_to(PyObject* src, Ptr<cv::xphoto::WhiteBalancer>& dst, const char* name)
{
    if( src == NULL || src == Py_None )
        return true;
    if(!PyObject_TypeCheck(src, &pyopencv_xphoto_WhiteBalancer_Type))
    {
        failmsg("Expected cv::xphoto::WhiteBalancer for argument '%s'", name);
        return false;
    }
    dst = ((pyopencv_xphoto_WhiteBalancer_t*)src)->v.dynamicCast<cv::xphoto::WhiteBalancer>();
    return true;
}


static PyObject* pyopencv_Algorithm_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<Algorithm %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_Algorithm_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_Algorithm_clear(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Algorithm* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Algorithm_Type))
        _self_ = ((pyopencv_Algorithm_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Algorithm_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Algorithm* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Algorithm_Type))
        _self_ = ((pyopencv_Algorithm_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Algorithm_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Algorithm* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Algorithm_Type))
        _self_ = ((pyopencv_Algorithm_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Algorithm_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Algorithm* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Algorithm_Type))
        _self_ = ((pyopencv_Algorithm_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    PyObject* pyobj_fn = NULL;
    FileNode fn;

    const char* keywords[] = { "fn", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Algorithm.read", (char**)keywords, &pyobj_fn) &&
        pyopencv_to(pyobj_fn, fn, ArgInfo("fn", 0)) )
    {
        ERRWRAP2(_self_->read(fn));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Algorithm_save(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Algorithm* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Algorithm_Type))
        _self_ = ((pyopencv_Algorithm_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Algorithm.save", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(_self_->save(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Algorithm_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Algorithm* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Algorithm_Type))
        _self_ = ((pyopencv_Algorithm_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Algorithm' or its derivative)");
    PyObject* pyobj_fs = NULL;
    Ptr<FileStorage> fs;
    PyObject* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Algorithm.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        pyopencv_to(pyobj_fs, fs, ArgInfo("fs", 0)) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) )
    {
        ERRWRAP2(_self_->write(fs, name));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_Algorithm_methods[] =
{
    {"clear", CV_PY_FN_WITH_KW_(pyopencv_cv_Algorithm_clear, 0), "clear() -> None\n.   @brief Clears the algorithm state"},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_Algorithm_empty, 0), "empty() -> retval\n.   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read"},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_Algorithm_getDefaultName, 0), "getDefaultName() -> retval\n.   Returns the algorithm string identifier.\n.   This string is used as top level xml/yml node tag when the object is saved to a file or string."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_Algorithm_read, 0), "read(fn) -> None\n.   @brief Reads algorithm parameters from a file storage"},
    {"save", CV_PY_FN_WITH_KW_(pyopencv_cv_Algorithm_save, 0), "save(filename) -> None\n.   Saves the algorithm to a file.\n.   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs)."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_Algorithm_write, 0), "write(fs[, name]) -> None\n.   @brief simplified API for language bindings\n.   * @overload"},

    {NULL,          NULL}
};

static void pyopencv_Algorithm_specials(void)
{
    pyopencv_Algorithm_Type.tp_base = NULL;
    pyopencv_Algorithm_Type.tp_dealloc = pyopencv_Algorithm_dealloc;
    pyopencv_Algorithm_Type.tp_repr = pyopencv_Algorithm_repr;
    pyopencv_Algorithm_Type.tp_getset = pyopencv_Algorithm_getseters;
    pyopencv_Algorithm_Type.tp_init = (initproc)0;
    pyopencv_Algorithm_Type.tp_methods = pyopencv_Algorithm_methods;
}

static PyObject* pyopencv_FileStorage_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<FileStorage %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_FileStorage_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_FileStorage_FileStorage(pyopencv_FileStorage_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::FileStorage>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::FileStorage()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int flags=0;
    PyObject* pyobj_encoding = NULL;
    String encoding;

    const char* keywords[] = { "filename", "flags", "encoding", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|O:FileStorage", (char**)keywords, &pyobj_filename, &flags, &pyobj_encoding) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_encoding, encoding, ArgInfo("encoding", 0)) )
    {
        new (&(self->v)) Ptr<cv::FileStorage>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::FileStorage(filename, flags, encoding)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_FileStorage_getFirstTopLevelNode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    FileNode retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFirstTopLevelNode());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_getFormat(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFormat());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_getNode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    char* nodename=(char*)"";
    FileNode retval;

    const char* keywords[] = { "nodename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "s:FileStorage.getNode", (char**)keywords, &nodename) )
    {
        ERRWRAP2(retval = _self_->operator[](nodename));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_isOpened(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isOpened());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_open(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;
    int flags=0;
    PyObject* pyobj_encoding = NULL;
    String encoding;
    bool retval;

    const char* keywords[] = { "filename", "flags", "encoding", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|O:FileStorage.open", (char**)keywords, &pyobj_filename, &flags, &pyobj_encoding) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_encoding, encoding, ArgInfo("encoding", 0)) )
    {
        ERRWRAP2(retval = _self_->open(filename, flags, encoding));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_release(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_releaseAndGetString(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->releaseAndGetString());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_root(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    int streamidx=0;
    FileNode retval;

    const char* keywords[] = { "streamidx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:FileStorage.root", (char**)keywords, &streamidx) )
    {
        ERRWRAP2(retval = _self_->root(streamidx));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    {
    PyObject* pyobj_name = NULL;
    String name;
    int val=0;

    const char* keywords[] = { "name", "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:FileStorage.write", (char**)keywords, &pyobj_name, &val) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) )
    {
        ERRWRAP2(_self_->write(name, val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_name = NULL;
    String name;
    double val=0;

    const char* keywords[] = { "name", "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Od:FileStorage.write", (char**)keywords, &pyobj_name, &val) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) )
    {
        ERRWRAP2(_self_->write(name, val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_name = NULL;
    String name;
    PyObject* pyobj_val = NULL;
    String val;

    const char* keywords[] = { "name", "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->write(name, val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_name = NULL;
    String name;
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "name", "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->write(name, val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_name = NULL;
    String name;
    PyObject* pyobj_val = NULL;
    UMat val;

    const char* keywords[] = { "name", "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:FileStorage.write", (char**)keywords, &pyobj_name, &pyobj_val) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->write(name, val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileStorage_writeComment(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileStorage* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileStorage_Type))
        _self_ = ((pyopencv_FileStorage_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileStorage' or its derivative)");
    PyObject* pyobj_comment = NULL;
    String comment;
    bool append=false;

    const char* keywords[] = { "comment", "append", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|b:FileStorage.writeComment", (char**)keywords, &pyobj_comment, &append) &&
        pyopencv_to(pyobj_comment, comment, ArgInfo("comment", 0)) )
    {
        ERRWRAP2(_self_->writeComment(comment, append));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_FileStorage_methods[] =
{
    {"getFirstTopLevelNode", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_getFirstTopLevelNode, 0), "getFirstTopLevelNode() -> retval\n.   @brief Returns the first element of the top-level mapping.\n.   @returns The first element of the top-level mapping."},
    {"getFormat", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_getFormat, 0), "getFormat() -> retval\n.   @brief Returns the current format.\n.   * @returns The current format, see FileStorage::Mode"},
    {"getNode", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_getNode, 0), "getNode(nodename) -> retval\n.   @overload"},
    {"isOpened", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_isOpened, 0), "isOpened() -> retval\n.   @brief Checks whether the file is opened.\n.   \n.   @returns true if the object is associated with the current file and false otherwise. It is a\n.   good practice to call this method after you tried to open a file."},
    {"open", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_open, 0), "open(filename, flags[, encoding]) -> retval\n.   @brief Opens a file.\n.   \n.   See description of parameters in FileStorage::FileStorage. The method calls FileStorage::release\n.   before opening the file.\n.   @param filename Name of the file to open or the text string to read the data from.\n.   Extension of the file (.xml, .yml/.yaml or .json) determines its format (XML, YAML or JSON\n.   respectively). Also you can append .gz to work with compressed files, for example myHugeMatrix.xml.gz. If both\n.   FileStorage::WRITE and FileStorage::MEMORY flags are specified, source is used just to specify\n.   the output file format (e.g. mydata.xml, .yml etc.). A file name can also contain parameters.\n.   You can use this format, \"*?base64\" (e.g. \"file.json?base64\" (case sensitive)), as an alternative to\n.   FileStorage::BASE64 flag.\n.   @param flags Mode of operation. One of FileStorage::Mode\n.   @param encoding Encoding of the file. Note that UTF-16 XML encoding is not supported currently and\n.   you should use 8-bit encoding instead of it."},
    {"release", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_release, 0), "release() -> None\n.   @brief Closes the file and releases all the memory buffers.\n.   \n.   Call this method after all I/O operations with the storage are finished."},
    {"releaseAndGetString", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_releaseAndGetString, 0), "releaseAndGetString() -> retval\n.   @brief Closes the file and releases all the memory buffers.\n.   \n.   Call this method after all I/O operations with the storage are finished. If the storage was\n.   opened for writing data and FileStorage::WRITE was specified"},
    {"root", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_root, 0), "root([, streamidx]) -> retval\n.   @brief Returns the top-level mapping\n.   @param streamidx Zero-based index of the stream. In most cases there is only one stream in the file.\n.   However, YAML supports multiple streams and so there can be several.\n.   @returns The top-level mapping."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_write, 0), "write(name, val) -> None\n.   * @brief Simplified writing API to use with bindings.\n.   * @param name Name of the written object\n.   * @param val Value of the written object"},
    {"writeComment", CV_PY_FN_WITH_KW_(pyopencv_cv_FileStorage_writeComment, 0), "writeComment(comment[, append]) -> None\n.   @brief Writes a comment.\n.   \n.   The function writes a comment into file storage. The comments are skipped when the storage is read.\n.   @param comment The written comment, single-line or multi-line\n.   @param append If true, the function tries to put the comment at the end of current line.\n.   Else if the comment is multi-line, or if it does not fit at the end of the current\n.   line, the comment starts a new line."},

    {NULL,          NULL}
};

static void pyopencv_FileStorage_specials(void)
{
    pyopencv_FileStorage_Type.tp_base = NULL;
    pyopencv_FileStorage_Type.tp_dealloc = pyopencv_FileStorage_dealloc;
    pyopencv_FileStorage_Type.tp_repr = pyopencv_FileStorage_repr;
    pyopencv_FileStorage_Type.tp_getset = pyopencv_FileStorage_getseters;
    pyopencv_FileStorage_Type.tp_init = (initproc)pyopencv_cv_FileStorage_FileStorage;
    pyopencv_FileStorage_Type.tp_methods = pyopencv_FileStorage_methods;
}

static PyObject* pyopencv_FileNode_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<FileNode %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_FileNode_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_FileNode_FileNode(pyopencv_FileNode_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::FileNode());
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_FileNode_at(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    int i=0;
    FileNode retval;

    const char* keywords[] = { "i", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FileNode.at", (char**)keywords, &i) )
    {
        ERRWRAP2(retval = _self_->operator[](i));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_getNode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    char* nodename=(char*)"";
    FileNode retval;

    const char* keywords[] = { "nodename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "s:FileNode.getNode", (char**)keywords, &nodename) )
    {
        ERRWRAP2(retval = _self_->operator[](nodename));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isInt(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isInt());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isNamed(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isNamed());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isNone(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isNone());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isReal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isReal());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isSeq(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isSeq());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_isString(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isString());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_mat(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->mat());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_name(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->name());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_real(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->real());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_size(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    size_t retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->size());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_string(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->string());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FileNode_type(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FileNode* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FileNode_Type))
        _self_ = &((pyopencv_FileNode_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FileNode' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->type());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_FileNode_methods[] =
{
    {"at", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_at, 0), "at(i) -> retval\n.   @overload\n.   @param i Index of an element in the sequence node."},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_empty, 0), "empty() -> retval\n."},
    {"getNode", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_getNode, 0), "getNode(nodename) -> retval\n.   @overload\n.   @param nodename Name of an element in the mapping node."},
    {"isInt", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isInt, 0), "isInt() -> retval\n."},
    {"isMap", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isMap, 0), "isMap() -> retval\n."},
    {"isNamed", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isNamed, 0), "isNamed() -> retval\n."},
    {"isNone", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isNone, 0), "isNone() -> retval\n."},
    {"isReal", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isReal, 0), "isReal() -> retval\n."},
    {"isSeq", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isSeq, 0), "isSeq() -> retval\n."},
    {"isString", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_isString, 0), "isString() -> retval\n."},
    {"mat", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_mat, 0), "mat() -> retval\n."},
    {"name", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_name, 0), "name() -> retval\n."},
    {"real", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_real, 0), "real() -> retval\n.   @brief Reads node elements to the buffer with the specified format.\n.   \n.   Usually it is more convenient to use operator `>>` instead of this method.\n.   @param fmt Specification of each array element. See @ref format_spec \"format specification\"\n.   @param vec Pointer to the destination array.\n.   @param len Number of elements to read. If it is greater than number of remaining elements then all\n.   of them will be read."},
    {"size", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_size, 0), "size() -> retval\n."},
    {"string", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_string, 0), "string() -> retval\n."},
    {"type", CV_PY_FN_WITH_KW_(pyopencv_cv_FileNode_type, 0), "type() -> retval\n.   @brief Returns type of the node.\n.   @returns Type of the node. See FileNode::Type"},

    {NULL,          NULL}
};

static void pyopencv_FileNode_specials(void)
{
    pyopencv_FileNode_Type.tp_base = NULL;
    pyopencv_FileNode_Type.tp_dealloc = pyopencv_FileNode_dealloc;
    pyopencv_FileNode_Type.tp_repr = pyopencv_FileNode_repr;
    pyopencv_FileNode_Type.tp_getset = pyopencv_FileNode_getseters;
    pyopencv_FileNode_Type.tp_init = (initproc)pyopencv_cv_FileNode_FileNode;
    pyopencv_FileNode_Type.tp_methods = pyopencv_FileNode_methods;
}

static PyObject* pyopencv_KeyPoint_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<KeyPoint %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_KeyPoint_get_angle(pyopencv_KeyPoint_t* p, void *closure)
{
    return pyopencv_from(p->v.angle);
}

static int pyopencv_KeyPoint_set_angle(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the angle attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.angle) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_class_id(pyopencv_KeyPoint_t* p, void *closure)
{
    return pyopencv_from(p->v.class_id);
}

static int pyopencv_KeyPoint_set_class_id(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the class_id attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.class_id) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_octave(pyopencv_KeyPoint_t* p, void *closure)
{
    return pyopencv_from(p->v.octave);
}

static int pyopencv_KeyPoint_set_octave(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the octave attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.octave) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_pt(pyopencv_KeyPoint_t* p, void *closure)
{
    return pyopencv_from(p->v.pt);
}

static int pyopencv_KeyPoint_set_pt(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the pt attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.pt) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_response(pyopencv_KeyPoint_t* p, void *closure)
{
    return pyopencv_from(p->v.response);
}

static int pyopencv_KeyPoint_set_response(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the response attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.response) ? 0 : -1;
}

static PyObject* pyopencv_KeyPoint_get_size(pyopencv_KeyPoint_t* p, void *closure)
{
    return pyopencv_from(p->v.size);
}

static int pyopencv_KeyPoint_set_size(pyopencv_KeyPoint_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the size attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.size) ? 0 : -1;
}


static PyGetSetDef pyopencv_KeyPoint_getseters[] =
{
    {(char*)"angle", (getter)pyopencv_KeyPoint_get_angle, (setter)pyopencv_KeyPoint_set_angle, (char*)"angle", NULL},
    {(char*)"class_id", (getter)pyopencv_KeyPoint_get_class_id, (setter)pyopencv_KeyPoint_set_class_id, (char*)"class_id", NULL},
    {(char*)"octave", (getter)pyopencv_KeyPoint_get_octave, (setter)pyopencv_KeyPoint_set_octave, (char*)"octave", NULL},
    {(char*)"pt", (getter)pyopencv_KeyPoint_get_pt, (setter)pyopencv_KeyPoint_set_pt, (char*)"pt", NULL},
    {(char*)"response", (getter)pyopencv_KeyPoint_get_response, (setter)pyopencv_KeyPoint_set_response, (char*)"response", NULL},
    {(char*)"size", (getter)pyopencv_KeyPoint_get_size, (setter)pyopencv_KeyPoint_set_size, (char*)"size", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_KeyPoint_KeyPoint(pyopencv_KeyPoint_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::KeyPoint());
        return 0;
    }
    }
    PyErr_Clear();

    {
    float x=0.f;
    float y=0.f;
    float _size=0.f;
    float _angle=-1;
    float _response=0;
    int _octave=0;
    int _class_id=-1;

    const char* keywords[] = { "x", "y", "_size", "_angle", "_response", "_octave", "_class_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "fff|ffii:KeyPoint", (char**)keywords, &x, &y, &_size, &_angle, &_response, &_octave, &_class_id) )
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::KeyPoint(x, y, _size, _angle, _response, _octave, _class_id));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_KeyPoint_convert_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {
    PyObject* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    vector_Point2f points2f;
    PyObject* pyobj_keypointIndexes = NULL;
    vector_int keypointIndexes=std::vector<int>();

    const char* keywords[] = { "keypoints", "keypointIndexes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:KeyPoint.convert", (char**)keywords, &pyobj_keypoints, &pyobj_keypointIndexes) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 0)) &&
        pyopencv_to(pyobj_keypointIndexes, keypointIndexes, ArgInfo("keypointIndexes", 0)) )
    {
        ERRWRAP2(cv::KeyPoint::convert(keypoints, points2f, keypointIndexes));
        return pyopencv_from(points2f);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_points2f = NULL;
    vector_Point2f points2f;
    vector_KeyPoint keypoints;
    float size=1;
    float response=1;
    int octave=0;
    int class_id=-1;

    const char* keywords[] = { "points2f", "size", "response", "octave", "class_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|ffii:KeyPoint.convert", (char**)keywords, &pyobj_points2f, &size, &response, &octave, &class_id) &&
        pyopencv_to(pyobj_points2f, points2f, ArgInfo("points2f", 0)) )
    {
        ERRWRAP2(cv::KeyPoint::convert(points2f, keypoints, size, response, octave, class_id));
        return pyopencv_from(keypoints);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_KeyPoint_overlap_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_kp1 = NULL;
    KeyPoint kp1;
    PyObject* pyobj_kp2 = NULL;
    KeyPoint kp2;
    float retval;

    const char* keywords[] = { "kp1", "kp2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:KeyPoint.overlap", (char**)keywords, &pyobj_kp1, &pyobj_kp2) &&
        pyopencv_to(pyobj_kp1, kp1, ArgInfo("kp1", 0)) &&
        pyopencv_to(pyobj_kp2, kp2, ArgInfo("kp2", 0)) )
    {
        ERRWRAP2(retval = cv::KeyPoint::overlap(kp1, kp2));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_KeyPoint_methods[] =
{
    {"convert", CV_PY_FN_WITH_KW_(pyopencv_cv_KeyPoint_convert_cls, METH_CLASS), "convert(keypoints[, keypointIndexes]) -> points2f\n.   This method converts vector of keypoints to vector of points or the reverse, where each keypoint is\n.   assigned the same size and the same orientation.\n.   \n.   @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n.   @param points2f Array of (x,y) coordinates of each keypoint\n.   @param keypointIndexes Array of indexes of keypoints to be converted to points. (Acts like a mask to\n.   convert only specified keypoints)\n\n\n\nconvert(points2f[, size[, response[, octave[, class_id]]]]) -> keypoints\n.   @overload\n.   @param points2f Array of (x,y) coordinates of each keypoint\n.   @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n.   @param size keypoint diameter\n.   @param response keypoint detector response on the keypoint (that is, strength of the keypoint)\n.   @param octave pyramid octave in which the keypoint has been detected\n.   @param class_id object id"},
    {"overlap", CV_PY_FN_WITH_KW_(pyopencv_cv_KeyPoint_overlap_cls, METH_CLASS), "overlap(kp1, kp2) -> retval\n.   This method computes overlap for pair of keypoints. Overlap is the ratio between area of keypoint\n.   regions' intersection and area of keypoint regions' union (considering keypoint region as circle).\n.   If they don't overlap, we get zero. If they coincide at same location with same size, we get 1.\n.   @param kp1 First keypoint\n.   @param kp2 Second keypoint"},

    {NULL,          NULL}
};

static void pyopencv_KeyPoint_specials(void)
{
    pyopencv_KeyPoint_Type.tp_base = NULL;
    pyopencv_KeyPoint_Type.tp_dealloc = pyopencv_KeyPoint_dealloc;
    pyopencv_KeyPoint_Type.tp_repr = pyopencv_KeyPoint_repr;
    pyopencv_KeyPoint_Type.tp_getset = pyopencv_KeyPoint_getseters;
    pyopencv_KeyPoint_Type.tp_init = (initproc)pyopencv_cv_KeyPoint_KeyPoint;
    pyopencv_KeyPoint_Type.tp_methods = pyopencv_KeyPoint_methods;
}

static PyObject* pyopencv_DMatch_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<DMatch %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_DMatch_get_distance(pyopencv_DMatch_t* p, void *closure)
{
    return pyopencv_from(p->v.distance);
}

static int pyopencv_DMatch_set_distance(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the distance attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.distance) ? 0 : -1;
}

static PyObject* pyopencv_DMatch_get_imgIdx(pyopencv_DMatch_t* p, void *closure)
{
    return pyopencv_from(p->v.imgIdx);
}

static int pyopencv_DMatch_set_imgIdx(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the imgIdx attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.imgIdx) ? 0 : -1;
}

static PyObject* pyopencv_DMatch_get_queryIdx(pyopencv_DMatch_t* p, void *closure)
{
    return pyopencv_from(p->v.queryIdx);
}

static int pyopencv_DMatch_set_queryIdx(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the queryIdx attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.queryIdx) ? 0 : -1;
}

static PyObject* pyopencv_DMatch_get_trainIdx(pyopencv_DMatch_t* p, void *closure)
{
    return pyopencv_from(p->v.trainIdx);
}

static int pyopencv_DMatch_set_trainIdx(pyopencv_DMatch_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the trainIdx attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.trainIdx) ? 0 : -1;
}


static PyGetSetDef pyopencv_DMatch_getseters[] =
{
    {(char*)"distance", (getter)pyopencv_DMatch_get_distance, (setter)pyopencv_DMatch_set_distance, (char*)"distance", NULL},
    {(char*)"imgIdx", (getter)pyopencv_DMatch_get_imgIdx, (setter)pyopencv_DMatch_set_imgIdx, (char*)"imgIdx", NULL},
    {(char*)"queryIdx", (getter)pyopencv_DMatch_get_queryIdx, (setter)pyopencv_DMatch_set_queryIdx, (char*)"queryIdx", NULL},
    {(char*)"trainIdx", (getter)pyopencv_DMatch_get_trainIdx, (setter)pyopencv_DMatch_set_trainIdx, (char*)"trainIdx", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_DMatch_DMatch(pyopencv_DMatch_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::DMatch());
        return 0;
    }
    }
    PyErr_Clear();

    {
    int _queryIdx=0;
    int _trainIdx=0;
    float _distance=0.f;

    const char* keywords[] = { "_queryIdx", "_trainIdx", "_distance", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iif:DMatch", (char**)keywords, &_queryIdx, &_trainIdx, &_distance) )
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::DMatch(_queryIdx, _trainIdx, _distance));
        return 0;
    }
    }
    PyErr_Clear();

    {
    int _queryIdx=0;
    int _trainIdx=0;
    int _imgIdx=0;
    float _distance=0.f;

    const char* keywords[] = { "_queryIdx", "_trainIdx", "_imgIdx", "_distance", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiif:DMatch", (char**)keywords, &_queryIdx, &_trainIdx, &_imgIdx, &_distance) )
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::DMatch(_queryIdx, _trainIdx, _imgIdx, _distance));
        return 0;
    }
    }

    return -1;
}



static PyMethodDef pyopencv_DMatch_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_DMatch_specials(void)
{
    pyopencv_DMatch_Type.tp_base = NULL;
    pyopencv_DMatch_Type.tp_dealloc = pyopencv_DMatch_dealloc;
    pyopencv_DMatch_Type.tp_repr = pyopencv_DMatch_repr;
    pyopencv_DMatch_Type.tp_getset = pyopencv_DMatch_getseters;
    pyopencv_DMatch_Type.tp_init = (initproc)pyopencv_cv_DMatch_DMatch;
    pyopencv_DMatch_Type.tp_methods = pyopencv_DMatch_methods;
}
static bool pyopencv_to(PyObject* src, cv::Moments& dst, const char* name)
{
    PyObject* tmp;
    bool ok;

    if( PyMapping_HasKeyString(src, (char*)"m00") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m00");
        ok = tmp && pyopencv_to(tmp, dst.m00);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m10") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m10");
        ok = tmp && pyopencv_to(tmp, dst.m10);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m01") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m01");
        ok = tmp && pyopencv_to(tmp, dst.m01);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m20") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m20");
        ok = tmp && pyopencv_to(tmp, dst.m20);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m11") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m11");
        ok = tmp && pyopencv_to(tmp, dst.m11);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m02") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m02");
        ok = tmp && pyopencv_to(tmp, dst.m02);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m30") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m30");
        ok = tmp && pyopencv_to(tmp, dst.m30);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m21") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m21");
        ok = tmp && pyopencv_to(tmp, dst.m21);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m12") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m12");
        ok = tmp && pyopencv_to(tmp, dst.m12);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"m03") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"m03");
        ok = tmp && pyopencv_to(tmp, dst.m03);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu20") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu20");
        ok = tmp && pyopencv_to(tmp, dst.mu20);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu11") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu11");
        ok = tmp && pyopencv_to(tmp, dst.mu11);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu02") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu02");
        ok = tmp && pyopencv_to(tmp, dst.mu02);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu30") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu30");
        ok = tmp && pyopencv_to(tmp, dst.mu30);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu21") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu21");
        ok = tmp && pyopencv_to(tmp, dst.mu21);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu12") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu12");
        ok = tmp && pyopencv_to(tmp, dst.mu12);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"mu03") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"mu03");
        ok = tmp && pyopencv_to(tmp, dst.mu03);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu20") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu20");
        ok = tmp && pyopencv_to(tmp, dst.nu20);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu11") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu11");
        ok = tmp && pyopencv_to(tmp, dst.nu11);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu02") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu02");
        ok = tmp && pyopencv_to(tmp, dst.nu02);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu30") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu30");
        ok = tmp && pyopencv_to(tmp, dst.nu30);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu21") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu21");
        ok = tmp && pyopencv_to(tmp, dst.nu21);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu12") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu12");
        ok = tmp && pyopencv_to(tmp, dst.nu12);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    if( PyMapping_HasKeyString(src, (char*)"nu03") )
    {
        tmp = PyMapping_GetItemString(src, (char*)"nu03");
        ok = tmp && pyopencv_to(tmp, dst.nu03);
        Py_DECREF(tmp);
        if(!ok) return false;
    }
    return true;
}

static PyObject* pyopencv_TickMeter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TickMeter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TickMeter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_TickMeter_TickMeter(pyopencv_TickMeter_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::TickMeter>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::TickMeter()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_TickMeter_getCounter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    int64 retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCounter());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_getTimeMicro(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTimeMicro());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_getTimeMilli(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTimeMilli());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_getTimeSec(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTimeSec());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_getTimeTicks(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");
    int64 retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTimeTicks());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_reset(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->reset());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_start(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->start());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TickMeter_stop(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TickMeter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TickMeter_Type))
        _self_ = ((pyopencv_TickMeter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TickMeter' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->stop());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_TickMeter_methods[] =
{
    {"getCounter", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_getCounter, 0), "getCounter() -> retval\n.   returns internal counter value."},
    {"getTimeMicro", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeMicro, 0), "getTimeMicro() -> retval\n.   returns passed time in microseconds."},
    {"getTimeMilli", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeMilli, 0), "getTimeMilli() -> retval\n.   returns passed time in milliseconds."},
    {"getTimeSec", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeSec, 0), "getTimeSec() -> retval\n.   returns passed time in seconds."},
    {"getTimeTicks", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_getTimeTicks, 0), "getTimeTicks() -> retval\n.   returns counted ticks."},
    {"reset", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_reset, 0), "reset() -> None\n.   resets internal values."},
    {"start", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_start, 0), "start() -> None\n.   starts counting ticks."},
    {"stop", CV_PY_FN_WITH_KW_(pyopencv_cv_TickMeter_stop, 0), "stop() -> None\n.   stops counting ticks."},

    {NULL,          NULL}
};

static void pyopencv_TickMeter_specials(void)
{
    pyopencv_TickMeter_Type.tp_base = NULL;
    pyopencv_TickMeter_Type.tp_dealloc = pyopencv_TickMeter_dealloc;
    pyopencv_TickMeter_Type.tp_repr = pyopencv_TickMeter_repr;
    pyopencv_TickMeter_Type.tp_getset = pyopencv_TickMeter_getseters;
    pyopencv_TickMeter_Type.tp_init = (initproc)pyopencv_cv_TickMeter_TickMeter;
    pyopencv_TickMeter_Type.tp_methods = pyopencv_TickMeter_methods;
}

static PyObject* pyopencv_flann_Index_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<flann_Index %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_flann_Index_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_flann_flann_Index_Index(pyopencv_flann_Index_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::flann::Index()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_features = NULL;
    Mat features;
    PyObject* pyobj_params = NULL;
    IndexParams params;
    PyObject* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:Index", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) &&
        pyopencv_to(pyobj_distType, distType, ArgInfo("distType", 0)) )
    {
        new (&(self->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::flann::Index(features, params, distType)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_features = NULL;
    UMat features;
    PyObject* pyobj_params = NULL;
    IndexParams params;
    PyObject* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:Index", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) &&
        pyopencv_to(pyobj_distType, distType, ArgInfo("distType", 0)) )
    {
        new (&(self->v)) Ptr<cv::flann::Index>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::flann::Index(features, params, distType)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_flann_flann_Index_build(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    {
    PyObject* pyobj_features = NULL;
    Mat features;
    PyObject* pyobj_params = NULL;
    IndexParams params;
    PyObject* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:flann_Index.build", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) &&
        pyopencv_to(pyobj_distType, distType, ArgInfo("distType", 0)) )
    {
        ERRWRAP2(_self_->build(features, params, distType));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_features = NULL;
    UMat features;
    PyObject* pyobj_params = NULL;
    IndexParams params;
    PyObject* pyobj_distType = NULL;
    cvflann_flann_distance_t distType=cvflann::FLANN_DIST_L2;

    const char* keywords[] = { "features", "params", "distType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:flann_Index.build", (char**)keywords, &pyobj_features, &pyobj_params, &pyobj_distType) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) &&
        pyopencv_to(pyobj_distType, distType, ArgInfo("distType", 0)) )
    {
        ERRWRAP2(_self_->build(features, params, distType));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_getAlgorithm(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    cvflann::flann_algorithm_t retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAlgorithm());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_getDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    cvflann::flann_distance_t retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDistance());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_knnSearch(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    {
    PyObject* pyobj_query = NULL;
    Mat query;
    PyObject* pyobj_indices = NULL;
    Mat indices;
    PyObject* pyobj_dists = NULL;
    Mat dists;
    int knn=0;
    PyObject* pyobj_params = NULL;
    SearchParams params;

    const char* keywords[] = { "query", "knn", "indices", "dists", "params", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|OOO:flann_Index.knnSearch", (char**)keywords, &pyobj_query, &knn, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        pyopencv_to(pyobj_query, query, ArgInfo("query", 0)) &&
        pyopencv_to(pyobj_indices, indices, ArgInfo("indices", 1)) &&
        pyopencv_to(pyobj_dists, dists, ArgInfo("dists", 1)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) )
    {
        ERRWRAP2(_self_->knnSearch(query, indices, dists, knn, params));
        return Py_BuildValue("(NN)", pyopencv_from(indices), pyopencv_from(dists));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_query = NULL;
    UMat query;
    PyObject* pyobj_indices = NULL;
    UMat indices;
    PyObject* pyobj_dists = NULL;
    UMat dists;
    int knn=0;
    PyObject* pyobj_params = NULL;
    SearchParams params;

    const char* keywords[] = { "query", "knn", "indices", "dists", "params", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|OOO:flann_Index.knnSearch", (char**)keywords, &pyobj_query, &knn, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        pyopencv_to(pyobj_query, query, ArgInfo("query", 0)) &&
        pyopencv_to(pyobj_indices, indices, ArgInfo("indices", 1)) &&
        pyopencv_to(pyobj_dists, dists, ArgInfo("dists", 1)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) )
    {
        ERRWRAP2(_self_->knnSearch(query, indices, dists, knn, params));
        return Py_BuildValue("(NN)", pyopencv_from(indices), pyopencv_from(dists));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_load(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    {
    PyObject* pyobj_features = NULL;
    Mat features;
    PyObject* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "features", "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:flann_Index.load", (char**)keywords, &pyobj_features, &pyobj_filename) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(retval = _self_->load(features, filename));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_features = NULL;
    UMat features;
    PyObject* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "features", "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:flann_Index.load", (char**)keywords, &pyobj_features, &pyobj_filename) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(retval = _self_->load(features, filename));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_radiusSearch(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    {
    PyObject* pyobj_query = NULL;
    Mat query;
    PyObject* pyobj_indices = NULL;
    Mat indices;
    PyObject* pyobj_dists = NULL;
    Mat dists;
    double radius=0;
    int maxResults=0;
    PyObject* pyobj_params = NULL;
    SearchParams params;
    int retval;

    const char* keywords[] = { "query", "radius", "maxResults", "indices", "dists", "params", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Odi|OOO:flann_Index.radiusSearch", (char**)keywords, &pyobj_query, &radius, &maxResults, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        pyopencv_to(pyobj_query, query, ArgInfo("query", 0)) &&
        pyopencv_to(pyobj_indices, indices, ArgInfo("indices", 1)) &&
        pyopencv_to(pyobj_dists, dists, ArgInfo("dists", 1)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) )
    {
        ERRWRAP2(retval = _self_->radiusSearch(query, indices, dists, radius, maxResults, params));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(indices), pyopencv_from(dists));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_query = NULL;
    UMat query;
    PyObject* pyobj_indices = NULL;
    UMat indices;
    PyObject* pyobj_dists = NULL;
    UMat dists;
    double radius=0;
    int maxResults=0;
    PyObject* pyobj_params = NULL;
    SearchParams params;
    int retval;

    const char* keywords[] = { "query", "radius", "maxResults", "indices", "dists", "params", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Odi|OOO:flann_Index.radiusSearch", (char**)keywords, &pyobj_query, &radius, &maxResults, &pyobj_indices, &pyobj_dists, &pyobj_params) &&
        pyopencv_to(pyobj_query, query, ArgInfo("query", 0)) &&
        pyopencv_to(pyobj_indices, indices, ArgInfo("indices", 1)) &&
        pyopencv_to(pyobj_dists, dists, ArgInfo("dists", 1)) &&
        pyopencv_to(pyobj_params, params, ArgInfo("params", 0)) )
    {
        ERRWRAP2(retval = _self_->radiusSearch(query, indices, dists, radius, maxResults, params));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(indices), pyopencv_from(dists));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_release(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_flann_flann_Index_save(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::flann;

    cv::flann::Index* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_flann_Index_Type))
        _self_ = ((pyopencv_flann_Index_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'flann_Index' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:flann_Index.save", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(_self_->save(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_flann_Index_methods[] =
{
    {"build", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_build, 0), "build(features, params[, distType]) -> None\n."},
    {"getAlgorithm", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_getAlgorithm, 0), "getAlgorithm() -> retval\n."},
    {"getDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_getDistance, 0), "getDistance() -> retval\n."},
    {"knnSearch", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_knnSearch, 0), "knnSearch(query, knn[, indices[, dists[, params]]]) -> indices, dists\n."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_load, 0), "load(features, filename) -> retval\n."},
    {"radiusSearch", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_radiusSearch, 0), "radiusSearch(query, radius, maxResults[, indices[, dists[, params]]]) -> retval, indices, dists\n."},
    {"release", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_release, 0), "release() -> None\n."},
    {"save", CV_PY_FN_WITH_KW_(pyopencv_cv_flann_flann_Index_save, 0), "save(filename) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_flann_Index_specials(void)
{
    pyopencv_flann_Index_Type.tp_base = NULL;
    pyopencv_flann_Index_Type.tp_dealloc = pyopencv_flann_Index_dealloc;
    pyopencv_flann_Index_Type.tp_repr = pyopencv_flann_Index_repr;
    pyopencv_flann_Index_Type.tp_getset = pyopencv_flann_Index_getseters;
    pyopencv_flann_Index_Type.tp_init = (initproc)pyopencv_cv_flann_flann_Index_Index;
    pyopencv_flann_Index_Type.tp_methods = pyopencv_flann_Index_methods;
}

static PyObject* pyopencv_CLAHE_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CLAHE %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_CLAHE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_CLAHE_apply(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CLAHE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CLAHE_Type))
        _self_ = dynamic_cast<cv::CLAHE*>(((pyopencv_CLAHE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:CLAHE.apply", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->apply(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:CLAHE.apply", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->apply(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_CLAHE_collectGarbage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CLAHE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CLAHE_Type))
        _self_ = dynamic_cast<cv::CLAHE*>(((pyopencv_CLAHE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->collectGarbage());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_CLAHE_getClipLimit(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CLAHE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CLAHE_Type))
        _self_ = dynamic_cast<cv::CLAHE*>(((pyopencv_CLAHE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getClipLimit());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CLAHE_getTilesGridSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CLAHE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CLAHE_Type))
        _self_ = dynamic_cast<cv::CLAHE*>(((pyopencv_CLAHE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTilesGridSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CLAHE_setClipLimit(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CLAHE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CLAHE_Type))
        _self_ = dynamic_cast<cv::CLAHE*>(((pyopencv_CLAHE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    double clipLimit=0;

    const char* keywords[] = { "clipLimit", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:CLAHE.setClipLimit", (char**)keywords, &clipLimit) )
    {
        ERRWRAP2(_self_->setClipLimit(clipLimit));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_CLAHE_setTilesGridSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CLAHE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CLAHE_Type))
        _self_ = dynamic_cast<cv::CLAHE*>(((pyopencv_CLAHE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CLAHE' or its derivative)");
    PyObject* pyobj_tileGridSize = NULL;
    Size tileGridSize;

    const char* keywords[] = { "tileGridSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:CLAHE.setTilesGridSize", (char**)keywords, &pyobj_tileGridSize) &&
        pyopencv_to(pyobj_tileGridSize, tileGridSize, ArgInfo("tileGridSize", 0)) )
    {
        ERRWRAP2(_self_->setTilesGridSize(tileGridSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_CLAHE_methods[] =
{
    {"apply", CV_PY_FN_WITH_KW_(pyopencv_cv_CLAHE_apply, 0), "apply(src[, dst]) -> dst\n.   @brief Equalizes the histogram of a grayscale image using Contrast Limited Adaptive Histogram Equalization.\n.   \n.   @param src Source image with CV_8UC1 type.\n.   @param dst Destination image."},
    {"collectGarbage", CV_PY_FN_WITH_KW_(pyopencv_cv_CLAHE_collectGarbage, 0), "collectGarbage() -> None\n."},
    {"getClipLimit", CV_PY_FN_WITH_KW_(pyopencv_cv_CLAHE_getClipLimit, 0), "getClipLimit() -> retval\n."},
    {"getTilesGridSize", CV_PY_FN_WITH_KW_(pyopencv_cv_CLAHE_getTilesGridSize, 0), "getTilesGridSize() -> retval\n."},
    {"setClipLimit", CV_PY_FN_WITH_KW_(pyopencv_cv_CLAHE_setClipLimit, 0), "setClipLimit(clipLimit) -> None\n.   @brief Sets threshold for contrast limiting.\n.   \n.   @param clipLimit threshold value."},
    {"setTilesGridSize", CV_PY_FN_WITH_KW_(pyopencv_cv_CLAHE_setTilesGridSize, 0), "setTilesGridSize(tileGridSize) -> None\n.   @brief Sets size of grid for histogram equalization. Input image will be divided into\n.   equally sized rectangular tiles.\n.   \n.   @param tileGridSize defines the number of tiles in row and column."},

    {NULL,          NULL}
};

static void pyopencv_CLAHE_specials(void)
{
    pyopencv_CLAHE_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_CLAHE_Type.tp_dealloc = pyopencv_CLAHE_dealloc;
    pyopencv_CLAHE_Type.tp_repr = pyopencv_CLAHE_repr;
    pyopencv_CLAHE_Type.tp_getset = pyopencv_CLAHE_getseters;
    pyopencv_CLAHE_Type.tp_init = (initproc)0;
    pyopencv_CLAHE_Type.tp_methods = pyopencv_CLAHE_methods;
}

static PyObject* pyopencv_Subdiv2D_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<Subdiv2D %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_Subdiv2D_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_Subdiv2D_Subdiv2D(pyopencv_Subdiv2D_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::Subdiv2D>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::Subdiv2D()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_rect = NULL;
    Rect rect;

    const char* keywords[] = { "rect", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D", (char**)keywords, &pyobj_rect) &&
        pyopencv_to(pyobj_rect, rect, ArgInfo("rect", 0)) )
    {
        new (&(self->v)) Ptr<cv::Subdiv2D>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::Subdiv2D(rect)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_Subdiv2D_edgeDst(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int edge=0;
    Point2f dstpt;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:Subdiv2D.edgeDst", (char**)keywords, &edge) )
    {
        ERRWRAP2(retval = _self_->edgeDst(edge, &dstpt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(dstpt));
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_edgeOrg(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int edge=0;
    Point2f orgpt;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:Subdiv2D.edgeOrg", (char**)keywords, &edge) )
    {
        ERRWRAP2(retval = _self_->edgeOrg(edge, &orgpt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(orgpt));
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_findNearest(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    PyObject* pyobj_pt = NULL;
    Point2f pt;
    Point2f nearestPt;
    int retval;

    const char* keywords[] = { "pt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D.findNearest", (char**)keywords, &pyobj_pt) &&
        pyopencv_to(pyobj_pt, pt, ArgInfo("pt", 0)) )
    {
        ERRWRAP2(retval = _self_->findNearest(pt, &nearestPt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(nearestPt));
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_getEdge(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int edge=0;
    int nextEdgeType=0;
    int retval;

    const char* keywords[] = { "edge", "nextEdgeType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:Subdiv2D.getEdge", (char**)keywords, &edge, &nextEdgeType) )
    {
        ERRWRAP2(retval = _self_->getEdge(edge, nextEdgeType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_getEdgeList(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    vector_Vec4f edgeList;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->getEdgeList(edgeList));
        return pyopencv_from(edgeList);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_getLeadingEdgeList(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    vector_int leadingEdgeList;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->getLeadingEdgeList(leadingEdgeList));
        return pyopencv_from(leadingEdgeList);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_getTriangleList(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    vector_Vec6f triangleList;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->getTriangleList(triangleList));
        return pyopencv_from(triangleList);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_getVertex(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int vertex=0;
    int firstEdge;
    Point2f retval;

    const char* keywords[] = { "vertex", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:Subdiv2D.getVertex", (char**)keywords, &vertex) )
    {
        ERRWRAP2(retval = _self_->getVertex(vertex, &firstEdge));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(firstEdge));
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_getVoronoiFacetList(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    PyObject* pyobj_idx = NULL;
    vector_int idx;
    vector_vector_Point2f facetList;
    vector_Point2f facetCenters;

    const char* keywords[] = { "idx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D.getVoronoiFacetList", (char**)keywords, &pyobj_idx) &&
        pyopencv_to(pyobj_idx, idx, ArgInfo("idx", 0)) )
    {
        ERRWRAP2(_self_->getVoronoiFacetList(idx, facetList, facetCenters));
        return Py_BuildValue("(NN)", pyopencv_from(facetList), pyopencv_from(facetCenters));
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_initDelaunay(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    PyObject* pyobj_rect = NULL;
    Rect rect;

    const char* keywords[] = { "rect", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D.initDelaunay", (char**)keywords, &pyobj_rect) &&
        pyopencv_to(pyobj_rect, rect, ArgInfo("rect", 0)) )
    {
        ERRWRAP2(_self_->initDelaunay(rect));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_insert(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    {
    PyObject* pyobj_pt = NULL;
    Point2f pt;
    int retval;

    const char* keywords[] = { "pt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D.insert", (char**)keywords, &pyobj_pt) &&
        pyopencv_to(pyobj_pt, pt, ArgInfo("pt", 0)) )
    {
        ERRWRAP2(retval = _self_->insert(pt));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_ptvec = NULL;
    vector_Point2f ptvec;

    const char* keywords[] = { "ptvec", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D.insert", (char**)keywords, &pyobj_ptvec) &&
        pyopencv_to(pyobj_ptvec, ptvec, ArgInfo("ptvec", 0)) )
    {
        ERRWRAP2(_self_->insert(ptvec));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_locate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    PyObject* pyobj_pt = NULL;
    Point2f pt;
    int edge;
    int vertex;
    int retval;

    const char* keywords[] = { "pt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Subdiv2D.locate", (char**)keywords, &pyobj_pt) &&
        pyopencv_to(pyobj_pt, pt, ArgInfo("pt", 0)) )
    {
        ERRWRAP2(retval = _self_->locate(pt, edge, vertex));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(edge), pyopencv_from(vertex));
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_nextEdge(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int edge=0;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:Subdiv2D.nextEdge", (char**)keywords, &edge) )
    {
        ERRWRAP2(retval = _self_->nextEdge(edge));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_rotateEdge(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int edge=0;
    int rotate=0;
    int retval;

    const char* keywords[] = { "edge", "rotate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:Subdiv2D.rotateEdge", (char**)keywords, &edge, &rotate) )
    {
        ERRWRAP2(retval = _self_->rotateEdge(edge, rotate));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Subdiv2D_symEdge(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Subdiv2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Subdiv2D_Type))
        _self_ = ((pyopencv_Subdiv2D_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Subdiv2D' or its derivative)");
    int edge=0;
    int retval;

    const char* keywords[] = { "edge", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:Subdiv2D.symEdge", (char**)keywords, &edge) )
    {
        ERRWRAP2(retval = _self_->symEdge(edge));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_Subdiv2D_methods[] =
{
    {"edgeDst", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_edgeDst, 0), "edgeDst(edge) -> retval, dstpt\n.   @brief Returns the edge destination.\n.   \n.   @param edge Subdivision edge ID.\n.   @param dstpt Output vertex location.\n.   \n.   @returns vertex ID."},
    {"edgeOrg", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_edgeOrg, 0), "edgeOrg(edge) -> retval, orgpt\n.   @brief Returns the edge origin.\n.   \n.   @param edge Subdivision edge ID.\n.   @param orgpt Output vertex location.\n.   \n.   @returns vertex ID."},
    {"findNearest", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_findNearest, 0), "findNearest(pt) -> retval, nearestPt\n.   @brief Finds the subdivision vertex closest to the given point.\n.   \n.   @param pt Input point.\n.   @param nearestPt Output subdivision vertex point.\n.   \n.   The function is another function that locates the input point within the subdivision. It finds the\n.   subdivision vertex that is the closest to the input point. It is not necessarily one of vertices\n.   of the facet containing the input point, though the facet (located using locate() ) is used as a\n.   starting point.\n.   \n.   @returns vertex ID."},
    {"getEdge", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getEdge, 0), "getEdge(edge, nextEdgeType) -> retval\n.   @brief Returns one of the edges related to the given edge.\n.   \n.   @param edge Subdivision edge ID.\n.   @param nextEdgeType Parameter specifying which of the related edges to return.\n.   The following values are possible:\n.   -   NEXT_AROUND_ORG next around the edge origin ( eOnext on the picture below if e is the input edge)\n.   -   NEXT_AROUND_DST next around the edge vertex ( eDnext )\n.   -   PREV_AROUND_ORG previous around the edge origin (reversed eRnext )\n.   -   PREV_AROUND_DST previous around the edge destination (reversed eLnext )\n.   -   NEXT_AROUND_LEFT next around the left facet ( eLnext )\n.   -   NEXT_AROUND_RIGHT next around the right facet ( eRnext )\n.   -   PREV_AROUND_LEFT previous around the left facet (reversed eOnext )\n.   -   PREV_AROUND_RIGHT previous around the right facet (reversed eDnext )\n.   \n.   ![sample output](pics/quadedge.png)\n.   \n.   @returns edge ID related to the input edge."},
    {"getEdgeList", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getEdgeList, 0), "getEdgeList() -> edgeList\n.   @brief Returns a list of all edges.\n.   \n.   @param edgeList Output vector.\n.   \n.   The function gives each edge as a 4 numbers vector, where each two are one of the edge\n.   vertices. i.e. org_x = v[0], org_y = v[1], dst_x = v[2], dst_y = v[3]."},
    {"getLeadingEdgeList", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getLeadingEdgeList, 0), "getLeadingEdgeList() -> leadingEdgeList\n.   @brief Returns a list of the leading edge ID connected to each triangle.\n.   \n.   @param leadingEdgeList Output vector.\n.   \n.   The function gives one edge ID for each triangle."},
    {"getTriangleList", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getTriangleList, 0), "getTriangleList() -> triangleList\n.   @brief Returns a list of all triangles.\n.   \n.   @param triangleList Output vector.\n.   \n.   The function gives each triangle as a 6 numbers vector, where each two are one of the triangle\n.   vertices. i.e. p1_x = v[0], p1_y = v[1], p2_x = v[2], p2_y = v[3], p3_x = v[4], p3_y = v[5]."},
    {"getVertex", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getVertex, 0), "getVertex(vertex) -> retval, firstEdge\n.   @brief Returns vertex location from vertex ID.\n.   \n.   @param vertex vertex ID.\n.   @param firstEdge Optional. The first edge ID which is connected to the vertex.\n.   @returns vertex (x,y)"},
    {"getVoronoiFacetList", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_getVoronoiFacetList, 0), "getVoronoiFacetList(idx) -> facetList, facetCenters\n.   @brief Returns a list of all Voroni facets.\n.   \n.   @param idx Vector of vertices IDs to consider. For all vertices you can pass empty vector.\n.   @param facetList Output vector of the Voroni facets.\n.   @param facetCenters Output vector of the Voroni facets center points."},
    {"initDelaunay", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_initDelaunay, 0), "initDelaunay(rect) -> None\n.   @brief Creates a new empty Delaunay subdivision\n.   \n.   @param rect Rectangle that includes all of the 2D points that are to be added to the subdivision."},
    {"insert", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_insert, 0), "insert(pt) -> retval\n.   @brief Insert a single point into a Delaunay triangulation.\n.   \n.   @param pt Point to insert.\n.   \n.   The function inserts a single point into a subdivision and modifies the subdivision topology\n.   appropriately. If a point with the same coordinates exists already, no new point is added.\n.   @returns the ID of the point.\n.   \n.   @note If the point is outside of the triangulation specified rect a runtime error is raised.\n\n\n\ninsert(ptvec) -> None\n.   @brief Insert multiple points into a Delaunay triangulation.\n.   \n.   @param ptvec Points to insert.\n.   \n.   The function inserts a vector of points into a subdivision and modifies the subdivision topology\n.   appropriately."},
    {"locate", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_locate, 0), "locate(pt) -> retval, edge, vertex\n.   @brief Returns the location of a point within a Delaunay triangulation.\n.   \n.   @param pt Point to locate.\n.   @param edge Output edge that the point belongs to or is located to the right of it.\n.   @param vertex Optional output vertex the input point coincides with.\n.   \n.   The function locates the input point within the subdivision and gives one of the triangle edges\n.   or vertices.\n.   \n.   @returns an integer which specify one of the following five cases for point location:\n.   -  The point falls into some facet. The function returns #PTLOC_INSIDE and edge will contain one of\n.   edges of the facet.\n.   -  The point falls onto the edge. The function returns #PTLOC_ON_EDGE and edge will contain this edge.\n.   -  The point coincides with one of the subdivision vertices. The function returns #PTLOC_VERTEX and\n.   vertex will contain a pointer to the vertex.\n.   -  The point is outside the subdivision reference rectangle. The function returns #PTLOC_OUTSIDE_RECT\n.   and no pointers are filled.\n.   -  One of input arguments is invalid. A runtime error is raised or, if silent or \"parent\" error\n.   processing mode is selected, #PTLOC_ERROR is returned."},
    {"nextEdge", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_nextEdge, 0), "nextEdge(edge) -> retval\n.   @brief Returns next edge around the edge origin.\n.   \n.   @param edge Subdivision edge ID.\n.   \n.   @returns an integer which is next edge ID around the edge origin: eOnext on the\n.   picture above if e is the input edge)."},
    {"rotateEdge", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_rotateEdge, 0), "rotateEdge(edge, rotate) -> retval\n.   @brief Returns another edge of the same quad-edge.\n.   \n.   @param edge Subdivision edge ID.\n.   @param rotate Parameter specifying which of the edges of the same quad-edge as the input\n.   one to return. The following values are possible:\n.   -   0 - the input edge ( e on the picture below if e is the input edge)\n.   -   1 - the rotated edge ( eRot )\n.   -   2 - the reversed edge (reversed e (in green))\n.   -   3 - the reversed rotated edge (reversed eRot (in green))\n.   \n.   @returns one of the edges ID of the same quad-edge as the input edge."},
    {"symEdge", CV_PY_FN_WITH_KW_(pyopencv_cv_Subdiv2D_symEdge, 0), "symEdge(edge) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_Subdiv2D_specials(void)
{
    pyopencv_Subdiv2D_Type.tp_base = NULL;
    pyopencv_Subdiv2D_Type.tp_dealloc = pyopencv_Subdiv2D_dealloc;
    pyopencv_Subdiv2D_Type.tp_repr = pyopencv_Subdiv2D_repr;
    pyopencv_Subdiv2D_Type.tp_getset = pyopencv_Subdiv2D_getseters;
    pyopencv_Subdiv2D_Type.tp_init = (initproc)pyopencv_cv_Subdiv2D_Subdiv2D;
    pyopencv_Subdiv2D_Type.tp_methods = pyopencv_Subdiv2D_methods;
}

static PyObject* pyopencv_LineSegmentDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<LineSegmentDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_LineSegmentDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_LineSegmentDetector_compareSegments(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::LineSegmentDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_LineSegmentDetector_Type))
        _self_ = dynamic_cast<cv::LineSegmentDetector*>(((pyopencv_LineSegmentDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'LineSegmentDetector' or its derivative)");
    {
    PyObject* pyobj_size = NULL;
    Size size;
    PyObject* pyobj_lines1 = NULL;
    Mat lines1;
    PyObject* pyobj_lines2 = NULL;
    Mat lines2;
    PyObject* pyobj__image = NULL;
    Mat _image;
    int retval;

    const char* keywords[] = { "size", "lines1", "lines2", "_image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:LineSegmentDetector.compareSegments", (char**)keywords, &pyobj_size, &pyobj_lines1, &pyobj_lines2, &pyobj__image) &&
        pyopencv_to(pyobj_size, size, ArgInfo("size", 0)) &&
        pyopencv_to(pyobj_lines1, lines1, ArgInfo("lines1", 0)) &&
        pyopencv_to(pyobj_lines2, lines2, ArgInfo("lines2", 0)) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 1)) )
    {
        ERRWRAP2(retval = _self_->compareSegments(size, lines1, lines2, _image));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(_image));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_size = NULL;
    Size size;
    PyObject* pyobj_lines1 = NULL;
    UMat lines1;
    PyObject* pyobj_lines2 = NULL;
    UMat lines2;
    PyObject* pyobj__image = NULL;
    UMat _image;
    int retval;

    const char* keywords[] = { "size", "lines1", "lines2", "_image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:LineSegmentDetector.compareSegments", (char**)keywords, &pyobj_size, &pyobj_lines1, &pyobj_lines2, &pyobj__image) &&
        pyopencv_to(pyobj_size, size, ArgInfo("size", 0)) &&
        pyopencv_to(pyobj_lines1, lines1, ArgInfo("lines1", 0)) &&
        pyopencv_to(pyobj_lines2, lines2, ArgInfo("lines2", 0)) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 1)) )
    {
        ERRWRAP2(retval = _self_->compareSegments(size, lines1, lines2, _image));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(_image));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_LineSegmentDetector_detect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::LineSegmentDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_LineSegmentDetector_Type))
        _self_ = dynamic_cast<cv::LineSegmentDetector*>(((pyopencv_LineSegmentDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'LineSegmentDetector' or its derivative)");
    {
    PyObject* pyobj__image = NULL;
    Mat _image;
    PyObject* pyobj__lines = NULL;
    Mat _lines;
    PyObject* pyobj_width = NULL;
    Mat width;
    PyObject* pyobj_prec = NULL;
    Mat prec;
    PyObject* pyobj_nfa = NULL;
    Mat nfa;

    const char* keywords[] = { "_image", "_lines", "width", "prec", "nfa", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOOO:LineSegmentDetector.detect", (char**)keywords, &pyobj__image, &pyobj__lines, &pyobj_width, &pyobj_prec, &pyobj_nfa) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 0)) &&
        pyopencv_to(pyobj__lines, _lines, ArgInfo("_lines", 1)) &&
        pyopencv_to(pyobj_width, width, ArgInfo("width", 1)) &&
        pyopencv_to(pyobj_prec, prec, ArgInfo("prec", 1)) &&
        pyopencv_to(pyobj_nfa, nfa, ArgInfo("nfa", 1)) )
    {
        ERRWRAP2(_self_->detect(_image, _lines, width, prec, nfa));
        return Py_BuildValue("(NNNN)", pyopencv_from(_lines), pyopencv_from(width), pyopencv_from(prec), pyopencv_from(nfa));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__image = NULL;
    UMat _image;
    PyObject* pyobj__lines = NULL;
    UMat _lines;
    PyObject* pyobj_width = NULL;
    UMat width;
    PyObject* pyobj_prec = NULL;
    UMat prec;
    PyObject* pyobj_nfa = NULL;
    UMat nfa;

    const char* keywords[] = { "_image", "_lines", "width", "prec", "nfa", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOOO:LineSegmentDetector.detect", (char**)keywords, &pyobj__image, &pyobj__lines, &pyobj_width, &pyobj_prec, &pyobj_nfa) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 0)) &&
        pyopencv_to(pyobj__lines, _lines, ArgInfo("_lines", 1)) &&
        pyopencv_to(pyobj_width, width, ArgInfo("width", 1)) &&
        pyopencv_to(pyobj_prec, prec, ArgInfo("prec", 1)) &&
        pyopencv_to(pyobj_nfa, nfa, ArgInfo("nfa", 1)) )
    {
        ERRWRAP2(_self_->detect(_image, _lines, width, prec, nfa));
        return Py_BuildValue("(NNNN)", pyopencv_from(_lines), pyopencv_from(width), pyopencv_from(prec), pyopencv_from(nfa));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_LineSegmentDetector_drawSegments(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::LineSegmentDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_LineSegmentDetector_Type))
        _self_ = dynamic_cast<cv::LineSegmentDetector*>(((pyopencv_LineSegmentDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'LineSegmentDetector' or its derivative)");
    {
    PyObject* pyobj__image = NULL;
    Mat _image;
    PyObject* pyobj_lines = NULL;
    Mat lines;

    const char* keywords[] = { "_image", "lines", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:LineSegmentDetector.drawSegments", (char**)keywords, &pyobj__image, &pyobj_lines) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 1)) &&
        pyopencv_to(pyobj_lines, lines, ArgInfo("lines", 0)) )
    {
        ERRWRAP2(_self_->drawSegments(_image, lines));
        return pyopencv_from(_image);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__image = NULL;
    UMat _image;
    PyObject* pyobj_lines = NULL;
    UMat lines;

    const char* keywords[] = { "_image", "lines", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:LineSegmentDetector.drawSegments", (char**)keywords, &pyobj__image, &pyobj_lines) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 1)) &&
        pyopencv_to(pyobj_lines, lines, ArgInfo("lines", 0)) )
    {
        ERRWRAP2(_self_->drawSegments(_image, lines));
        return pyopencv_from(_image);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_LineSegmentDetector_methods[] =
{
    {"compareSegments", CV_PY_FN_WITH_KW_(pyopencv_cv_LineSegmentDetector_compareSegments, 0), "compareSegments(size, lines1, lines2[, _image]) -> retval, _image\n.   @brief Draws two groups of lines in blue and red, counting the non overlapping (mismatching) pixels.\n.   \n.   @param size The size of the image, where lines1 and lines2 were found.\n.   @param lines1 The first group of lines that needs to be drawn. It is visualized in blue color.\n.   @param lines2 The second group of lines. They visualized in red color.\n.   @param _image Optional image, where the lines will be drawn. The image should be color(3-channel)\n.   in order for lines1 and lines2 to be drawn in the above mentioned colors."},
    {"detect", CV_PY_FN_WITH_KW_(pyopencv_cv_LineSegmentDetector_detect, 0), "detect(_image[, _lines[, width[, prec[, nfa]]]]) -> _lines, width, prec, nfa\n.   @brief Finds lines in the input image.\n.   \n.   This is the output of the default parameters of the algorithm on the above shown image.\n.   \n.   ![image](pics/building_lsd.png)\n.   \n.   @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:\n.   `lsd_ptr-\\>detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);`\n.   @param _lines A vector of Vec4i or Vec4f elements specifying the beginning and ending point of a line. Where\n.   Vec4i/Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly\n.   oriented depending on the gradient.\n.   @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.\n.   @param prec Vector of precisions with which the lines are found.\n.   @param nfa Vector containing number of false alarms in the line region, with precision of 10%. The\n.   bigger the value, logarithmically better the detection.\n.   - -1 corresponds to 10 mean false alarms\n.   - 0 corresponds to 1 mean false alarm\n.   - 1 corresponds to 0.1 mean false alarms\n.   This vector will be calculated only when the objects type is #LSD_REFINE_ADV."},
    {"drawSegments", CV_PY_FN_WITH_KW_(pyopencv_cv_LineSegmentDetector_drawSegments, 0), "drawSegments(_image, lines) -> _image\n.   @brief Draws the line segments on a given image.\n.   @param _image The image, where the lines will be drawn. Should be bigger or equal to the image,\n.   where the lines were found.\n.   @param lines A vector of the lines that needed to be drawn."},

    {NULL,          NULL}
};

static void pyopencv_LineSegmentDetector_specials(void)
{
    pyopencv_LineSegmentDetector_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_LineSegmentDetector_Type.tp_dealloc = pyopencv_LineSegmentDetector_dealloc;
    pyopencv_LineSegmentDetector_Type.tp_repr = pyopencv_LineSegmentDetector_repr;
    pyopencv_LineSegmentDetector_Type.tp_getset = pyopencv_LineSegmentDetector_getseters;
    pyopencv_LineSegmentDetector_Type.tp_init = (initproc)0;
    pyopencv_LineSegmentDetector_Type.tp_methods = pyopencv_LineSegmentDetector_methods;
}

static PyObject* pyopencv_ml_ParamGrid_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_ParamGrid %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_ml_ParamGrid_get_logStep(pyopencv_ml_ParamGrid_t* p, void *closure)
{
    return pyopencv_from(p->v->logStep);
}

static int pyopencv_ml_ParamGrid_set_logStep(pyopencv_ml_ParamGrid_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the logStep attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->logStep) ? 0 : -1;
}

static PyObject* pyopencv_ml_ParamGrid_get_maxVal(pyopencv_ml_ParamGrid_t* p, void *closure)
{
    return pyopencv_from(p->v->maxVal);
}

static int pyopencv_ml_ParamGrid_set_maxVal(pyopencv_ml_ParamGrid_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxVal attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->maxVal) ? 0 : -1;
}

static PyObject* pyopencv_ml_ParamGrid_get_minVal(pyopencv_ml_ParamGrid_t* p, void *closure)
{
    return pyopencv_from(p->v->minVal);
}

static int pyopencv_ml_ParamGrid_set_minVal(pyopencv_ml_ParamGrid_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minVal attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->minVal) ? 0 : -1;
}


static PyGetSetDef pyopencv_ml_ParamGrid_getseters[] =
{
    {(char*)"logStep", (getter)pyopencv_ml_ParamGrid_get_logStep, (setter)pyopencv_ml_ParamGrid_set_logStep, (char*)"logStep", NULL},
    {(char*)"maxVal", (getter)pyopencv_ml_ParamGrid_get_maxVal, (setter)pyopencv_ml_ParamGrid_set_maxVal, (char*)"maxVal", NULL},
    {(char*)"minVal", (getter)pyopencv_ml_ParamGrid_get_minVal, (setter)pyopencv_ml_ParamGrid_set_minVal, (char*)"minVal", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_ParamGrid_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    double minVal=0.;
    double maxVal=0.;
    double logstep=1.;
    Ptr<ParamGrid> retval;

    const char* keywords[] = { "minVal", "maxVal", "logstep", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ddd:ml_ParamGrid.create", (char**)keywords, &minVal, &maxVal, &logstep) )
    {
        ERRWRAP2(retval = cv::ml::ParamGrid::create(minVal, maxVal, logstep));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_ParamGrid_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ParamGrid_create_cls, METH_CLASS), "create([, minVal[, maxVal[, logstep]]]) -> retval\n.   @brief Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method\n.   \n.   @param minVal minimum value of the parameter grid\n.   @param maxVal maximum value of the parameter grid\n.   @param logstep Logarithmic step for iterating the statmodel parameter"},

    {NULL,          NULL}
};

static void pyopencv_ml_ParamGrid_specials(void)
{
    pyopencv_ml_ParamGrid_Type.tp_base = NULL;
    pyopencv_ml_ParamGrid_Type.tp_dealloc = pyopencv_ml_ParamGrid_dealloc;
    pyopencv_ml_ParamGrid_Type.tp_repr = pyopencv_ml_ParamGrid_repr;
    pyopencv_ml_ParamGrid_Type.tp_getset = pyopencv_ml_ParamGrid_getseters;
    pyopencv_ml_ParamGrid_Type.tp_init = (initproc)0;
    pyopencv_ml_ParamGrid_Type.tp_methods = pyopencv_ml_ParamGrid_methods;
}

static PyObject* pyopencv_ml_TrainData_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_TrainData %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_TrainData_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_TrainData_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    int layout=0;
    PyObject* pyobj_responses = NULL;
    Mat responses;
    PyObject* pyobj_varIdx = NULL;
    Mat varIdx;
    PyObject* pyobj_sampleIdx = NULL;
    Mat sampleIdx;
    PyObject* pyobj_sampleWeights = NULL;
    Mat sampleWeights;
    PyObject* pyobj_varType = NULL;
    Mat varType;
    Ptr<TrainData> retval;

    const char* keywords[] = { "samples", "layout", "responses", "varIdx", "sampleIdx", "sampleWeights", "varType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO|OOOO:ml_TrainData.create", (char**)keywords, &pyobj_samples, &layout, &pyobj_responses, &pyobj_varIdx, &pyobj_sampleIdx, &pyobj_sampleWeights, &pyobj_varType) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_responses, responses, ArgInfo("responses", 0)) &&
        pyopencv_to(pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) &&
        pyopencv_to(pyobj_sampleIdx, sampleIdx, ArgInfo("sampleIdx", 0)) &&
        pyopencv_to(pyobj_sampleWeights, sampleWeights, ArgInfo("sampleWeights", 0)) &&
        pyopencv_to(pyobj_varType, varType, ArgInfo("varType", 0)) )
    {
        ERRWRAP2(retval = cv::ml::TrainData::create(samples, layout, responses, varIdx, sampleIdx, sampleWeights, varType));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    int layout=0;
    PyObject* pyobj_responses = NULL;
    UMat responses;
    PyObject* pyobj_varIdx = NULL;
    UMat varIdx;
    PyObject* pyobj_sampleIdx = NULL;
    UMat sampleIdx;
    PyObject* pyobj_sampleWeights = NULL;
    UMat sampleWeights;
    PyObject* pyobj_varType = NULL;
    UMat varType;
    Ptr<TrainData> retval;

    const char* keywords[] = { "samples", "layout", "responses", "varIdx", "sampleIdx", "sampleWeights", "varType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO|OOOO:ml_TrainData.create", (char**)keywords, &pyobj_samples, &layout, &pyobj_responses, &pyobj_varIdx, &pyobj_sampleIdx, &pyobj_sampleWeights, &pyobj_varType) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_responses, responses, ArgInfo("responses", 0)) &&
        pyopencv_to(pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) &&
        pyopencv_to(pyobj_sampleIdx, sampleIdx, ArgInfo("sampleIdx", 0)) &&
        pyopencv_to(pyobj_sampleWeights, sampleWeights, ArgInfo("sampleWeights", 0)) &&
        pyopencv_to(pyobj_varType, varType, ArgInfo("varType", 0)) )
    {
        ERRWRAP2(retval = cv::ml::TrainData::create(samples, layout, responses, varIdx, sampleIdx, sampleWeights, varType));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getCatCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int vi=0;
    int retval;

    const char* keywords[] = { "vi", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_TrainData.getCatCount", (char**)keywords, &vi) )
    {
        ERRWRAP2(retval = _self_->getCatCount(vi));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getCatMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCatMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getCatOfs(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCatOfs());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getClassLabels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getClassLabels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getDefaultSubstValues(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultSubstValues());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getLayout(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLayout());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getMissing(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMissing());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNAllVars(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNAllVars());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNTestSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNTestSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNTrainSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNTrainSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNVars(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNVars());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNames(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    PyObject* pyobj_names = NULL;
    vector_String names;

    const char* keywords[] = { "names", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_TrainData.getNames", (char**)keywords, &pyobj_names) &&
        pyopencv_to(pyobj_names, names, ArgInfo("names", 0)) )
    {
        ERRWRAP2(_self_->getNames(names));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getNormCatResponses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNormCatResponses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getResponseType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getResponseType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getResponses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getResponses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getSample(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    {
    PyObject* pyobj_varIdx = NULL;
    Mat varIdx;
    int sidx=0;
    float buf=0.f;

    const char* keywords[] = { "varIdx", "sidx", "buf", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oif:ml_TrainData.getSample", (char**)keywords, &pyobj_varIdx, &sidx, &buf) &&
        pyopencv_to(pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) )
    {
        ERRWRAP2(_self_->getSample(varIdx, sidx, &buf));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_varIdx = NULL;
    UMat varIdx;
    int sidx=0;
    float buf=0.f;

    const char* keywords[] = { "varIdx", "sidx", "buf", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oif:ml_TrainData.getSample", (char**)keywords, &pyobj_varIdx, &sidx, &buf) &&
        pyopencv_to(pyobj_varIdx, varIdx, ArgInfo("varIdx", 0)) )
    {
        ERRWRAP2(_self_->getSample(varIdx, sidx, &buf));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getSampleWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSampleWeights());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getSubMatrix_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    {
    PyObject* pyobj_matrix = NULL;
    Mat matrix;
    PyObject* pyobj_idx = NULL;
    Mat idx;
    int layout=0;
    Mat retval;

    const char* keywords[] = { "matrix", "idx", "layout", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOi:ml_TrainData.getSubMatrix", (char**)keywords, &pyobj_matrix, &pyobj_idx, &layout) &&
        pyopencv_to(pyobj_matrix, matrix, ArgInfo("matrix", 0)) &&
        pyopencv_to(pyobj_idx, idx, ArgInfo("idx", 0)) )
    {
        ERRWRAP2(retval = cv::ml::TrainData::getSubMatrix(matrix, idx, layout));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_matrix = NULL;
    Mat matrix;
    PyObject* pyobj_idx = NULL;
    Mat idx;
    int layout=0;
    Mat retval;

    const char* keywords[] = { "matrix", "idx", "layout", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOi:ml_TrainData.getSubMatrix", (char**)keywords, &pyobj_matrix, &pyobj_idx, &layout) &&
        pyopencv_to(pyobj_matrix, matrix, ArgInfo("matrix", 0)) &&
        pyopencv_to(pyobj_idx, idx, ArgInfo("idx", 0)) )
    {
        ERRWRAP2(retval = cv::ml::TrainData::getSubMatrix(matrix, idx, layout));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getSubVector_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    {
    PyObject* pyobj_vec = NULL;
    Mat vec;
    PyObject* pyobj_idx = NULL;
    Mat idx;
    Mat retval;

    const char* keywords[] = { "vec", "idx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ml_TrainData.getSubVector", (char**)keywords, &pyobj_vec, &pyobj_idx) &&
        pyopencv_to(pyobj_vec, vec, ArgInfo("vec", 0)) &&
        pyopencv_to(pyobj_idx, idx, ArgInfo("idx", 0)) )
    {
        ERRWRAP2(retval = cv::ml::TrainData::getSubVector(vec, idx));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_vec = NULL;
    Mat vec;
    PyObject* pyobj_idx = NULL;
    Mat idx;
    Mat retval;

    const char* keywords[] = { "vec", "idx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ml_TrainData.getSubVector", (char**)keywords, &pyobj_vec, &pyobj_idx) &&
        pyopencv_to(pyobj_vec, vec, ArgInfo("vec", 0)) &&
        pyopencv_to(pyobj_idx, idx, ArgInfo("idx", 0)) )
    {
        ERRWRAP2(retval = cv::ml::TrainData::getSubVector(vec, idx));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTestNormCatResponses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTestNormCatResponses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTestResponses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTestResponses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTestSampleIdx(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTestSampleIdx());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTestSampleWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTestSampleWeights());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTestSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTestSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTrainNormCatResponses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainNormCatResponses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTrainResponses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainResponses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTrainSampleIdx(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainSampleIdx());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTrainSampleWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainSampleWeights());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getTrainSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int layout=ROW_SAMPLE;
    bool compressSamples=true;
    bool compressVars=true;
    Mat retval;

    const char* keywords[] = { "layout", "compressSamples", "compressVars", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ibb:ml_TrainData.getTrainSamples", (char**)keywords, &layout, &compressSamples, &compressVars) )
    {
        ERRWRAP2(retval = _self_->getTrainSamples(layout, compressSamples, compressVars));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getValues(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    {
    int vi=0;
    PyObject* pyobj_sidx = NULL;
    Mat sidx;
    float values=0.f;

    const char* keywords[] = { "vi", "sidx", "values", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iOf:ml_TrainData.getValues", (char**)keywords, &vi, &pyobj_sidx, &values) &&
        pyopencv_to(pyobj_sidx, sidx, ArgInfo("sidx", 0)) )
    {
        ERRWRAP2(_self_->getValues(vi, sidx, &values));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    int vi=0;
    PyObject* pyobj_sidx = NULL;
    UMat sidx;
    float values=0.f;

    const char* keywords[] = { "vi", "sidx", "values", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iOf:ml_TrainData.getValues", (char**)keywords, &vi, &pyobj_sidx, &values) &&
        pyopencv_to(pyobj_sidx, sidx, ArgInfo("sidx", 0)) )
    {
        ERRWRAP2(_self_->getValues(vi, sidx, &values));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getVarIdx(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarIdx());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getVarSymbolFlags(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarSymbolFlags());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_getVarType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_setTrainTestSplit(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    int count=0;
    bool shuffle=true;

    const char* keywords[] = { "count", "shuffle", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|b:ml_TrainData.setTrainTestSplit", (char**)keywords, &count, &shuffle) )
    {
        ERRWRAP2(_self_->setTrainTestSplit(count, shuffle));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_setTrainTestSplitRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");
    double ratio=0;
    bool shuffle=true;

    const char* keywords[] = { "ratio", "shuffle", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d|b:ml_TrainData.setTrainTestSplitRatio", (char**)keywords, &ratio, &shuffle) )
    {
        ERRWRAP2(_self_->setTrainTestSplitRatio(ratio, shuffle));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_TrainData_shuffleTrainTest(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::TrainData* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_TrainData_Type))
        _self_ = ((pyopencv_ml_TrainData_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_TrainData' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->shuffleTrainTest());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_TrainData_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_create_cls, METH_CLASS), "create(samples, layout, responses[, varIdx[, sampleIdx[, sampleWeights[, varType]]]]) -> retval\n.   @brief Creates training data from in-memory arrays.\n.   \n.   @param samples matrix of samples. It should have CV_32F type.\n.   @param layout see ml::SampleTypes.\n.   @param responses matrix of responses. If the responses are scalar, they should be stored as a\n.   single row or as a single column. The matrix should have type CV_32F or CV_32S (in the\n.   former case the responses are considered as ordered by default; in the latter case - as\n.   categorical)\n.   @param varIdx vector specifying which variables to use for training. It can be an integer vector\n.   (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of\n.   active variables.\n.   @param sampleIdx vector specifying which samples to use for training. It can be an integer\n.   vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask\n.   of training samples.\n.   @param sampleWeights optional vector with weights for each sample. It should have CV_32F type.\n.   @param varType optional vector of type CV_8U and size `<number_of_variables_in_samples> +\n.   <number_of_variables_in_responses>`, containing types of each input and output variable. See\n.   ml::VariableTypes."},
    {"getCatCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getCatCount, 0), "getCatCount(vi) -> retval\n."},
    {"getCatMap", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getCatMap, 0), "getCatMap() -> retval\n."},
    {"getCatOfs", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getCatOfs, 0), "getCatOfs() -> retval\n."},
    {"getClassLabels", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getClassLabels, 0), "getClassLabels() -> retval\n.   @brief Returns the vector of class labels\n.   \n.   The function returns vector of unique labels occurred in the responses."},
    {"getDefaultSubstValues", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getDefaultSubstValues, 0), "getDefaultSubstValues() -> retval\n."},
    {"getLayout", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getLayout, 0), "getLayout() -> retval\n."},
    {"getMissing", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getMissing, 0), "getMissing() -> retval\n."},
    {"getNAllVars", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNAllVars, 0), "getNAllVars() -> retval\n."},
    {"getNSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNSamples, 0), "getNSamples() -> retval\n."},
    {"getNTestSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNTestSamples, 0), "getNTestSamples() -> retval\n."},
    {"getNTrainSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNTrainSamples, 0), "getNTrainSamples() -> retval\n."},
    {"getNVars", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNVars, 0), "getNVars() -> retval\n."},
    {"getNames", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNames, 0), "getNames(names) -> None\n.   @brief Returns vector of symbolic names captured in loadFromCSV()"},
    {"getNormCatResponses", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getNormCatResponses, 0), "getNormCatResponses() -> retval\n."},
    {"getResponseType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getResponseType, 0), "getResponseType() -> retval\n."},
    {"getResponses", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getResponses, 0), "getResponses() -> retval\n."},
    {"getSample", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSample, 0), "getSample(varIdx, sidx, buf) -> None\n."},
    {"getSampleWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSampleWeights, 0), "getSampleWeights() -> retval\n."},
    {"getSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSamples, 0), "getSamples() -> retval\n."},
    {"getSubMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSubMatrix_cls, METH_CLASS), "getSubMatrix(matrix, idx, layout) -> retval\n.   @brief Extract from matrix rows/cols specified by passed indexes.\n.   @param matrix input matrix (supported types: CV_32S, CV_32F, CV_64F)\n.   @param idx 1D index vector\n.   @param layout specifies to extract rows (cv::ml::ROW_SAMPLES) or to extract columns (cv::ml::COL_SAMPLES)"},
    {"getSubVector", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getSubVector_cls, METH_CLASS), "getSubVector(vec, idx) -> retval\n.   @brief Extract from 1D vector elements specified by passed indexes.\n.   @param vec input vector (supported types: CV_32S, CV_32F, CV_64F)\n.   @param idx 1D index vector"},
    {"getTestNormCatResponses", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestNormCatResponses, 0), "getTestNormCatResponses() -> retval\n."},
    {"getTestResponses", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestResponses, 0), "getTestResponses() -> retval\n."},
    {"getTestSampleIdx", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestSampleIdx, 0), "getTestSampleIdx() -> retval\n."},
    {"getTestSampleWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestSampleWeights, 0), "getTestSampleWeights() -> retval\n."},
    {"getTestSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTestSamples, 0), "getTestSamples() -> retval\n.   @brief Returns matrix of test samples"},
    {"getTrainNormCatResponses", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainNormCatResponses, 0), "getTrainNormCatResponses() -> retval\n.   @brief Returns the vector of normalized categorical responses\n.   \n.   The function returns vector of responses. Each response is integer from `0` to `<number of\n.   classes>-1`. The actual label value can be retrieved then from the class label vector, see\n.   TrainData::getClassLabels."},
    {"getTrainResponses", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainResponses, 0), "getTrainResponses() -> retval\n.   @brief Returns the vector of responses\n.   \n.   The function returns ordered or the original categorical responses. Usually it's used in\n.   regression algorithms."},
    {"getTrainSampleIdx", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainSampleIdx, 0), "getTrainSampleIdx() -> retval\n."},
    {"getTrainSampleWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainSampleWeights, 0), "getTrainSampleWeights() -> retval\n."},
    {"getTrainSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getTrainSamples, 0), "getTrainSamples([, layout[, compressSamples[, compressVars]]]) -> retval\n.   @brief Returns matrix of train samples\n.   \n.   @param layout The requested layout. If it's different from the initial one, the matrix is\n.   transposed. See ml::SampleTypes.\n.   @param compressSamples if true, the function returns only the training samples (specified by\n.   sampleIdx)\n.   @param compressVars if true, the function returns the shorter training samples, containing only\n.   the active variables.\n.   \n.   In current implementation the function tries to avoid physical data copying and returns the\n.   matrix stored inside TrainData (unless the transposition or compression is needed)."},
    {"getValues", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getValues, 0), "getValues(vi, sidx, values) -> None\n."},
    {"getVarIdx", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getVarIdx, 0), "getVarIdx() -> retval\n."},
    {"getVarSymbolFlags", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getVarSymbolFlags, 0), "getVarSymbolFlags() -> retval\n."},
    {"getVarType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_getVarType, 0), "getVarType() -> retval\n."},
    {"setTrainTestSplit", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_setTrainTestSplit, 0), "setTrainTestSplit(count[, shuffle]) -> None\n.   @brief Splits the training data into the training and test parts\n.   @sa TrainData::setTrainTestSplitRatio"},
    {"setTrainTestSplitRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_setTrainTestSplitRatio, 0), "setTrainTestSplitRatio(ratio[, shuffle]) -> None\n.   @brief Splits the training data into the training and test parts\n.   \n.   The function selects a subset of specified relative size and then returns it as the training\n.   set. If the function is not called, all the data is used for training. Please, note that for\n.   each of TrainData::getTrain\\* there is corresponding TrainData::getTest\\*, so that the test\n.   subset can be retrieved and processed as well.\n.   @sa TrainData::setTrainTestSplit"},
    {"shuffleTrainTest", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_TrainData_shuffleTrainTest, 0), "shuffleTrainTest() -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_ml_TrainData_specials(void)
{
    pyopencv_ml_TrainData_Type.tp_base = NULL;
    pyopencv_ml_TrainData_Type.tp_dealloc = pyopencv_ml_TrainData_dealloc;
    pyopencv_ml_TrainData_Type.tp_repr = pyopencv_ml_TrainData_repr;
    pyopencv_ml_TrainData_Type.tp_getset = pyopencv_ml_TrainData_getseters;
    pyopencv_ml_TrainData_Type.tp_init = (initproc)0;
    pyopencv_ml_TrainData_Type.tp_methods = pyopencv_ml_TrainData_methods;
}

static PyObject* pyopencv_ml_StatModel_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_StatModel %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_StatModel_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_StatModel_calcError(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    {
    PyObject* pyobj_data = NULL;
    Ptr<TrainData> data;
    bool test=0;
    PyObject* pyobj_resp = NULL;
    Mat resp;
    float retval;

    const char* keywords[] = { "data", "test", "resp", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Ob|O:ml_StatModel.calcError", (char**)keywords, &pyobj_data, &test, &pyobj_resp) &&
        pyopencv_to(pyobj_data, data, ArgInfo("data", 0)) &&
        pyopencv_to(pyobj_resp, resp, ArgInfo("resp", 1)) )
    {
        ERRWRAP2(retval = _self_->calcError(data, test, resp));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(resp));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_data = NULL;
    Ptr<TrainData> data;
    bool test=0;
    PyObject* pyobj_resp = NULL;
    UMat resp;
    float retval;

    const char* keywords[] = { "data", "test", "resp", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Ob|O:ml_StatModel.calcError", (char**)keywords, &pyobj_data, &test, &pyobj_resp) &&
        pyopencv_to(pyobj_data, data, ArgInfo("data", 0)) &&
        pyopencv_to(pyobj_resp, resp, ArgInfo("resp", 1)) )
    {
        ERRWRAP2(retval = _self_->calcError(data, test, resp));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(resp));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_StatModel_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_StatModel_getVarCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_StatModel_isClassifier(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isClassifier());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_StatModel_isTrained(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isTrained());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_StatModel_predict(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_results = NULL;
    Mat results;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ml_StatModel.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &flags) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(results));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_results = NULL;
    UMat results;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ml_StatModel.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &flags) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(results));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_StatModel_train(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::StatModel* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_StatModel_Type))
        _self_ = dynamic_cast<cv::ml::StatModel*>(((pyopencv_ml_StatModel_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_StatModel' or its derivative)");
    {
    PyObject* pyobj_trainData = NULL;
    Ptr<TrainData> trainData;
    int flags=0;
    bool retval;

    const char* keywords[] = { "trainData", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|i:ml_StatModel.train", (char**)keywords, &pyobj_trainData, &flags) &&
        pyopencv_to(pyobj_trainData, trainData, ArgInfo("trainData", 0)) )
    {
        ERRWRAP2(retval = _self_->train(trainData, flags));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    int layout=0;
    PyObject* pyobj_responses = NULL;
    Mat responses;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO:ml_StatModel.train", (char**)keywords, &pyobj_samples, &layout, &pyobj_responses) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_responses, responses, ArgInfo("responses", 0)) )
    {
        ERRWRAP2(retval = _self_->train(samples, layout, responses));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    int layout=0;
    PyObject* pyobj_responses = NULL;
    UMat responses;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO:ml_StatModel.train", (char**)keywords, &pyobj_samples, &layout, &pyobj_responses) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_responses, responses, ArgInfo("responses", 0)) )
    {
        ERRWRAP2(retval = _self_->train(samples, layout, responses));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_StatModel_methods[] =
{
    {"calcError", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_calcError, 0), "calcError(data, test[, resp]) -> retval, resp\n.   @brief Computes error on the training or test dataset\n.   \n.   @param data the training data\n.   @param test if true, the error is computed over the test subset of the data, otherwise it's\n.   computed over the training subset of the data. Please note that if you loaded a completely\n.   different dataset to evaluate already trained classifier, you will probably want not to set\n.   the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n.   that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n.   @param resp the optional output responses.\n.   \n.   The method uses StatModel::predict to compute the error. For regression models the error is\n.   computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%)."},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_empty, 0), "empty() -> retval\n."},
    {"getVarCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_getVarCount, 0), "getVarCount() -> retval\n.   @brief Returns the number of variables in training samples"},
    {"isClassifier", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_isClassifier, 0), "isClassifier() -> retval\n.   @brief Returns true if the model is classifier"},
    {"isTrained", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_isTrained, 0), "isTrained() -> retval\n.   @brief Returns true if the model is trained"},
    {"predict", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_predict, 0), "predict(samples[, results[, flags]]) -> retval, results\n.   @brief Predicts response(s) for the provided sample(s)\n.   \n.   @param samples The input samples, floating-point matrix\n.   @param results The optional output matrix of results.\n.   @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags."},
    {"train", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_StatModel_train, 0), "train(trainData[, flags]) -> retval\n.   @brief Trains the statistical model\n.   \n.   @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n.   created with TrainData::create.\n.   @param flags optional flags, depending on the model. Some of the models can be updated with the\n.   new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n\n\n\ntrain(samples, layout, responses) -> retval\n.   @brief Trains the statistical model\n.   \n.   @param samples training samples\n.   @param layout See ml::SampleTypes.\n.   @param responses vector of responses associated with the training samples."},

    {NULL,          NULL}
};

static void pyopencv_ml_StatModel_specials(void)
{
    pyopencv_ml_StatModel_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ml_StatModel_Type.tp_dealloc = pyopencv_ml_StatModel_dealloc;
    pyopencv_ml_StatModel_Type.tp_repr = pyopencv_ml_StatModel_repr;
    pyopencv_ml_StatModel_Type.tp_getset = pyopencv_ml_StatModel_getseters;
    pyopencv_ml_StatModel_Type.tp_init = (initproc)0;
    pyopencv_ml_StatModel_Type.tp_methods = pyopencv_ml_StatModel_methods;
}

static PyObject* pyopencv_ml_NormalBayesClassifier_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_NormalBayesClassifier %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_NormalBayesClassifier_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_NormalBayesClassifier_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<NormalBayesClassifier> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::NormalBayesClassifier::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_NormalBayesClassifier_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<NormalBayesClassifier> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_NormalBayesClassifier.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::NormalBayesClassifier::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_NormalBayesClassifier_predictProb(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::NormalBayesClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_NormalBayesClassifier_Type))
        _self_ = dynamic_cast<cv::ml::NormalBayesClassifier*>(((pyopencv_ml_NormalBayesClassifier_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_NormalBayesClassifier' or its derivative)");
    {
    PyObject* pyobj_inputs = NULL;
    Mat inputs;
    PyObject* pyobj_outputs = NULL;
    Mat outputs;
    PyObject* pyobj_outputProbs = NULL;
    Mat outputProbs;
    int flags=0;
    float retval;

    const char* keywords[] = { "inputs", "outputs", "outputProbs", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOi:ml_NormalBayesClassifier.predictProb", (char**)keywords, &pyobj_inputs, &pyobj_outputs, &pyobj_outputProbs, &flags) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        pyopencv_to(pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        pyopencv_to(pyobj_outputProbs, outputProbs, ArgInfo("outputProbs", 1)) )
    {
        ERRWRAP2(retval = _self_->predictProb(inputs, outputs, outputProbs, flags));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(outputs), pyopencv_from(outputProbs));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputs = NULL;
    UMat inputs;
    PyObject* pyobj_outputs = NULL;
    UMat outputs;
    PyObject* pyobj_outputProbs = NULL;
    UMat outputProbs;
    int flags=0;
    float retval;

    const char* keywords[] = { "inputs", "outputs", "outputProbs", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOi:ml_NormalBayesClassifier.predictProb", (char**)keywords, &pyobj_inputs, &pyobj_outputs, &pyobj_outputProbs, &flags) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        pyopencv_to(pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        pyopencv_to(pyobj_outputProbs, outputProbs, ArgInfo("outputProbs", 1)) )
    {
        ERRWRAP2(retval = _self_->predictProb(inputs, outputs, outputProbs, flags));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(outputs), pyopencv_from(outputProbs));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_NormalBayesClassifier_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_NormalBayesClassifier_create_cls, METH_CLASS), "create() -> retval\n.   Creates empty model\n.   Use StatModel::train to train the model after creation."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_NormalBayesClassifier_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized NormalBayesClassifier from a file\n.   *\n.   * Use NormalBayesClassifier::save to serialize and store an NormalBayesClassifier to disk.\n.   * Load the NormalBayesClassifier from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized NormalBayesClassifier\n.   * @param nodeName name of node containing the classifier"},
    {"predictProb", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_NormalBayesClassifier_predictProb, 0), "predictProb(inputs[, outputs[, outputProbs[, flags]]]) -> retval, outputs, outputProbs\n.   @brief Predicts the response for sample(s).\n.   \n.   The method estimates the most probable classes for input vectors. Input vectors (one or more)\n.   are stored as rows of the matrix inputs. In case of multiple input vectors, there should be one\n.   output vector outputs. The predicted class for a single input vector is returned by the method.\n.   The vector outputProbs contains the output probabilities corresponding to each element of\n.   result."},

    {NULL,          NULL}
};

static void pyopencv_ml_NormalBayesClassifier_specials(void)
{
    pyopencv_ml_NormalBayesClassifier_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_NormalBayesClassifier_Type.tp_dealloc = pyopencv_ml_NormalBayesClassifier_dealloc;
    pyopencv_ml_NormalBayesClassifier_Type.tp_repr = pyopencv_ml_NormalBayesClassifier_repr;
    pyopencv_ml_NormalBayesClassifier_Type.tp_getset = pyopencv_ml_NormalBayesClassifier_getseters;
    pyopencv_ml_NormalBayesClassifier_Type.tp_init = (initproc)0;
    pyopencv_ml_NormalBayesClassifier_Type.tp_methods = pyopencv_ml_NormalBayesClassifier_methods;
}

static PyObject* pyopencv_ml_KNearest_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_KNearest %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_KNearest_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_KNearest_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<KNearest> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::KNearest::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_findNearest(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    int k=0;
    PyObject* pyobj_results = NULL;
    Mat results;
    PyObject* pyobj_neighborResponses = NULL;
    Mat neighborResponses;
    PyObject* pyobj_dist = NULL;
    Mat dist;
    float retval;

    const char* keywords[] = { "samples", "k", "results", "neighborResponses", "dist", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|OOO:ml_KNearest.findNearest", (char**)keywords, &pyobj_samples, &k, &pyobj_results, &pyobj_neighborResponses, &pyobj_dist) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) &&
        pyopencv_to(pyobj_neighborResponses, neighborResponses, ArgInfo("neighborResponses", 1)) &&
        pyopencv_to(pyobj_dist, dist, ArgInfo("dist", 1)) )
    {
        ERRWRAP2(retval = _self_->findNearest(samples, k, results, neighborResponses, dist));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(results), pyopencv_from(neighborResponses), pyopencv_from(dist));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    int k=0;
    PyObject* pyobj_results = NULL;
    UMat results;
    PyObject* pyobj_neighborResponses = NULL;
    UMat neighborResponses;
    PyObject* pyobj_dist = NULL;
    UMat dist;
    float retval;

    const char* keywords[] = { "samples", "k", "results", "neighborResponses", "dist", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|OOO:ml_KNearest.findNearest", (char**)keywords, &pyobj_samples, &k, &pyobj_results, &pyobj_neighborResponses, &pyobj_dist) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) &&
        pyopencv_to(pyobj_neighborResponses, neighborResponses, ArgInfo("neighborResponses", 1)) &&
        pyopencv_to(pyobj_dist, dist, ArgInfo("dist", 1)) )
    {
        ERRWRAP2(retval = _self_->findNearest(samples, k, results, neighborResponses, dist));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(results), pyopencv_from(neighborResponses), pyopencv_from(dist));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_getAlgorithmType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAlgorithmType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_getDefaultK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultK());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_getEmax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEmax());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_getIsClassifier(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIsClassifier());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_setAlgorithmType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_KNearest.setAlgorithmType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAlgorithmType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_setDefaultK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_KNearest.setDefaultK", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setDefaultK(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_setEmax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_KNearest.setEmax", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setEmax(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_KNearest_setIsClassifier(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::KNearest* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_KNearest_Type))
        _self_ = dynamic_cast<cv::ml::KNearest*>(((pyopencv_ml_KNearest_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_KNearest' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ml_KNearest.setIsClassifier", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setIsClassifier(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_KNearest_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_create_cls, METH_CLASS), "create() -> retval\n.   @brief Creates the empty model\n.   \n.   The static method creates empty %KNearest classifier. It should be then trained using StatModel::train method."},
    {"findNearest", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_findNearest, 0), "findNearest(samples, k[, results[, neighborResponses[, dist]]]) -> retval, results, neighborResponses, dist\n.   @brief Finds the neighbors and predicts responses for input vectors.\n.   \n.   @param samples Input samples stored by rows. It is a single-precision floating-point matrix of\n.   `<number_of_samples> * k` size.\n.   @param k Number of used nearest neighbors. Should be greater than 1.\n.   @param results Vector with results of prediction (regression or classification) for each input\n.   sample. It is a single-precision floating-point vector with `<number_of_samples>` elements.\n.   @param neighborResponses Optional output values for corresponding neighbors. It is a single-\n.   precision floating-point matrix of `<number_of_samples> * k` size.\n.   @param dist Optional output distances from the input vectors to the corresponding neighbors. It\n.   is a single-precision floating-point matrix of `<number_of_samples> * k` size.\n.   \n.   For each input vector (a row of the matrix samples), the method finds the k nearest neighbors.\n.   In case of regression, the predicted result is a mean value of the particular vector's neighbor\n.   responses. In case of classification, the class is determined by voting.\n.   \n.   For each input vector, the neighbors are sorted by their distances to the vector.\n.   \n.   In case of C++ interface you can use output pointers to empty matrices and the function will\n.   allocate memory itself.\n.   \n.   If only a single input vector is passed, all output matrices are optional and the predicted\n.   value is returned by the method.\n.   \n.   The function is parallelized with the TBB library."},
    {"getAlgorithmType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getAlgorithmType, 0), "getAlgorithmType() -> retval\n.   @see setAlgorithmType"},
    {"getDefaultK", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getDefaultK, 0), "getDefaultK() -> retval\n.   @see setDefaultK"},
    {"getEmax", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getEmax, 0), "getEmax() -> retval\n.   @see setEmax"},
    {"getIsClassifier", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_getIsClassifier, 0), "getIsClassifier() -> retval\n.   @see setIsClassifier"},
    {"setAlgorithmType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setAlgorithmType, 0), "setAlgorithmType(val) -> None\n.   @copybrief getAlgorithmType @see getAlgorithmType"},
    {"setDefaultK", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setDefaultK, 0), "setDefaultK(val) -> None\n.   @copybrief getDefaultK @see getDefaultK"},
    {"setEmax", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setEmax, 0), "setEmax(val) -> None\n.   @copybrief getEmax @see getEmax"},
    {"setIsClassifier", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_KNearest_setIsClassifier, 0), "setIsClassifier(val) -> None\n.   @copybrief getIsClassifier @see getIsClassifier"},

    {NULL,          NULL}
};

static void pyopencv_ml_KNearest_specials(void)
{
    pyopencv_ml_KNearest_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_KNearest_Type.tp_dealloc = pyopencv_ml_KNearest_dealloc;
    pyopencv_ml_KNearest_Type.tp_repr = pyopencv_ml_KNearest_repr;
    pyopencv_ml_KNearest_Type.tp_getset = pyopencv_ml_KNearest_getseters;
    pyopencv_ml_KNearest_Type.tp_init = (initproc)0;
    pyopencv_ml_KNearest_Type.tp_methods = pyopencv_ml_KNearest_methods;
}

static PyObject* pyopencv_ml_SVM_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_SVM %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_SVM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_SVM_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<SVM> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::SVM::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getC(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getC());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getClassWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getClassWeights());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getCoef0(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCoef0());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getDecisionFunction(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    {
    int i=0;
    PyObject* pyobj_alpha = NULL;
    Mat alpha;
    PyObject* pyobj_svidx = NULL;
    Mat svidx;
    double retval;

    const char* keywords[] = { "i", "alpha", "svidx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|OO:ml_SVM.getDecisionFunction", (char**)keywords, &i, &pyobj_alpha, &pyobj_svidx) &&
        pyopencv_to(pyobj_alpha, alpha, ArgInfo("alpha", 1)) &&
        pyopencv_to(pyobj_svidx, svidx, ArgInfo("svidx", 1)) )
    {
        ERRWRAP2(retval = _self_->getDecisionFunction(i, alpha, svidx));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(alpha), pyopencv_from(svidx));
    }
    }
    PyErr_Clear();

    {
    int i=0;
    PyObject* pyobj_alpha = NULL;
    UMat alpha;
    PyObject* pyobj_svidx = NULL;
    UMat svidx;
    double retval;

    const char* keywords[] = { "i", "alpha", "svidx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|OO:ml_SVM.getDecisionFunction", (char**)keywords, &i, &pyobj_alpha, &pyobj_svidx) &&
        pyopencv_to(pyobj_alpha, alpha, ArgInfo("alpha", 1)) &&
        pyopencv_to(pyobj_svidx, svidx, ArgInfo("svidx", 1)) )
    {
        ERRWRAP2(retval = _self_->getDecisionFunction(i, alpha, svidx));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(alpha), pyopencv_from(svidx));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getDefaultGridPtr_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    int param_id=0;
    Ptr<ParamGrid> retval;

    const char* keywords[] = { "param_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_SVM.getDefaultGridPtr", (char**)keywords, &param_id) )
    {
        ERRWRAP2(retval = cv::ml::SVM::getDefaultGridPtr(param_id));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getDegree(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDegree());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGamma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getKernelType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getKernelType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getNu(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNu());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getP(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getP());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getSupportVectors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSupportVectors());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    cv::TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_getUncompressedSupportVectors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUncompressedSupportVectors());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    Ptr<SVM> retval;

    const char* keywords[] = { "filepath", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_SVM.load", (char**)keywords, &pyobj_filepath) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) )
    {
        ERRWRAP2(retval = cv::ml::SVM::load(filepath));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setC(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_SVM.setC", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setC(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setClassWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_SVM.setClassWeights", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setClassWeights(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_SVM.setClassWeights", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setClassWeights(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setCoef0(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_SVM.setCoef0", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setCoef0(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setDegree(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_SVM.setDegree", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setDegree(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_SVM.setGamma", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setKernel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    int kernelType=0;

    const char* keywords[] = { "kernelType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_SVM.setKernel", (char**)keywords, &kernelType) )
    {
        ERRWRAP2(_self_->setKernel(kernelType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setNu(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_SVM.setNu", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setNu(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setP(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_SVM.setP", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setP(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    PyObject* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_SVM.setTermCriteria", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_setType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_SVM.setType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVM_trainAuto(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVM_Type))
        _self_ = dynamic_cast<cv::ml::SVM*>(((pyopencv_ml_SVM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVM' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    int layout=0;
    PyObject* pyobj_responses = NULL;
    Mat responses;
    int kFold=10;
    PyObject* pyobj_Cgrid = NULL;
    Ptr<ParamGrid> Cgrid=SVM::getDefaultGridPtr(SVM::C);
    PyObject* pyobj_gammaGrid = NULL;
    Ptr<ParamGrid> gammaGrid=SVM::getDefaultGridPtr(SVM::GAMMA);
    PyObject* pyobj_pGrid = NULL;
    Ptr<ParamGrid> pGrid=SVM::getDefaultGridPtr(SVM::P);
    PyObject* pyobj_nuGrid = NULL;
    Ptr<ParamGrid> nuGrid=SVM::getDefaultGridPtr(SVM::NU);
    PyObject* pyobj_coeffGrid = NULL;
    Ptr<ParamGrid> coeffGrid=SVM::getDefaultGridPtr(SVM::COEF);
    PyObject* pyobj_degreeGrid = NULL;
    Ptr<ParamGrid> degreeGrid=SVM::getDefaultGridPtr(SVM::DEGREE);
    bool balanced=false;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", "kFold", "Cgrid", "gammaGrid", "pGrid", "nuGrid", "coeffGrid", "degreeGrid", "balanced", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO|iOOOOOOb:ml_SVM.trainAuto", (char**)keywords, &pyobj_samples, &layout, &pyobj_responses, &kFold, &pyobj_Cgrid, &pyobj_gammaGrid, &pyobj_pGrid, &pyobj_nuGrid, &pyobj_coeffGrid, &pyobj_degreeGrid, &balanced) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_responses, responses, ArgInfo("responses", 0)) &&
        pyopencv_to(pyobj_Cgrid, Cgrid, ArgInfo("Cgrid", 0)) &&
        pyopencv_to(pyobj_gammaGrid, gammaGrid, ArgInfo("gammaGrid", 0)) &&
        pyopencv_to(pyobj_pGrid, pGrid, ArgInfo("pGrid", 0)) &&
        pyopencv_to(pyobj_nuGrid, nuGrid, ArgInfo("nuGrid", 0)) &&
        pyopencv_to(pyobj_coeffGrid, coeffGrid, ArgInfo("coeffGrid", 0)) &&
        pyopencv_to(pyobj_degreeGrid, degreeGrid, ArgInfo("degreeGrid", 0)) )
    {
        ERRWRAP2(retval = _self_->trainAuto(samples, layout, responses, kFold, Cgrid, gammaGrid, pGrid, nuGrid, coeffGrid, degreeGrid, balanced));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    int layout=0;
    PyObject* pyobj_responses = NULL;
    UMat responses;
    int kFold=10;
    PyObject* pyobj_Cgrid = NULL;
    Ptr<ParamGrid> Cgrid=SVM::getDefaultGridPtr(SVM::C);
    PyObject* pyobj_gammaGrid = NULL;
    Ptr<ParamGrid> gammaGrid=SVM::getDefaultGridPtr(SVM::GAMMA);
    PyObject* pyobj_pGrid = NULL;
    Ptr<ParamGrid> pGrid=SVM::getDefaultGridPtr(SVM::P);
    PyObject* pyobj_nuGrid = NULL;
    Ptr<ParamGrid> nuGrid=SVM::getDefaultGridPtr(SVM::NU);
    PyObject* pyobj_coeffGrid = NULL;
    Ptr<ParamGrid> coeffGrid=SVM::getDefaultGridPtr(SVM::COEF);
    PyObject* pyobj_degreeGrid = NULL;
    Ptr<ParamGrid> degreeGrid=SVM::getDefaultGridPtr(SVM::DEGREE);
    bool balanced=false;
    bool retval;

    const char* keywords[] = { "samples", "layout", "responses", "kFold", "Cgrid", "gammaGrid", "pGrid", "nuGrid", "coeffGrid", "degreeGrid", "balanced", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO|iOOOOOOb:ml_SVM.trainAuto", (char**)keywords, &pyobj_samples, &layout, &pyobj_responses, &kFold, &pyobj_Cgrid, &pyobj_gammaGrid, &pyobj_pGrid, &pyobj_nuGrid, &pyobj_coeffGrid, &pyobj_degreeGrid, &balanced) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_responses, responses, ArgInfo("responses", 0)) &&
        pyopencv_to(pyobj_Cgrid, Cgrid, ArgInfo("Cgrid", 0)) &&
        pyopencv_to(pyobj_gammaGrid, gammaGrid, ArgInfo("gammaGrid", 0)) &&
        pyopencv_to(pyobj_pGrid, pGrid, ArgInfo("pGrid", 0)) &&
        pyopencv_to(pyobj_nuGrid, nuGrid, ArgInfo("nuGrid", 0)) &&
        pyopencv_to(pyobj_coeffGrid, coeffGrid, ArgInfo("coeffGrid", 0)) &&
        pyopencv_to(pyobj_degreeGrid, degreeGrid, ArgInfo("degreeGrid", 0)) )
    {
        ERRWRAP2(retval = _self_->trainAuto(samples, layout, responses, kFold, Cgrid, gammaGrid, pGrid, nuGrid, coeffGrid, degreeGrid, balanced));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_SVM_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_create_cls, METH_CLASS), "create() -> retval\n.   Creates empty model.\n.   Use StatModel::train to train the model. Since %SVM has several parameters, you may want to\n.   find the best parameters for your problem, it can be done with SVM::trainAuto."},
    {"getC", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getC, 0), "getC() -> retval\n.   @see setC"},
    {"getClassWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getClassWeights, 0), "getClassWeights() -> retval\n.   @see setClassWeights"},
    {"getCoef0", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getCoef0, 0), "getCoef0() -> retval\n.   @see setCoef0"},
    {"getDecisionFunction", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getDecisionFunction, 0), "getDecisionFunction(i[, alpha[, svidx]]) -> retval, alpha, svidx\n.   @brief Retrieves the decision function\n.   \n.   @param i the index of the decision function. If the problem solved is regression, 1-class or\n.   2-class classification, then there will be just one decision function and the index should\n.   always be 0. Otherwise, in the case of N-class classification, there will be \\f$N(N-1)/2\\f$\n.   decision functions.\n.   @param alpha the optional output vector for weights, corresponding to different support vectors.\n.   In the case of linear %SVM all the alpha's will be 1's.\n.   @param svidx the optional output vector of indices of support vectors within the matrix of\n.   support vectors (which can be retrieved by SVM::getSupportVectors). In the case of linear\n.   %SVM each decision function consists of a single \"compressed\" support vector.\n.   \n.   The method returns rho parameter of the decision function, a scalar subtracted from the weighted\n.   sum of kernel responses."},
    {"getDefaultGridPtr", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getDefaultGridPtr_cls, METH_CLASS), "getDefaultGridPtr(param_id) -> retval\n.   @brief Generates a grid for %SVM parameters.\n.   \n.   @param param_id %SVM parameters IDs that must be one of the SVM::ParamTypes. The grid is\n.   generated for the parameter with this ID.\n.   \n.   The function generates a grid pointer for the specified parameter of the %SVM algorithm.\n.   The grid may be passed to the function SVM::trainAuto."},
    {"getDegree", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getDegree, 0), "getDegree() -> retval\n.   @see setDegree"},
    {"getGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getGamma, 0), "getGamma() -> retval\n.   @see setGamma"},
    {"getKernelType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getKernelType, 0), "getKernelType() -> retval\n.   Type of a %SVM kernel.\n.   See SVM::KernelTypes. Default value is SVM::RBF."},
    {"getNu", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getNu, 0), "getNu() -> retval\n.   @see setNu"},
    {"getP", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getP, 0), "getP() -> retval\n.   @see setP"},
    {"getSupportVectors", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getSupportVectors, 0), "getSupportVectors() -> retval\n.   @brief Retrieves all the support vectors\n.   \n.   The method returns all the support vectors as a floating-point matrix, where support vectors are\n.   stored as matrix rows."},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getType, 0), "getType() -> retval\n.   @see setType"},
    {"getUncompressedSupportVectors", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_getUncompressedSupportVectors, 0), "getUncompressedSupportVectors() -> retval\n.   @brief Retrieves all the uncompressed support vectors of a linear %SVM\n.   \n.   The method returns all the uncompressed support vectors of a linear %SVM that the compressed\n.   support vector, used for prediction, was derived from. They are returned in a floating-point\n.   matrix, where the support vectors are stored as matrix rows."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_load_cls, METH_CLASS), "load(filepath) -> retval\n.   @brief Loads and creates a serialized svm from a file\n.   *\n.   * Use SVM::save to serialize and store an SVM to disk.\n.   * Load the SVM from this file again, by calling this function with the path to the file.\n.   *\n.   * @param filepath path to serialized svm"},
    {"setC", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setC, 0), "setC(val) -> None\n.   @copybrief getC @see getC"},
    {"setClassWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setClassWeights, 0), "setClassWeights(val) -> None\n.   @copybrief getClassWeights @see getClassWeights"},
    {"setCoef0", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setCoef0, 0), "setCoef0(val) -> None\n.   @copybrief getCoef0 @see getCoef0"},
    {"setDegree", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setDegree, 0), "setDegree(val) -> None\n.   @copybrief getDegree @see getDegree"},
    {"setGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setGamma, 0), "setGamma(val) -> None\n.   @copybrief getGamma @see getGamma"},
    {"setKernel", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setKernel, 0), "setKernel(kernelType) -> None\n.   Initialize with one of predefined kernels.\n.   See SVM::KernelTypes."},
    {"setNu", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setNu, 0), "setNu(val) -> None\n.   @copybrief getNu @see getNu"},
    {"setP", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setP, 0), "setP(val) -> None\n.   @copybrief getP @see getP"},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"setType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_setType, 0), "setType(val) -> None\n.   @copybrief getType @see getType"},
    {"trainAuto", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVM_trainAuto, 0), "trainAuto(samples, layout, responses[, kFold[, Cgrid[, gammaGrid[, pGrid[, nuGrid[, coeffGrid[, degreeGrid[, balanced]]]]]]]]) -> retval\n.   @brief Trains an %SVM with optimal parameters\n.   \n.   @param samples training samples\n.   @param layout See ml::SampleTypes.\n.   @param responses vector of responses associated with the training samples.\n.   @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One\n.   subset is used to test the model, the others form the train set. So, the %SVM algorithm is\n.   @param Cgrid grid for C\n.   @param gammaGrid grid for gamma\n.   @param pGrid grid for p\n.   @param nuGrid grid for nu\n.   @param coeffGrid grid for coeff\n.   @param degreeGrid grid for degree\n.   @param balanced If true and the problem is 2-class classification then the method creates more\n.   balanced cross-validation subsets that is proportions between classes in subsets are close\n.   to such proportion in the whole train dataset.\n.   \n.   The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,\n.   nu, coef0, degree. Parameters are considered optimal when the cross-validation\n.   estimate of the test set error is minimal.\n.   \n.   This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only\n.   offers rudimentary parameter options.\n.   \n.   This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the\n.   regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and\n.   the usual %SVM with parameters specified in params is executed."},

    {NULL,          NULL}
};

static void pyopencv_ml_SVM_specials(void)
{
    pyopencv_ml_SVM_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_SVM_Type.tp_dealloc = pyopencv_ml_SVM_dealloc;
    pyopencv_ml_SVM_Type.tp_repr = pyopencv_ml_SVM_repr;
    pyopencv_ml_SVM_Type.tp_getset = pyopencv_ml_SVM_getseters;
    pyopencv_ml_SVM_Type.tp_init = (initproc)0;
    pyopencv_ml_SVM_Type.tp_methods = pyopencv_ml_SVM_methods;
}

static PyObject* pyopencv_ml_EM_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_EM %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_EM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_EM_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<EM> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::EM::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_getClustersNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getClustersNumber());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_getCovarianceMatrixType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCovarianceMatrixType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_getCovs(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    {
    PyObject* pyobj_covs = NULL;
    vector_Mat covs;

    const char* keywords[] = { "covs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ml_EM.getCovs", (char**)keywords, &pyobj_covs) &&
        pyopencv_to(pyobj_covs, covs, ArgInfo("covs", 1)) )
    {
        ERRWRAP2(_self_->getCovs(covs));
        return pyopencv_from(covs);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_covs = NULL;
    vector_Mat covs;

    const char* keywords[] = { "covs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ml_EM.getCovs", (char**)keywords, &pyobj_covs) &&
        pyopencv_to(pyobj_covs, covs, ArgInfo("covs", 1)) )
    {
        ERRWRAP2(_self_->getCovs(covs));
        return pyopencv_from(covs);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_getMeans(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMeans());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_getWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeights());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<EM> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_EM.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::EM::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_predict(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_results = NULL;
    Mat results;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ml_EM.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &flags) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(results));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_results = NULL;
    UMat results;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ml_EM.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &flags) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(results));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_predict2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    {
    PyObject* pyobj_sample = NULL;
    Mat sample;
    PyObject* pyobj_probs = NULL;
    Mat probs;
    Vec2d retval;

    const char* keywords[] = { "sample", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_EM.predict2", (char**)keywords, &pyobj_sample, &pyobj_probs) &&
        pyopencv_to(pyobj_sample, sample, ArgInfo("sample", 0)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->predict2(sample, probs));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(probs));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_sample = NULL;
    UMat sample;
    PyObject* pyobj_probs = NULL;
    UMat probs;
    Vec2d retval;

    const char* keywords[] = { "sample", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_EM.predict2", (char**)keywords, &pyobj_sample, &pyobj_probs) &&
        pyopencv_to(pyobj_sample, sample, ArgInfo("sample", 0)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->predict2(sample, probs));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(probs));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_setClustersNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_EM.setClustersNumber", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setClustersNumber(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_setCovarianceMatrixType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_EM.setCovarianceMatrixType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setCovarianceMatrixType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    PyObject* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_EM.setTermCriteria", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_trainE(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_means0 = NULL;
    Mat means0;
    PyObject* pyobj_covs0 = NULL;
    Mat covs0;
    PyObject* pyobj_weights0 = NULL;
    Mat weights0;
    PyObject* pyobj_logLikelihoods = NULL;
    Mat logLikelihoods;
    PyObject* pyobj_labels = NULL;
    Mat labels;
    PyObject* pyobj_probs = NULL;
    Mat probs;
    bool retval;

    const char* keywords[] = { "samples", "means0", "covs0", "weights0", "logLikelihoods", "labels", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OOOOO:ml_EM.trainE", (char**)keywords, &pyobj_samples, &pyobj_means0, &pyobj_covs0, &pyobj_weights0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_means0, means0, ArgInfo("means0", 0)) &&
        pyopencv_to(pyobj_covs0, covs0, ArgInfo("covs0", 0)) &&
        pyopencv_to(pyobj_weights0, weights0, ArgInfo("weights0", 0)) &&
        pyopencv_to(pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 1)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->trainE(samples, means0, covs0, weights0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(logLikelihoods), pyopencv_from(labels), pyopencv_from(probs));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_means0 = NULL;
    UMat means0;
    PyObject* pyobj_covs0 = NULL;
    UMat covs0;
    PyObject* pyobj_weights0 = NULL;
    UMat weights0;
    PyObject* pyobj_logLikelihoods = NULL;
    UMat logLikelihoods;
    PyObject* pyobj_labels = NULL;
    UMat labels;
    PyObject* pyobj_probs = NULL;
    UMat probs;
    bool retval;

    const char* keywords[] = { "samples", "means0", "covs0", "weights0", "logLikelihoods", "labels", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OOOOO:ml_EM.trainE", (char**)keywords, &pyobj_samples, &pyobj_means0, &pyobj_covs0, &pyobj_weights0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_means0, means0, ArgInfo("means0", 0)) &&
        pyopencv_to(pyobj_covs0, covs0, ArgInfo("covs0", 0)) &&
        pyopencv_to(pyobj_weights0, weights0, ArgInfo("weights0", 0)) &&
        pyopencv_to(pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 1)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->trainE(samples, means0, covs0, weights0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(logLikelihoods), pyopencv_from(labels), pyopencv_from(probs));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_trainEM(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_logLikelihoods = NULL;
    Mat logLikelihoods;
    PyObject* pyobj_labels = NULL;
    Mat labels;
    PyObject* pyobj_probs = NULL;
    Mat probs;
    bool retval;

    const char* keywords[] = { "samples", "logLikelihoods", "labels", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOO:ml_EM.trainEM", (char**)keywords, &pyobj_samples, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 1)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->trainEM(samples, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(logLikelihoods), pyopencv_from(labels), pyopencv_from(probs));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_logLikelihoods = NULL;
    UMat logLikelihoods;
    PyObject* pyobj_labels = NULL;
    UMat labels;
    PyObject* pyobj_probs = NULL;
    UMat probs;
    bool retval;

    const char* keywords[] = { "samples", "logLikelihoods", "labels", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOO:ml_EM.trainEM", (char**)keywords, &pyobj_samples, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 1)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->trainEM(samples, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(logLikelihoods), pyopencv_from(labels), pyopencv_from(probs));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_EM_trainM(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::EM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_EM_Type))
        _self_ = dynamic_cast<cv::ml::EM*>(((pyopencv_ml_EM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_EM' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_probs0 = NULL;
    Mat probs0;
    PyObject* pyobj_logLikelihoods = NULL;
    Mat logLikelihoods;
    PyObject* pyobj_labels = NULL;
    Mat labels;
    PyObject* pyobj_probs = NULL;
    Mat probs;
    bool retval;

    const char* keywords[] = { "samples", "probs0", "logLikelihoods", "labels", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OOO:ml_EM.trainM", (char**)keywords, &pyobj_samples, &pyobj_probs0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_probs0, probs0, ArgInfo("probs0", 0)) &&
        pyopencv_to(pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 1)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->trainM(samples, probs0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(logLikelihoods), pyopencv_from(labels), pyopencv_from(probs));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_probs0 = NULL;
    UMat probs0;
    PyObject* pyobj_logLikelihoods = NULL;
    UMat logLikelihoods;
    PyObject* pyobj_labels = NULL;
    UMat labels;
    PyObject* pyobj_probs = NULL;
    UMat probs;
    bool retval;

    const char* keywords[] = { "samples", "probs0", "logLikelihoods", "labels", "probs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OOO:ml_EM.trainM", (char**)keywords, &pyobj_samples, &pyobj_probs0, &pyobj_logLikelihoods, &pyobj_labels, &pyobj_probs) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_probs0, probs0, ArgInfo("probs0", 0)) &&
        pyopencv_to(pyobj_logLikelihoods, logLikelihoods, ArgInfo("logLikelihoods", 1)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 1)) &&
        pyopencv_to(pyobj_probs, probs, ArgInfo("probs", 1)) )
    {
        ERRWRAP2(retval = _self_->trainM(samples, probs0, logLikelihoods, labels, probs));
        return Py_BuildValue("(NNNN)", pyopencv_from(retval), pyopencv_from(logLikelihoods), pyopencv_from(labels), pyopencv_from(probs));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_EM_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_create_cls, METH_CLASS), "create() -> retval\n.   Creates empty %EM model.\n.   The model should be trained then using StatModel::train(traindata, flags) method. Alternatively, you\n.   can use one of the EM::train\\* methods or load it from file using Algorithm::load\\<EM\\>(filename)."},
    {"getClustersNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getClustersNumber, 0), "getClustersNumber() -> retval\n.   @see setClustersNumber"},
    {"getCovarianceMatrixType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getCovarianceMatrixType, 0), "getCovarianceMatrixType() -> retval\n.   @see setCovarianceMatrixType"},
    {"getCovs", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getCovs, 0), "getCovs([, covs]) -> covs\n.   @brief Returns covariation matrices\n.   \n.   Returns vector of covariation matrices. Number of matrices is the number of gaussian mixtures,\n.   each matrix is a square floating-point matrix NxN, where N is the space dimensionality."},
    {"getMeans", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getMeans, 0), "getMeans() -> retval\n.   @brief Returns the cluster centers (means of the Gaussian mixture)\n.   \n.   Returns matrix with the number of rows equal to the number of mixtures and number of columns\n.   equal to the space dimensionality."},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_getWeights, 0), "getWeights() -> retval\n.   @brief Returns weights of the mixtures\n.   \n.   Returns vector with the number of elements equal to the number of mixtures."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized EM from a file\n.   *\n.   * Use EM::save to serialize and store an EM to disk.\n.   * Load the EM from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized EM\n.   * @param nodeName name of node containing the classifier"},
    {"predict", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_predict, 0), "predict(samples[, results[, flags]]) -> retval, results\n.   @brief Returns posterior probabilities for the provided samples\n.   \n.   @param samples The input samples, floating-point matrix\n.   @param results The optional output \\f$ nSamples \\times nClusters\\f$ matrix of results. It contains\n.   posterior probabilities for each sample from the input\n.   @param flags This parameter will be ignored"},
    {"predict2", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_predict2, 0), "predict2(sample[, probs]) -> retval, probs\n.   @brief Returns a likelihood logarithm value and an index of the most probable mixture component\n.   for the given sample.\n.   \n.   @param sample A sample for classification. It should be a one-channel matrix of\n.   \\f$1 \\times dims\\f$ or \\f$dims \\times 1\\f$ size.\n.   @param probs Optional output matrix that contains posterior probabilities of each component\n.   given the sample. It has \\f$1 \\times nclusters\\f$ size and CV_64FC1 type.\n.   \n.   The method returns a two-element double vector. Zero element is a likelihood logarithm value for\n.   the sample. First element is an index of the most probable mixture component for the given\n.   sample."},
    {"setClustersNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_setClustersNumber, 0), "setClustersNumber(val) -> None\n.   @copybrief getClustersNumber @see getClustersNumber"},
    {"setCovarianceMatrixType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_setCovarianceMatrixType, 0), "setCovarianceMatrixType(val) -> None\n.   @copybrief getCovarianceMatrixType @see getCovarianceMatrixType"},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"trainE", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_trainE, 0), "trainE(samples, means0[, covs0[, weights0[, logLikelihoods[, labels[, probs]]]]]) -> retval, logLikelihoods, labels, probs\n.   @brief Estimate the Gaussian mixture parameters from a samples set.\n.   \n.   This variation starts with Expectation step. You need to provide initial means \\f$a_k\\f$ of\n.   mixture components. Optionally you can pass initial weights \\f$\\pi_k\\f$ and covariance matrices\n.   \\f$S_k\\f$ of mixture components.\n.   \n.   @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n.   one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n.   it will be converted to the inner matrix of such type for the further computing.\n.   @param means0 Initial means \\f$a_k\\f$ of mixture components. It is a one-channel matrix of\n.   \\f$nclusters \\times dims\\f$ size. If the matrix does not have CV_64F type it will be\n.   converted to the inner matrix of such type for the further computing.\n.   @param covs0 The vector of initial covariance matrices \\f$S_k\\f$ of mixture components. Each of\n.   covariance matrices is a one-channel matrix of \\f$dims \\times dims\\f$ size. If the matrices\n.   do not have CV_64F type they will be converted to the inner matrices of such type for the\n.   further computing.\n.   @param weights0 Initial weights \\f$\\pi_k\\f$ of mixture components. It should be a one-channel\n.   floating-point matrix with \\f$1 \\times nclusters\\f$ or \\f$nclusters \\times 1\\f$ size.\n.   @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n.   each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n.   @param labels The optional output \"class label\" for each sample:\n.   \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n.   mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n.   @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n.   mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n.   CV_64FC1 type."},
    {"trainEM", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_trainEM, 0), "trainEM(samples[, logLikelihoods[, labels[, probs]]]) -> retval, logLikelihoods, labels, probs\n.   @brief Estimate the Gaussian mixture parameters from a samples set.\n.   \n.   This variation starts with Expectation step. Initial values of the model parameters will be\n.   estimated by the k-means algorithm.\n.   \n.   Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take\n.   responses (class labels or function values) as input. Instead, it computes the *Maximum\n.   Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the\n.   parameters inside the structure: \\f$p_{i,k}\\f$ in probs, \\f$a_k\\f$ in means , \\f$S_k\\f$ in\n.   covs[k], \\f$\\pi_k\\f$ in weights , and optionally computes the output \"class label\" for each\n.   sample: \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most\n.   probable mixture component for each sample).\n.   \n.   The trained model can be used further for prediction, just like any other classifier. The\n.   trained model is similar to the NormalBayesClassifier.\n.   \n.   @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n.   one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n.   it will be converted to the inner matrix of such type for the further computing.\n.   @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n.   each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n.   @param labels The optional output \"class label\" for each sample:\n.   \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n.   mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n.   @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n.   mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n.   CV_64FC1 type."},
    {"trainM", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_EM_trainM, 0), "trainM(samples, probs0[, logLikelihoods[, labels[, probs]]]) -> retval, logLikelihoods, labels, probs\n.   @brief Estimate the Gaussian mixture parameters from a samples set.\n.   \n.   This variation starts with Maximization step. You need to provide initial probabilities\n.   \\f$p_{i,k}\\f$ to use this option.\n.   \n.   @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n.   one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n.   it will be converted to the inner matrix of such type for the further computing.\n.   @param probs0\n.   @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n.   each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n.   @param labels The optional output \"class label\" for each sample:\n.   \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n.   mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n.   @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n.   mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n.   CV_64FC1 type."},

    {NULL,          NULL}
};

static void pyopencv_ml_EM_specials(void)
{
    pyopencv_ml_EM_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_EM_Type.tp_dealloc = pyopencv_ml_EM_dealloc;
    pyopencv_ml_EM_Type.tp_repr = pyopencv_ml_EM_repr;
    pyopencv_ml_EM_Type.tp_getset = pyopencv_ml_EM_getseters;
    pyopencv_ml_EM_Type.tp_init = (initproc)0;
    pyopencv_ml_EM_Type.tp_methods = pyopencv_ml_EM_methods;
}

static PyObject* pyopencv_ml_DTrees_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_DTrees %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_DTrees_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_DTrees_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<DTrees> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::DTrees::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getCVFolds(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCVFolds());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getMaxCategories(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxCategories());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getMinSampleCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinSampleCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getPriors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPriors());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getRegressionAccuracy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRegressionAccuracy());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getTruncatePrunedTree(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTruncatePrunedTree());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getUse1SERule(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUse1SERule());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_getUseSurrogates(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseSurrogates());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<DTrees> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_DTrees.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::DTrees::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setCVFolds(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_DTrees.setCVFolds", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setCVFolds(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setMaxCategories(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_DTrees.setMaxCategories", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxCategories(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_DTrees.setMaxDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setMinSampleCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_DTrees.setMinSampleCount", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMinSampleCount(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setPriors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_DTrees.setPriors", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setPriors(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_DTrees.setPriors", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setPriors(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setRegressionAccuracy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ml_DTrees.setRegressionAccuracy", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRegressionAccuracy(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setTruncatePrunedTree(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ml_DTrees.setTruncatePrunedTree", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTruncatePrunedTree(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setUse1SERule(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ml_DTrees.setUse1SERule", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setUse1SERule(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_DTrees_setUseSurrogates(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::DTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_DTrees_Type))
        _self_ = dynamic_cast<cv::ml::DTrees*>(((pyopencv_ml_DTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_DTrees' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ml_DTrees.setUseSurrogates", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setUseSurrogates(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_DTrees_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_create_cls, METH_CLASS), "create() -> retval\n.   @brief Creates the empty model\n.   \n.   The static method creates empty decision tree with the specified parameters. It should be then\n.   trained using train method (see StatModel::train). Alternatively, you can load the model from\n.   file using Algorithm::load\\<DTrees\\>(filename)."},
    {"getCVFolds", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getCVFolds, 0), "getCVFolds() -> retval\n.   @see setCVFolds"},
    {"getMaxCategories", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getMaxCategories, 0), "getMaxCategories() -> retval\n.   @see setMaxCategories"},
    {"getMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getMaxDepth, 0), "getMaxDepth() -> retval\n.   @see setMaxDepth"},
    {"getMinSampleCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getMinSampleCount, 0), "getMinSampleCount() -> retval\n.   @see setMinSampleCount"},
    {"getPriors", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getPriors, 0), "getPriors() -> retval\n.   @see setPriors"},
    {"getRegressionAccuracy", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getRegressionAccuracy, 0), "getRegressionAccuracy() -> retval\n.   @see setRegressionAccuracy"},
    {"getTruncatePrunedTree", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getTruncatePrunedTree, 0), "getTruncatePrunedTree() -> retval\n.   @see setTruncatePrunedTree"},
    {"getUse1SERule", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getUse1SERule, 0), "getUse1SERule() -> retval\n.   @see setUse1SERule"},
    {"getUseSurrogates", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_getUseSurrogates, 0), "getUseSurrogates() -> retval\n.   @see setUseSurrogates"},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized DTrees from a file\n.   *\n.   * Use DTree::save to serialize and store an DTree to disk.\n.   * Load the DTree from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized DTree\n.   * @param nodeName name of node containing the classifier"},
    {"setCVFolds", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setCVFolds, 0), "setCVFolds(val) -> None\n.   @copybrief getCVFolds @see getCVFolds"},
    {"setMaxCategories", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setMaxCategories, 0), "setMaxCategories(val) -> None\n.   @copybrief getMaxCategories @see getMaxCategories"},
    {"setMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setMaxDepth, 0), "setMaxDepth(val) -> None\n.   @copybrief getMaxDepth @see getMaxDepth"},
    {"setMinSampleCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setMinSampleCount, 0), "setMinSampleCount(val) -> None\n.   @copybrief getMinSampleCount @see getMinSampleCount"},
    {"setPriors", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setPriors, 0), "setPriors(val) -> None\n.   @copybrief getPriors @see getPriors"},
    {"setRegressionAccuracy", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setRegressionAccuracy, 0), "setRegressionAccuracy(val) -> None\n.   @copybrief getRegressionAccuracy @see getRegressionAccuracy"},
    {"setTruncatePrunedTree", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setTruncatePrunedTree, 0), "setTruncatePrunedTree(val) -> None\n.   @copybrief getTruncatePrunedTree @see getTruncatePrunedTree"},
    {"setUse1SERule", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setUse1SERule, 0), "setUse1SERule(val) -> None\n.   @copybrief getUse1SERule @see getUse1SERule"},
    {"setUseSurrogates", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_DTrees_setUseSurrogates, 0), "setUseSurrogates(val) -> None\n.   @copybrief getUseSurrogates @see getUseSurrogates"},

    {NULL,          NULL}
};

static void pyopencv_ml_DTrees_specials(void)
{
    pyopencv_ml_DTrees_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_DTrees_Type.tp_dealloc = pyopencv_ml_DTrees_dealloc;
    pyopencv_ml_DTrees_Type.tp_repr = pyopencv_ml_DTrees_repr;
    pyopencv_ml_DTrees_Type.tp_getset = pyopencv_ml_DTrees_getseters;
    pyopencv_ml_DTrees_Type.tp_init = (initproc)0;
    pyopencv_ml_DTrees_Type.tp_methods = pyopencv_ml_DTrees_methods;
}

static PyObject* pyopencv_ml_RTrees_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_RTrees %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_RTrees_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_RTrees_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<RTrees> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::RTrees::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_getActiveVarCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getActiveVarCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_getCalculateVarImportance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCalculateVarImportance());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_getVarImportance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarImportance());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_getVotes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_results = NULL;
    Mat results;
    int flags=0;

    const char* keywords[] = { "samples", "flags", "results", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|O:ml_RTrees.getVotes", (char**)keywords, &pyobj_samples, &flags, &pyobj_results) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(_self_->getVotes(samples, results, flags));
        return pyopencv_from(results);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_results = NULL;
    UMat results;
    int flags=0;

    const char* keywords[] = { "samples", "flags", "results", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|O:ml_RTrees.getVotes", (char**)keywords, &pyobj_samples, &flags, &pyobj_results) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(_self_->getVotes(samples, results, flags));
        return pyopencv_from(results);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<RTrees> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_RTrees.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::RTrees::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_setActiveVarCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_RTrees.setActiveVarCount", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setActiveVarCount(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_setCalculateVarImportance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ml_RTrees.setCalculateVarImportance", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setCalculateVarImportance(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_RTrees_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::RTrees* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_RTrees_Type))
        _self_ = dynamic_cast<cv::ml::RTrees*>(((pyopencv_ml_RTrees_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_RTrees' or its derivative)");
    PyObject* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_RTrees.setTermCriteria", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_RTrees_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_create_cls, METH_CLASS), "create() -> retval\n.   Creates the empty model.\n.   Use StatModel::train to train the model, StatModel::train to create and train the model,\n.   Algorithm::load to load the pre-trained model."},
    {"getActiveVarCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getActiveVarCount, 0), "getActiveVarCount() -> retval\n.   @see setActiveVarCount"},
    {"getCalculateVarImportance", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getCalculateVarImportance, 0), "getCalculateVarImportance() -> retval\n.   @see setCalculateVarImportance"},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getVarImportance", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getVarImportance, 0), "getVarImportance() -> retval\n.   Returns the variable importance array.\n.   The method returns the variable importance vector, computed at the training stage when\n.   CalculateVarImportance is set to true. If this flag was set to false, the empty matrix is\n.   returned."},
    {"getVotes", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_getVotes, 0), "getVotes(samples, flags[, results]) -> results\n.   Returns the result of each individual tree in the forest.\n.   In case the model is a regression problem, the method will return each of the trees'\n.   results for each of the sample cases. If the model is a classifier, it will return\n.   a Mat with samples + 1 rows, where the first row gives the class number and the\n.   following rows return the votes each class had for each sample.\n.   @param samples Array containing the samples for which votes will be calculated.\n.   @param results Array where the result of the calculation will be written.\n.   @param flags Flags for defining the type of RTrees."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized RTree from a file\n.   *\n.   * Use RTree::save to serialize and store an RTree to disk.\n.   * Load the RTree from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized RTree\n.   * @param nodeName name of node containing the classifier"},
    {"setActiveVarCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_setActiveVarCount, 0), "setActiveVarCount(val) -> None\n.   @copybrief getActiveVarCount @see getActiveVarCount"},
    {"setCalculateVarImportance", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_setCalculateVarImportance, 0), "setCalculateVarImportance(val) -> None\n.   @copybrief getCalculateVarImportance @see getCalculateVarImportance"},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_RTrees_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},

    {NULL,          NULL}
};

static void pyopencv_ml_RTrees_specials(void)
{
    pyopencv_ml_RTrees_Type.tp_base = &pyopencv_ml_DTrees_Type;
    pyopencv_ml_RTrees_Type.tp_dealloc = pyopencv_ml_RTrees_dealloc;
    pyopencv_ml_RTrees_Type.tp_repr = pyopencv_ml_RTrees_repr;
    pyopencv_ml_RTrees_Type.tp_getset = pyopencv_ml_RTrees_getseters;
    pyopencv_ml_RTrees_Type.tp_init = (initproc)0;
    pyopencv_ml_RTrees_Type.tp_methods = pyopencv_ml_RTrees_methods;
}

static PyObject* pyopencv_ml_Boost_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_Boost %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_Boost_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_Boost_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<Boost> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::Boost::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_getBoostType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::Boost* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_Boost_Type))
        _self_ = dynamic_cast<cv::ml::Boost*>(((pyopencv_ml_Boost_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBoostType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_getWeakCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::Boost* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_Boost_Type))
        _self_ = dynamic_cast<cv::ml::Boost*>(((pyopencv_ml_Boost_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeakCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_getWeightTrimRate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::Boost* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_Boost_Type))
        _self_ = dynamic_cast<cv::ml::Boost*>(((pyopencv_ml_Boost_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightTrimRate());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<Boost> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_Boost.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::Boost::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_setBoostType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::Boost* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_Boost_Type))
        _self_ = dynamic_cast<cv::ml::Boost*>(((pyopencv_ml_Boost_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_Boost.setBoostType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setBoostType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_setWeakCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::Boost* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_Boost_Type))
        _self_ = dynamic_cast<cv::ml::Boost*>(((pyopencv_ml_Boost_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_Boost.setWeakCount", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setWeakCount(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_Boost_setWeightTrimRate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::Boost* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_Boost_Type))
        _self_ = dynamic_cast<cv::ml::Boost*>(((pyopencv_ml_Boost_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_Boost' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_Boost.setWeightTrimRate", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setWeightTrimRate(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_Boost_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_create_cls, METH_CLASS), "create() -> retval\n.   Creates the empty model.\n.   Use StatModel::train to train the model, Algorithm::load\\<Boost\\>(filename) to load the pre-trained model."},
    {"getBoostType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_getBoostType, 0), "getBoostType() -> retval\n.   @see setBoostType"},
    {"getWeakCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_getWeakCount, 0), "getWeakCount() -> retval\n.   @see setWeakCount"},
    {"getWeightTrimRate", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_getWeightTrimRate, 0), "getWeightTrimRate() -> retval\n.   @see setWeightTrimRate"},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized Boost from a file\n.   *\n.   * Use Boost::save to serialize and store an RTree to disk.\n.   * Load the Boost from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized Boost\n.   * @param nodeName name of node containing the classifier"},
    {"setBoostType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_setBoostType, 0), "setBoostType(val) -> None\n.   @copybrief getBoostType @see getBoostType"},
    {"setWeakCount", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_setWeakCount, 0), "setWeakCount(val) -> None\n.   @copybrief getWeakCount @see getWeakCount"},
    {"setWeightTrimRate", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_Boost_setWeightTrimRate, 0), "setWeightTrimRate(val) -> None\n.   @copybrief getWeightTrimRate @see getWeightTrimRate"},

    {NULL,          NULL}
};

static void pyopencv_ml_Boost_specials(void)
{
    pyopencv_ml_Boost_Type.tp_base = &pyopencv_ml_DTrees_Type;
    pyopencv_ml_Boost_Type.tp_dealloc = pyopencv_ml_Boost_dealloc;
    pyopencv_ml_Boost_Type.tp_repr = pyopencv_ml_Boost_repr;
    pyopencv_ml_Boost_Type.tp_getset = pyopencv_ml_Boost_getseters;
    pyopencv_ml_Boost_Type.tp_init = (initproc)0;
    pyopencv_ml_Boost_Type.tp_methods = pyopencv_ml_Boost_methods;
}

static PyObject* pyopencv_ml_ANN_MLP_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_ANN_MLP %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_ANN_MLP_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<ANN_MLP> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::ANN_MLP::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getAnnealCoolingRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealCoolingRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getAnnealFinalT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealFinalT());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getAnnealInitialT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealInitialT());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getAnnealItePerStep(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealItePerStep());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getBackpropMomentumScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBackpropMomentumScale());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getBackpropWeightScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBackpropWeightScale());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getLayerSizes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLayerSizes());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getRpropDW0(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRpropDW0());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRpropDWMax());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRpropDWMin());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMinus(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRpropDWMinus());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getRpropDWPlus(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRpropDWPlus());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getTrainMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainMethod());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_getWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    int layerIdx=0;
    Mat retval;

    const char* keywords[] = { "layerIdx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_ANN_MLP.getWeights", (char**)keywords, &layerIdx) )
    {
        ERRWRAP2(retval = _self_->getWeights(layerIdx));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    Ptr<ANN_MLP> retval;

    const char* keywords[] = { "filepath", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_ANN_MLP.load", (char**)keywords, &pyobj_filepath) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) )
    {
        ERRWRAP2(retval = cv::ml::ANN_MLP::load(filepath));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setActivationFunction(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    int type=0;
    double param1=0;
    double param2=0;

    const char* keywords[] = { "type", "param1", "param2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|dd:ml_ANN_MLP.setActivationFunction", (char**)keywords, &type, &param1, &param2) )
    {
        ERRWRAP2(_self_->setActivationFunction(type, param1, param2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setAnnealCoolingRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setAnnealCoolingRatio", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealCoolingRatio(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setAnnealFinalT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setAnnealFinalT", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealFinalT(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setAnnealInitialT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setAnnealInitialT", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealInitialT(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setAnnealItePerStep(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_ANN_MLP.setAnnealItePerStep", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealItePerStep(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setBackpropMomentumScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setBackpropMomentumScale", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setBackpropMomentumScale(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setBackpropWeightScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setBackpropWeightScale", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setBackpropWeightScale(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setLayerSizes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    {
    PyObject* pyobj__layer_sizes = NULL;
    Mat _layer_sizes;

    const char* keywords[] = { "_layer_sizes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_ANN_MLP.setLayerSizes", (char**)keywords, &pyobj__layer_sizes) &&
        pyopencv_to(pyobj__layer_sizes, _layer_sizes, ArgInfo("_layer_sizes", 0)) )
    {
        ERRWRAP2(_self_->setLayerSizes(_layer_sizes));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__layer_sizes = NULL;
    UMat _layer_sizes;

    const char* keywords[] = { "_layer_sizes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_ANN_MLP.setLayerSizes", (char**)keywords, &pyobj__layer_sizes) &&
        pyopencv_to(pyobj__layer_sizes, _layer_sizes, ArgInfo("_layer_sizes", 0)) )
    {
        ERRWRAP2(_self_->setLayerSizes(_layer_sizes));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setRpropDW0(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setRpropDW0", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRpropDW0(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setRpropDWMax", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRpropDWMax(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setRpropDWMin", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRpropDWMin(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMinus(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setRpropDWMinus", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRpropDWMinus(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setRpropDWPlus(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP.setRpropDWPlus", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRpropDWPlus(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    PyObject* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_ANN_MLP.setTermCriteria", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_setTrainMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP*>(((pyopencv_ml_ANN_MLP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP' or its derivative)");
    int method=0;
    double param1=0;
    double param2=0;

    const char* keywords[] = { "method", "param1", "param2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|dd:ml_ANN_MLP.setTrainMethod", (char**)keywords, &method, &param1, &param2) )
    {
        ERRWRAP2(_self_->setTrainMethod(method, param1, param2));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_ANN_MLP_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_create_cls, METH_CLASS), "create() -> retval\n.   @brief Creates empty model\n.   \n.   Use StatModel::train to train the model, Algorithm::load\\<ANN_MLP\\>(filename) to load the pre-trained model.\n.   Note that the train method has optional flags: ANN_MLP::TrainFlags."},
    {"getAnnealCoolingRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealCoolingRatio, 0), "getAnnealCoolingRatio() -> retval\n.   @see setAnnealCoolingRatio"},
    {"getAnnealFinalT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealFinalT, 0), "getAnnealFinalT() -> retval\n.   @see setAnnealFinalT"},
    {"getAnnealInitialT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealInitialT, 0), "getAnnealInitialT() -> retval\n.   @see setAnnealInitialT"},
    {"getAnnealItePerStep", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getAnnealItePerStep, 0), "getAnnealItePerStep() -> retval\n.   @see setAnnealItePerStep"},
    {"getBackpropMomentumScale", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getBackpropMomentumScale, 0), "getBackpropMomentumScale() -> retval\n.   @see setBackpropMomentumScale"},
    {"getBackpropWeightScale", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getBackpropWeightScale, 0), "getBackpropWeightScale() -> retval\n.   @see setBackpropWeightScale"},
    {"getLayerSizes", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getLayerSizes, 0), "getLayerSizes() -> retval\n.   Integer vector specifying the number of neurons in each layer including the input and output layers.\n.   The very first element specifies the number of elements in the input layer.\n.   The last element - number of elements in the output layer.\n.   @sa setLayerSizes"},
    {"getRpropDW0", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDW0, 0), "getRpropDW0() -> retval\n.   @see setRpropDW0"},
    {"getRpropDWMax", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMax, 0), "getRpropDWMax() -> retval\n.   @see setRpropDWMax"},
    {"getRpropDWMin", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMin, 0), "getRpropDWMin() -> retval\n.   @see setRpropDWMin"},
    {"getRpropDWMinus", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWMinus, 0), "getRpropDWMinus() -> retval\n.   @see setRpropDWMinus"},
    {"getRpropDWPlus", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getRpropDWPlus, 0), "getRpropDWPlus() -> retval\n.   @see setRpropDWPlus"},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getTrainMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getTrainMethod, 0), "getTrainMethod() -> retval\n.   Returns current training method"},
    {"getWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_getWeights, 0), "getWeights(layerIdx) -> retval\n."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_load_cls, METH_CLASS), "load(filepath) -> retval\n.   @brief Loads and creates a serialized ANN from a file\n.   *\n.   * Use ANN::save to serialize and store an ANN to disk.\n.   * Load the ANN from this file again, by calling this function with the path to the file.\n.   *\n.   * @param filepath path to serialized ANN"},
    {"setActivationFunction", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setActivationFunction, 0), "setActivationFunction(type[, param1[, param2]]) -> None\n.   Initialize the activation function for each neuron.\n.   Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.\n.   @param type The type of activation function. See ANN_MLP::ActivationFunctions.\n.   @param param1 The first parameter of the activation function, \\f$\\alpha\\f$. Default value is 0.\n.   @param param2 The second parameter of the activation function, \\f$\\beta\\f$. Default value is 0."},
    {"setAnnealCoolingRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealCoolingRatio, 0), "setAnnealCoolingRatio(val) -> None\n.   @copybrief getAnnealCoolingRatio @see getAnnealCoolingRatio"},
    {"setAnnealFinalT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealFinalT, 0), "setAnnealFinalT(val) -> None\n.   @copybrief getAnnealFinalT @see getAnnealFinalT"},
    {"setAnnealInitialT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealInitialT, 0), "setAnnealInitialT(val) -> None\n.   @copybrief getAnnealInitialT @see getAnnealInitialT"},
    {"setAnnealItePerStep", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setAnnealItePerStep, 0), "setAnnealItePerStep(val) -> None\n.   @copybrief getAnnealItePerStep @see getAnnealItePerStep"},
    {"setBackpropMomentumScale", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setBackpropMomentumScale, 0), "setBackpropMomentumScale(val) -> None\n.   @copybrief getBackpropMomentumScale @see getBackpropMomentumScale"},
    {"setBackpropWeightScale", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setBackpropWeightScale, 0), "setBackpropWeightScale(val) -> None\n.   @copybrief getBackpropWeightScale @see getBackpropWeightScale"},
    {"setLayerSizes", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setLayerSizes, 0), "setLayerSizes(_layer_sizes) -> None\n.   Integer vector specifying the number of neurons in each layer including the input and output layers.\n.   The very first element specifies the number of elements in the input layer.\n.   The last element - number of elements in the output layer. Default value is empty Mat.\n.   @sa getLayerSizes"},
    {"setRpropDW0", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDW0, 0), "setRpropDW0(val) -> None\n.   @copybrief getRpropDW0 @see getRpropDW0"},
    {"setRpropDWMax", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMax, 0), "setRpropDWMax(val) -> None\n.   @copybrief getRpropDWMax @see getRpropDWMax"},
    {"setRpropDWMin", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMin, 0), "setRpropDWMin(val) -> None\n.   @copybrief getRpropDWMin @see getRpropDWMin"},
    {"setRpropDWMinus", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWMinus, 0), "setRpropDWMinus(val) -> None\n.   @copybrief getRpropDWMinus @see getRpropDWMinus"},
    {"setRpropDWPlus", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setRpropDWPlus, 0), "setRpropDWPlus(val) -> None\n.   @copybrief getRpropDWPlus @see getRpropDWPlus"},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"setTrainMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_setTrainMethod, 0), "setTrainMethod(method[, param1[, param2]]) -> None\n.   Sets training method and common parameters.\n.   @param method Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.\n.   @param param1 passed to setRpropDW0 for ANN_MLP::RPROP and to setBackpropWeightScale for ANN_MLP::BACKPROP and to initialT for ANN_MLP::ANNEAL.\n.   @param param2 passed to setRpropDWMin for ANN_MLP::RPROP and to setBackpropMomentumScale for ANN_MLP::BACKPROP and to finalT for ANN_MLP::ANNEAL."},

    {NULL,          NULL}
};

static void pyopencv_ml_ANN_MLP_specials(void)
{
    pyopencv_ml_ANN_MLP_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_ANN_MLP_Type.tp_dealloc = pyopencv_ml_ANN_MLP_dealloc;
    pyopencv_ml_ANN_MLP_Type.tp_repr = pyopencv_ml_ANN_MLP_repr;
    pyopencv_ml_ANN_MLP_Type.tp_getset = pyopencv_ml_ANN_MLP_getseters;
    pyopencv_ml_ANN_MLP_Type.tp_init = (initproc)0;
    pyopencv_ml_ANN_MLP_Type.tp_methods = pyopencv_ml_ANN_MLP_methods;
}

static PyObject* pyopencv_ml_LogisticRegression_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_LogisticRegression %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_LogisticRegression_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<LogisticRegression> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::LogisticRegression::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_getIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_getLearningRate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLearningRate());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_getMiniBatchSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMiniBatchSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_getRegularization(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRegularization());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_getTrainMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainMethod());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_get_learnt_thetas(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->get_learnt_thetas());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<LogisticRegression> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_LogisticRegression.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::LogisticRegression::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_predict(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    {
    PyObject* pyobj_samples = NULL;
    Mat samples;
    PyObject* pyobj_results = NULL;
    Mat results;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ml_LogisticRegression.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &flags) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(results));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_samples = NULL;
    UMat samples;
    PyObject* pyobj_results = NULL;
    UMat results;
    int flags=0;
    float retval;

    const char* keywords[] = { "samples", "results", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ml_LogisticRegression.predict", (char**)keywords, &pyobj_samples, &pyobj_results, &flags) &&
        pyopencv_to(pyobj_samples, samples, ArgInfo("samples", 0)) &&
        pyopencv_to(pyobj_results, results, ArgInfo("results", 1)) )
    {
        ERRWRAP2(retval = _self_->predict(samples, results, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(results));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_setIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_LogisticRegression.setIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_setLearningRate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_LogisticRegression.setLearningRate", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setLearningRate(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_setMiniBatchSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_LogisticRegression.setMiniBatchSize", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMiniBatchSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_setRegularization(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_LogisticRegression.setRegularization", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRegularization(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    PyObject* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_LogisticRegression.setTermCriteria", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_LogisticRegression_setTrainMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::LogisticRegression* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_LogisticRegression_Type))
        _self_ = dynamic_cast<cv::ml::LogisticRegression*>(((pyopencv_ml_LogisticRegression_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_LogisticRegression' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_LogisticRegression.setTrainMethod", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTrainMethod(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_LogisticRegression_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_create_cls, METH_CLASS), "create() -> retval\n.   @brief Creates empty model.\n.   \n.   Creates Logistic Regression model with parameters given."},
    {"getIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getIterations, 0), "getIterations() -> retval\n.   @see setIterations"},
    {"getLearningRate", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getLearningRate, 0), "getLearningRate() -> retval\n.   @see setLearningRate"},
    {"getMiniBatchSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getMiniBatchSize, 0), "getMiniBatchSize() -> retval\n.   @see setMiniBatchSize"},
    {"getRegularization", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getRegularization, 0), "getRegularization() -> retval\n.   @see setRegularization"},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getTrainMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_getTrainMethod, 0), "getTrainMethod() -> retval\n.   @see setTrainMethod"},
    {"get_learnt_thetas", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_get_learnt_thetas, 0), "get_learnt_thetas() -> retval\n.   @brief This function returns the trained parameters arranged across rows.\n.   \n.   For a two class classifcation problem, it returns a row matrix. It returns learnt parameters of\n.   the Logistic Regression as a matrix of type CV_32F."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized LogisticRegression from a file\n.   *\n.   * Use LogisticRegression::save to serialize and store an LogisticRegression to disk.\n.   * Load the LogisticRegression from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized LogisticRegression\n.   * @param nodeName name of node containing the classifier"},
    {"predict", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_predict, 0), "predict(samples[, results[, flags]]) -> retval, results\n.   @brief Predicts responses for input samples and returns a float type.\n.   \n.   @param samples The input data for the prediction algorithm. Matrix [m x n], where each row\n.   contains variables (features) of one object being classified. Should have data type CV_32F.\n.   @param results Predicted labels as a column matrix of type CV_32S.\n.   @param flags Not used."},
    {"setIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setIterations, 0), "setIterations(val) -> None\n.   @copybrief getIterations @see getIterations"},
    {"setLearningRate", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setLearningRate, 0), "setLearningRate(val) -> None\n.   @copybrief getLearningRate @see getLearningRate"},
    {"setMiniBatchSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setMiniBatchSize, 0), "setMiniBatchSize(val) -> None\n.   @copybrief getMiniBatchSize @see getMiniBatchSize"},
    {"setRegularization", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setRegularization, 0), "setRegularization(val) -> None\n.   @copybrief getRegularization @see getRegularization"},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},
    {"setTrainMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_LogisticRegression_setTrainMethod, 0), "setTrainMethod(val) -> None\n.   @copybrief getTrainMethod @see getTrainMethod"},

    {NULL,          NULL}
};

static void pyopencv_ml_LogisticRegression_specials(void)
{
    pyopencv_ml_LogisticRegression_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_LogisticRegression_Type.tp_dealloc = pyopencv_ml_LogisticRegression_dealloc;
    pyopencv_ml_LogisticRegression_Type.tp_repr = pyopencv_ml_LogisticRegression_repr;
    pyopencv_ml_LogisticRegression_Type.tp_getset = pyopencv_ml_LogisticRegression_getseters;
    pyopencv_ml_LogisticRegression_Type.tp_init = (initproc)0;
    pyopencv_ml_LogisticRegression_Type.tp_methods = pyopencv_ml_LogisticRegression_methods;
}

static PyObject* pyopencv_ml_SVMSGD_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_SVMSGD %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_SVMSGD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_SVMSGD_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    Ptr<SVMSGD> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ml::SVMSGD::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getInitialStepSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInitialStepSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getMarginRegularization(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMarginRegularization());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getMarginType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMarginType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getShift(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getShift());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getStepDecreasingPower(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getStepDecreasingPower());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getSvmsgdType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSvmsgdType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_getWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeights());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    PyObject* pyobj_filepath = NULL;
    String filepath;
    PyObject* pyobj_nodeName = NULL;
    String nodeName;
    Ptr<SVMSGD> retval;

    const char* keywords[] = { "filepath", "nodeName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ml_SVMSGD.load", (char**)keywords, &pyobj_filepath, &pyobj_nodeName) &&
        pyopencv_to(pyobj_filepath, filepath, ArgInfo("filepath", 0)) &&
        pyopencv_to(pyobj_nodeName, nodeName, ArgInfo("nodeName", 0)) )
    {
        ERRWRAP2(retval = cv::ml::SVMSGD::load(filepath, nodeName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setInitialStepSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float InitialStepSize=0.f;

    const char* keywords[] = { "InitialStepSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ml_SVMSGD.setInitialStepSize", (char**)keywords, &InitialStepSize) )
    {
        ERRWRAP2(_self_->setInitialStepSize(InitialStepSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setMarginRegularization(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float marginRegularization=0.f;

    const char* keywords[] = { "marginRegularization", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ml_SVMSGD.setMarginRegularization", (char**)keywords, &marginRegularization) )
    {
        ERRWRAP2(_self_->setMarginRegularization(marginRegularization));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setMarginType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    int marginType=0;

    const char* keywords[] = { "marginType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_SVMSGD.setMarginType", (char**)keywords, &marginType) )
    {
        ERRWRAP2(_self_->setMarginType(marginType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setOptimalParameters(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    int svmsgdType=SVMSGD::ASGD;
    int marginType=SVMSGD::SOFT_MARGIN;

    const char* keywords[] = { "svmsgdType", "marginType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ii:ml_SVMSGD.setOptimalParameters", (char**)keywords, &svmsgdType, &marginType) )
    {
        ERRWRAP2(_self_->setOptimalParameters(svmsgdType, marginType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setStepDecreasingPower(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    float stepDecreasingPower=0.f;

    const char* keywords[] = { "stepDecreasingPower", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ml_SVMSGD.setStepDecreasingPower", (char**)keywords, &stepDecreasingPower) )
    {
        ERRWRAP2(_self_->setStepDecreasingPower(stepDecreasingPower));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setSvmsgdType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    int svmsgdType=0;

    const char* keywords[] = { "svmsgdType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_SVMSGD.setSvmsgdType", (char**)keywords, &svmsgdType) )
    {
        ERRWRAP2(_self_->setSvmsgdType(svmsgdType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_SVMSGD_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::SVMSGD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_SVMSGD_Type))
        _self_ = dynamic_cast<cv::ml::SVMSGD*>(((pyopencv_ml_SVMSGD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_SVMSGD' or its derivative)");
    PyObject* pyobj_val = NULL;
    TermCriteria val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ml_SVMSGD.setTermCriteria", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_SVMSGD_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_create_cls, METH_CLASS), "create() -> retval\n.   @brief Creates empty model.\n.   * Use StatModel::train to train the model. Since %SVMSGD has several parameters, you may want to\n.   * find the best parameters for your problem or use setOptimalParameters() to set some default parameters."},
    {"getInitialStepSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getInitialStepSize, 0), "getInitialStepSize() -> retval\n.   @see setInitialStepSize"},
    {"getMarginRegularization", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getMarginRegularization, 0), "getMarginRegularization() -> retval\n.   @see setMarginRegularization"},
    {"getMarginType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getMarginType, 0), "getMarginType() -> retval\n.   @see setMarginType"},
    {"getShift", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getShift, 0), "getShift() -> retval\n.   * @return the shift of the trained model (decision function f(x) = weights * x + shift)."},
    {"getStepDecreasingPower", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getStepDecreasingPower, 0), "getStepDecreasingPower() -> retval\n.   @see setStepDecreasingPower"},
    {"getSvmsgdType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getSvmsgdType, 0), "getSvmsgdType() -> retval\n.   @see setSvmsgdType"},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getTermCriteria, 0), "getTermCriteria() -> retval\n.   @see setTermCriteria"},
    {"getWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_getWeights, 0), "getWeights() -> retval\n.   * @return the weights of the trained model (decision function f(x) = weights * x + shift)."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_load_cls, METH_CLASS), "load(filepath[, nodeName]) -> retval\n.   @brief Loads and creates a serialized SVMSGD from a file\n.   *\n.   * Use SVMSGD::save to serialize and store an SVMSGD to disk.\n.   * Load the SVMSGD from this file again, by calling this function with the path to the file.\n.   * Optionally specify the node for the file containing the classifier\n.   *\n.   * @param filepath path to serialized SVMSGD\n.   * @param nodeName name of node containing the classifier"},
    {"setInitialStepSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setInitialStepSize, 0), "setInitialStepSize(InitialStepSize) -> None\n.   @copybrief getInitialStepSize @see getInitialStepSize"},
    {"setMarginRegularization", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setMarginRegularization, 0), "setMarginRegularization(marginRegularization) -> None\n.   @copybrief getMarginRegularization @see getMarginRegularization"},
    {"setMarginType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setMarginType, 0), "setMarginType(marginType) -> None\n.   @copybrief getMarginType @see getMarginType"},
    {"setOptimalParameters", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setOptimalParameters, 0), "setOptimalParameters([, svmsgdType[, marginType]]) -> None\n.   @brief Function sets optimal parameters values for chosen SVM SGD model.\n.   * @param svmsgdType is the type of SVMSGD classifier.\n.   * @param marginType is the type of margin constraint."},
    {"setStepDecreasingPower", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setStepDecreasingPower, 0), "setStepDecreasingPower(stepDecreasingPower) -> None\n.   @copybrief getStepDecreasingPower @see getStepDecreasingPower"},
    {"setSvmsgdType", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setSvmsgdType, 0), "setSvmsgdType(svmsgdType) -> None\n.   @copybrief getSvmsgdType @see getSvmsgdType"},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_SVMSGD_setTermCriteria, 0), "setTermCriteria(val) -> None\n.   @copybrief getTermCriteria @see getTermCriteria"},

    {NULL,          NULL}
};

static void pyopencv_ml_SVMSGD_specials(void)
{
    pyopencv_ml_SVMSGD_Type.tp_base = &pyopencv_ml_StatModel_Type;
    pyopencv_ml_SVMSGD_Type.tp_dealloc = pyopencv_ml_SVMSGD_dealloc;
    pyopencv_ml_SVMSGD_Type.tp_repr = pyopencv_ml_SVMSGD_repr;
    pyopencv_ml_SVMSGD_Type.tp_getset = pyopencv_ml_SVMSGD_getseters;
    pyopencv_ml_SVMSGD_Type.tp_init = (initproc)0;
    pyopencv_ml_SVMSGD_Type.tp_methods = pyopencv_ml_SVMSGD_methods;
}

static PyObject* pyopencv_ml_ANN_MLP_ANNEAL_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ml_ANN_MLP_ANNEAL %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ml_ANN_MLP_ANNEAL_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealCoolingRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealCoolingRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealFinalT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealFinalT());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealInitialT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealInitialT());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealItePerStep(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAnnealItePerStep());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealCoolingRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP_ANNEAL.setAnnealCoolingRatio", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealCoolingRatio(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealFinalT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP_ANNEAL.setAnnealFinalT", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealFinalT(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealInitialT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ml_ANN_MLP_ANNEAL.setAnnealInitialT", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealInitialT(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealItePerStep(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ml;

    cv::ml::ANN_MLP_ANNEAL* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ml_ANN_MLP_ANNEAL_Type))
        _self_ = dynamic_cast<cv::ml::ANN_MLP_ANNEAL*>(((pyopencv_ml_ANN_MLP_ANNEAL_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ml_ANN_MLP_ANNEAL' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ml_ANN_MLP_ANNEAL.setAnnealItePerStep", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAnnealItePerStep(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ml_ANN_MLP_ANNEAL_methods[] =
{
    {"getAnnealCoolingRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealCoolingRatio, 0), "getAnnealCoolingRatio() -> retval\n.   @see setAnnealCoolingRatio"},
    {"getAnnealFinalT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealFinalT, 0), "getAnnealFinalT() -> retval\n.   @see setAnnealFinalT"},
    {"getAnnealInitialT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealInitialT, 0), "getAnnealInitialT() -> retval\n.   @see setAnnealInitialT"},
    {"getAnnealItePerStep", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_getAnnealItePerStep, 0), "getAnnealItePerStep() -> retval\n.   @see setAnnealItePerStep"},
    {"setAnnealCoolingRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealCoolingRatio, 0), "setAnnealCoolingRatio(val) -> None\n.   @copybrief getAnnealCoolingRatio @see getAnnealCoolingRatio"},
    {"setAnnealFinalT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealFinalT, 0), "setAnnealFinalT(val) -> None\n.   @copybrief getAnnealFinalT @see getAnnealFinalT"},
    {"setAnnealInitialT", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealInitialT, 0), "setAnnealInitialT(val) -> None\n.   @copybrief getAnnealInitialT @see getAnnealInitialT"},
    {"setAnnealItePerStep", CV_PY_FN_WITH_KW_(pyopencv_cv_ml_ml_ANN_MLP_ANNEAL_setAnnealItePerStep, 0), "setAnnealItePerStep(val) -> None\n.   @copybrief getAnnealItePerStep @see getAnnealItePerStep"},

    {NULL,          NULL}
};

static void pyopencv_ml_ANN_MLP_ANNEAL_specials(void)
{
    pyopencv_ml_ANN_MLP_ANNEAL_Type.tp_base = &pyopencv_ml_ANN_MLP_Type;
    pyopencv_ml_ANN_MLP_ANNEAL_Type.tp_dealloc = pyopencv_ml_ANN_MLP_ANNEAL_dealloc;
    pyopencv_ml_ANN_MLP_ANNEAL_Type.tp_repr = pyopencv_ml_ANN_MLP_ANNEAL_repr;
    pyopencv_ml_ANN_MLP_ANNEAL_Type.tp_getset = pyopencv_ml_ANN_MLP_ANNEAL_getseters;
    pyopencv_ml_ANN_MLP_ANNEAL_Type.tp_init = (initproc)0;
    pyopencv_ml_ANN_MLP_ANNEAL_Type.tp_methods = pyopencv_ml_ANN_MLP_ANNEAL_methods;
}

static PyObject* pyopencv_BaseCascadeClassifier_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BaseCascadeClassifier %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BaseCascadeClassifier_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_BaseCascadeClassifier_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_BaseCascadeClassifier_specials(void)
{
    pyopencv_BaseCascadeClassifier_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_BaseCascadeClassifier_Type.tp_dealloc = pyopencv_BaseCascadeClassifier_dealloc;
    pyopencv_BaseCascadeClassifier_Type.tp_repr = pyopencv_BaseCascadeClassifier_repr;
    pyopencv_BaseCascadeClassifier_Type.tp_getset = pyopencv_BaseCascadeClassifier_getseters;
    pyopencv_BaseCascadeClassifier_Type.tp_init = (initproc)0;
    pyopencv_BaseCascadeClassifier_Type.tp_methods = pyopencv_BaseCascadeClassifier_methods;
}

static PyObject* pyopencv_CascadeClassifier_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CascadeClassifier %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_CascadeClassifier_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_CascadeClassifier_CascadeClassifier(pyopencv_CascadeClassifier_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::CascadeClassifier>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::CascadeClassifier()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:CascadeClassifier", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        new (&(self->v)) Ptr<cv::CascadeClassifier>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::CascadeClassifier(filename)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_CascadeClassifier_convert_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_oldcascade = NULL;
    String oldcascade;
    PyObject* pyobj_newcascade = NULL;
    String newcascade;
    bool retval;

    const char* keywords[] = { "oldcascade", "newcascade", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:CascadeClassifier.convert", (char**)keywords, &pyobj_oldcascade, &pyobj_newcascade) &&
        pyopencv_to(pyobj_oldcascade, oldcascade, ArgInfo("oldcascade", 0)) &&
        pyopencv_to(pyobj_newcascade, newcascade, ArgInfo("newcascade", 0)) )
    {
        ERRWRAP2(retval = cv::CascadeClassifier::convert(oldcascade, newcascade));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_detectMultiScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_Rect objects;
    double scaleFactor=1.1;
    int minNeighbors=3;
    int flags=0;
    PyObject* pyobj_minSize = NULL;
    Size minSize;
    PyObject* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|diiOO:CascadeClassifier.detectMultiScale", (char**)keywords, &pyobj_image, &scaleFactor, &minNeighbors, &flags, &pyobj_minSize, &pyobj_maxSize) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        pyopencv_to(pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(image, objects, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return pyopencv_from(objects);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    vector_Rect objects;
    double scaleFactor=1.1;
    int minNeighbors=3;
    int flags=0;
    PyObject* pyobj_minSize = NULL;
    Size minSize;
    PyObject* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|diiOO:CascadeClassifier.detectMultiScale", (char**)keywords, &pyobj_image, &scaleFactor, &minNeighbors, &flags, &pyobj_minSize, &pyobj_maxSize) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        pyopencv_to(pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(image, objects, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return pyopencv_from(objects);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_detectMultiScale2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_Rect objects;
    vector_int numDetections;
    double scaleFactor=1.1;
    int minNeighbors=3;
    int flags=0;
    PyObject* pyobj_minSize = NULL;
    Size minSize;
    PyObject* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|diiOO:CascadeClassifier.detectMultiScale2", (char**)keywords, &pyobj_image, &scaleFactor, &minNeighbors, &flags, &pyobj_minSize, &pyobj_maxSize) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        pyopencv_to(pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(image, objects, numDetections, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return Py_BuildValue("(NN)", pyopencv_from(objects), pyopencv_from(numDetections));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    vector_Rect objects;
    vector_int numDetections;
    double scaleFactor=1.1;
    int minNeighbors=3;
    int flags=0;
    PyObject* pyobj_minSize = NULL;
    Size minSize;
    PyObject* pyobj_maxSize = NULL;
    Size maxSize;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|diiOO:CascadeClassifier.detectMultiScale2", (char**)keywords, &pyobj_image, &scaleFactor, &minNeighbors, &flags, &pyobj_minSize, &pyobj_maxSize) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        pyopencv_to(pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(image, objects, numDetections, scaleFactor, minNeighbors, flags, minSize, maxSize));
        return Py_BuildValue("(NN)", pyopencv_from(objects), pyopencv_from(numDetections));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_detectMultiScale3(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_Rect objects;
    vector_int rejectLevels;
    vector_double levelWeights;
    double scaleFactor=1.1;
    int minNeighbors=3;
    int flags=0;
    PyObject* pyobj_minSize = NULL;
    Size minSize;
    PyObject* pyobj_maxSize = NULL;
    Size maxSize;
    bool outputRejectLevels=false;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", "outputRejectLevels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|diiOOb:CascadeClassifier.detectMultiScale3", (char**)keywords, &pyobj_image, &scaleFactor, &minNeighbors, &flags, &pyobj_minSize, &pyobj_maxSize, &outputRejectLevels) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        pyopencv_to(pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(image, objects, rejectLevels, levelWeights, scaleFactor, minNeighbors, flags, minSize, maxSize, outputRejectLevels));
        return Py_BuildValue("(NNN)", pyopencv_from(objects), pyopencv_from(rejectLevels), pyopencv_from(levelWeights));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    vector_Rect objects;
    vector_int rejectLevels;
    vector_double levelWeights;
    double scaleFactor=1.1;
    int minNeighbors=3;
    int flags=0;
    PyObject* pyobj_minSize = NULL;
    Size minSize;
    PyObject* pyobj_maxSize = NULL;
    Size maxSize;
    bool outputRejectLevels=false;

    const char* keywords[] = { "image", "scaleFactor", "minNeighbors", "flags", "minSize", "maxSize", "outputRejectLevels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|diiOOb:CascadeClassifier.detectMultiScale3", (char**)keywords, &pyobj_image, &scaleFactor, &minNeighbors, &flags, &pyobj_minSize, &pyobj_maxSize, &outputRejectLevels) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_minSize, minSize, ArgInfo("minSize", 0)) &&
        pyopencv_to(pyobj_maxSize, maxSize, ArgInfo("maxSize", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(image, objects, rejectLevels, levelWeights, scaleFactor, minNeighbors, flags, minSize, maxSize, outputRejectLevels));
        return Py_BuildValue("(NNN)", pyopencv_from(objects), pyopencv_from(rejectLevels), pyopencv_from(levelWeights));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_getFeatureType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFeatureType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_getOriginalWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOriginalWindowSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_isOldFormatCascade(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isOldFormatCascade());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_load(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:CascadeClassifier.load", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(retval = _self_->load(filename));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CascadeClassifier_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CascadeClassifier* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CascadeClassifier_Type))
        _self_ = ((pyopencv_CascadeClassifier_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CascadeClassifier' or its derivative)");
    PyObject* pyobj_node = NULL;
    FileNode node;
    bool retval;

    const char* keywords[] = { "node", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:CascadeClassifier.read", (char**)keywords, &pyobj_node) &&
        pyopencv_to(pyobj_node, node, ArgInfo("node", 0)) )
    {
        ERRWRAP2(retval = _self_->read(node));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_CascadeClassifier_methods[] =
{
    {"convert", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_convert_cls, METH_CLASS), "convert(oldcascade, newcascade) -> retval\n."},
    {"detectMultiScale", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_detectMultiScale, 0), "detectMultiScale(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) -> objects\n.   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n.   of rectangles.\n.   \n.   @param image Matrix of the type CV_8U containing an image where objects are detected.\n.   @param objects Vector of rectangles where each rectangle contains the detected object, the\n.   rectangles may be partially outside the original image.\n.   @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n.   @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n.   to retain it.\n.   @param flags Parameter with the same meaning for an old cascade as in the function\n.   cvHaarDetectObjects. It is not used for a new cascade.\n.   @param minSize Minimum possible object size. Objects smaller than that are ignored.\n.   @param maxSize Maximum possible object size. Objects larger than that are ignored. If `maxSize == minSize` model is evaluated on single scale.\n.   \n.   The function is parallelized with the TBB library.\n.   \n.   @note\n.   -   (Python) A face detection example using cascade classifiers can be found at\n.   opencv_source_code/samples/python/facedetect.py"},
    {"detectMultiScale2", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_detectMultiScale2, 0), "detectMultiScale2(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) -> objects, numDetections\n.   @overload\n.   @param image Matrix of the type CV_8U containing an image where objects are detected.\n.   @param objects Vector of rectangles where each rectangle contains the detected object, the\n.   rectangles may be partially outside the original image.\n.   @param numDetections Vector of detection numbers for the corresponding objects. An object's number\n.   of detections is the number of neighboring positively classified rectangles that were joined\n.   together to form the object.\n.   @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n.   @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n.   to retain it.\n.   @param flags Parameter with the same meaning for an old cascade as in the function\n.   cvHaarDetectObjects. It is not used for a new cascade.\n.   @param minSize Minimum possible object size. Objects smaller than that are ignored.\n.   @param maxSize Maximum possible object size. Objects larger than that are ignored. If `maxSize == minSize` model is evaluated on single scale."},
    {"detectMultiScale3", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_detectMultiScale3, 0), "detectMultiScale3(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize[, outputRejectLevels]]]]]]) -> objects, rejectLevels, levelWeights\n.   @overload\n.   This function allows you to retrieve the final stage decision certainty of classification.\n.   For this, one needs to set `outputRejectLevels` on true and provide the `rejectLevels` and `levelWeights` parameter.\n.   For each resulting detection, `levelWeights` will then contain the certainty of classification at the final stage.\n.   This value can then be used to separate strong from weaker classifications.\n.   \n.   A code sample on how to use it efficiently can be found below:\n.   @code\n.   Mat img;\n.   vector<double> weights;\n.   vector<int> levels;\n.   vector<Rect> detections;\n.   CascadeClassifier model(\"/path/to/your/model.xml\");\n.   model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);\n.   cerr << \"Detection \" << detections[0] << \" with weight \" << weights[0] << endl;\n.   @endcode"},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_empty, 0), "empty() -> retval\n.   @brief Checks whether the classifier has been loaded."},
    {"getFeatureType", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_getFeatureType, 0), "getFeatureType() -> retval\n."},
    {"getOriginalWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_getOriginalWindowSize, 0), "getOriginalWindowSize() -> retval\n."},
    {"isOldFormatCascade", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_isOldFormatCascade, 0), "isOldFormatCascade() -> retval\n."},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_load, 0), "load(filename) -> retval\n.   @brief Loads a classifier from a file.\n.   \n.   @param filename Name of the file from which the classifier is loaded. The file may contain an old\n.   HAAR classifier trained by the haartraining application or a new cascade classifier trained by the\n.   traincascade application."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_CascadeClassifier_read, 0), "read(node) -> retval\n.   @brief Reads a classifier from a FileStorage node.\n.   \n.   @note The file may contain a new cascade classifier (trained traincascade application) only."},

    {NULL,          NULL}
};

static void pyopencv_CascadeClassifier_specials(void)
{
    pyopencv_CascadeClassifier_Type.tp_base = NULL;
    pyopencv_CascadeClassifier_Type.tp_dealloc = pyopencv_CascadeClassifier_dealloc;
    pyopencv_CascadeClassifier_Type.tp_repr = pyopencv_CascadeClassifier_repr;
    pyopencv_CascadeClassifier_Type.tp_getset = pyopencv_CascadeClassifier_getseters;
    pyopencv_CascadeClassifier_Type.tp_init = (initproc)pyopencv_cv_CascadeClassifier_CascadeClassifier;
    pyopencv_CascadeClassifier_Type.tp_methods = pyopencv_CascadeClassifier_methods;
}

static PyObject* pyopencv_HOGDescriptor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<HOGDescriptor %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_HOGDescriptor_get_L2HysThreshold(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->L2HysThreshold);
}

static PyObject* pyopencv_HOGDescriptor_get_blockSize(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->blockSize);
}

static PyObject* pyopencv_HOGDescriptor_get_blockStride(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->blockStride);
}

static PyObject* pyopencv_HOGDescriptor_get_cellSize(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->cellSize);
}

static PyObject* pyopencv_HOGDescriptor_get_derivAperture(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->derivAperture);
}

static PyObject* pyopencv_HOGDescriptor_get_gammaCorrection(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->gammaCorrection);
}

static PyObject* pyopencv_HOGDescriptor_get_histogramNormType(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->histogramNormType);
}

static PyObject* pyopencv_HOGDescriptor_get_nbins(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->nbins);
}

static PyObject* pyopencv_HOGDescriptor_get_nlevels(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->nlevels);
}

static PyObject* pyopencv_HOGDescriptor_get_signedGradient(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->signedGradient);
}

static PyObject* pyopencv_HOGDescriptor_get_svmDetector(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->svmDetector);
}

static PyObject* pyopencv_HOGDescriptor_get_winSigma(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->winSigma);
}

static PyObject* pyopencv_HOGDescriptor_get_winSize(pyopencv_HOGDescriptor_t* p, void *closure)
{
    return pyopencv_from(p->v->winSize);
}


static PyGetSetDef pyopencv_HOGDescriptor_getseters[] =
{
    {(char*)"L2HysThreshold", (getter)pyopencv_HOGDescriptor_get_L2HysThreshold, NULL, (char*)"L2HysThreshold", NULL},
    {(char*)"blockSize", (getter)pyopencv_HOGDescriptor_get_blockSize, NULL, (char*)"blockSize", NULL},
    {(char*)"blockStride", (getter)pyopencv_HOGDescriptor_get_blockStride, NULL, (char*)"blockStride", NULL},
    {(char*)"cellSize", (getter)pyopencv_HOGDescriptor_get_cellSize, NULL, (char*)"cellSize", NULL},
    {(char*)"derivAperture", (getter)pyopencv_HOGDescriptor_get_derivAperture, NULL, (char*)"derivAperture", NULL},
    {(char*)"gammaCorrection", (getter)pyopencv_HOGDescriptor_get_gammaCorrection, NULL, (char*)"gammaCorrection", NULL},
    {(char*)"histogramNormType", (getter)pyopencv_HOGDescriptor_get_histogramNormType, NULL, (char*)"histogramNormType", NULL},
    {(char*)"nbins", (getter)pyopencv_HOGDescriptor_get_nbins, NULL, (char*)"nbins", NULL},
    {(char*)"nlevels", (getter)pyopencv_HOGDescriptor_get_nlevels, NULL, (char*)"nlevels", NULL},
    {(char*)"signedGradient", (getter)pyopencv_HOGDescriptor_get_signedGradient, NULL, (char*)"signedGradient", NULL},
    {(char*)"svmDetector", (getter)pyopencv_HOGDescriptor_get_svmDetector, NULL, (char*)"svmDetector", NULL},
    {(char*)"winSigma", (getter)pyopencv_HOGDescriptor_get_winSigma, NULL, (char*)"winSigma", NULL},
    {(char*)"winSize", (getter)pyopencv_HOGDescriptor_get_winSize, NULL, (char*)"winSize", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_HOGDescriptor_HOGDescriptor(pyopencv_HOGDescriptor_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::HOGDescriptor()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__winSize = NULL;
    Size _winSize;
    PyObject* pyobj__blockSize = NULL;
    Size _blockSize;
    PyObject* pyobj__blockStride = NULL;
    Size _blockStride;
    PyObject* pyobj__cellSize = NULL;
    Size _cellSize;
    int _nbins=0;
    int _derivAperture=1;
    double _winSigma=-1;
    int _histogramNormType=HOGDescriptor::L2Hys;
    double _L2HysThreshold=0.2;
    bool _gammaCorrection=false;
    int _nlevels=HOGDescriptor::DEFAULT_NLEVELS;
    bool _signedGradient=false;

    const char* keywords[] = { "_winSize", "_blockSize", "_blockStride", "_cellSize", "_nbins", "_derivAperture", "_winSigma", "_histogramNormType", "_L2HysThreshold", "_gammaCorrection", "_nlevels", "_signedGradient", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOOi|ididbib:HOGDescriptor", (char**)keywords, &pyobj__winSize, &pyobj__blockSize, &pyobj__blockStride, &pyobj__cellSize, &_nbins, &_derivAperture, &_winSigma, &_histogramNormType, &_L2HysThreshold, &_gammaCorrection, &_nlevels, &_signedGradient) &&
        pyopencv_to(pyobj__winSize, _winSize, ArgInfo("_winSize", 0)) &&
        pyopencv_to(pyobj__blockSize, _blockSize, ArgInfo("_blockSize", 0)) &&
        pyopencv_to(pyobj__blockStride, _blockStride, ArgInfo("_blockStride", 0)) &&
        pyopencv_to(pyobj__cellSize, _cellSize, ArgInfo("_cellSize", 0)) )
    {
        new (&(self->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::HOGDescriptor(_winSize, _blockSize, _blockStride, _cellSize, _nbins, _derivAperture, _winSigma, _histogramNormType, _L2HysThreshold, _gammaCorrection, _nlevels, _signedGradient)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:HOGDescriptor", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        new (&(self->v)) Ptr<cv::HOGDescriptor>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::HOGDescriptor(filename)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_HOGDescriptor_checkDetectorSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->checkDetectorSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    vector_float descriptors;
    PyObject* pyobj_winStride = NULL;
    Size winStride;
    PyObject* pyobj_padding = NULL;
    Size padding;
    PyObject* pyobj_locations = NULL;
    vector_Point locations=std::vector<Point>();

    const char* keywords[] = { "img", "winStride", "padding", "locations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOO:HOGDescriptor.compute", (char**)keywords, &pyobj_img, &pyobj_winStride, &pyobj_padding, &pyobj_locations) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        pyopencv_to(pyobj_padding, padding, ArgInfo("padding", 0)) &&
        pyopencv_to(pyobj_locations, locations, ArgInfo("locations", 0)) )
    {
        ERRWRAP2(_self_->compute(img, descriptors, winStride, padding, locations));
        return pyopencv_from(descriptors);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;
    vector_float descriptors;
    PyObject* pyobj_winStride = NULL;
    Size winStride;
    PyObject* pyobj_padding = NULL;
    Size padding;
    PyObject* pyobj_locations = NULL;
    vector_Point locations=std::vector<Point>();

    const char* keywords[] = { "img", "winStride", "padding", "locations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOO:HOGDescriptor.compute", (char**)keywords, &pyobj_img, &pyobj_winStride, &pyobj_padding, &pyobj_locations) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        pyopencv_to(pyobj_padding, padding, ArgInfo("padding", 0)) &&
        pyopencv_to(pyobj_locations, locations, ArgInfo("locations", 0)) )
    {
        ERRWRAP2(_self_->compute(img, descriptors, winStride, padding, locations));
        return pyopencv_from(descriptors);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_computeGradient(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    PyObject* pyobj_grad = NULL;
    Mat grad;
    PyObject* pyobj_angleOfs = NULL;
    Mat angleOfs;
    PyObject* pyobj_paddingTL = NULL;
    Size paddingTL;
    PyObject* pyobj_paddingBR = NULL;
    Size paddingBR;

    const char* keywords[] = { "img", "grad", "angleOfs", "paddingTL", "paddingBR", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOOO:HOGDescriptor.computeGradient", (char**)keywords, &pyobj_img, &pyobj_grad, &pyobj_angleOfs, &pyobj_paddingTL, &pyobj_paddingBR) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_grad, grad, ArgInfo("grad", 1)) &&
        pyopencv_to(pyobj_angleOfs, angleOfs, ArgInfo("angleOfs", 1)) &&
        pyopencv_to(pyobj_paddingTL, paddingTL, ArgInfo("paddingTL", 0)) &&
        pyopencv_to(pyobj_paddingBR, paddingBR, ArgInfo("paddingBR", 0)) )
    {
        ERRWRAP2(_self_->computeGradient(img, grad, angleOfs, paddingTL, paddingBR));
        return Py_BuildValue("(NN)", pyopencv_from(grad), pyopencv_from(angleOfs));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    Mat img;
    PyObject* pyobj_grad = NULL;
    Mat grad;
    PyObject* pyobj_angleOfs = NULL;
    Mat angleOfs;
    PyObject* pyobj_paddingTL = NULL;
    Size paddingTL;
    PyObject* pyobj_paddingBR = NULL;
    Size paddingBR;

    const char* keywords[] = { "img", "grad", "angleOfs", "paddingTL", "paddingBR", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOOO:HOGDescriptor.computeGradient", (char**)keywords, &pyobj_img, &pyobj_grad, &pyobj_angleOfs, &pyobj_paddingTL, &pyobj_paddingBR) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_grad, grad, ArgInfo("grad", 1)) &&
        pyopencv_to(pyobj_angleOfs, angleOfs, ArgInfo("angleOfs", 1)) &&
        pyopencv_to(pyobj_paddingTL, paddingTL, ArgInfo("paddingTL", 0)) &&
        pyopencv_to(pyobj_paddingBR, paddingBR, ArgInfo("paddingBR", 0)) )
    {
        ERRWRAP2(_self_->computeGradient(img, grad, angleOfs, paddingTL, paddingBR));
        return Py_BuildValue("(NN)", pyopencv_from(grad), pyopencv_from(angleOfs));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_detect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    vector_Point foundLocations;
    vector_double weights;
    double hitThreshold=0;
    PyObject* pyobj_winStride = NULL;
    Size winStride;
    PyObject* pyobj_padding = NULL;
    Size padding;
    PyObject* pyobj_searchLocations = NULL;
    vector_Point searchLocations=std::vector<Point>();

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "searchLocations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|dOOO:HOGDescriptor.detect", (char**)keywords, &pyobj_img, &hitThreshold, &pyobj_winStride, &pyobj_padding, &pyobj_searchLocations) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        pyopencv_to(pyobj_padding, padding, ArgInfo("padding", 0)) &&
        pyopencv_to(pyobj_searchLocations, searchLocations, ArgInfo("searchLocations", 0)) )
    {
        ERRWRAP2(_self_->detect(img, foundLocations, weights, hitThreshold, winStride, padding, searchLocations));
        return Py_BuildValue("(NN)", pyopencv_from(foundLocations), pyopencv_from(weights));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    Mat img;
    vector_Point foundLocations;
    vector_double weights;
    double hitThreshold=0;
    PyObject* pyobj_winStride = NULL;
    Size winStride;
    PyObject* pyobj_padding = NULL;
    Size padding;
    PyObject* pyobj_searchLocations = NULL;
    vector_Point searchLocations=std::vector<Point>();

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "searchLocations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|dOOO:HOGDescriptor.detect", (char**)keywords, &pyobj_img, &hitThreshold, &pyobj_winStride, &pyobj_padding, &pyobj_searchLocations) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        pyopencv_to(pyobj_padding, padding, ArgInfo("padding", 0)) &&
        pyopencv_to(pyobj_searchLocations, searchLocations, ArgInfo("searchLocations", 0)) )
    {
        ERRWRAP2(_self_->detect(img, foundLocations, weights, hitThreshold, winStride, padding, searchLocations));
        return Py_BuildValue("(NN)", pyopencv_from(foundLocations), pyopencv_from(weights));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_detectMultiScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    vector_Rect foundLocations;
    vector_double foundWeights;
    double hitThreshold=0;
    PyObject* pyobj_winStride = NULL;
    Size winStride;
    PyObject* pyobj_padding = NULL;
    Size padding;
    double scale=1.05;
    double finalThreshold=2.0;
    bool useMeanshiftGrouping=false;

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "scale", "finalThreshold", "useMeanshiftGrouping", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|dOOddb:HOGDescriptor.detectMultiScale", (char**)keywords, &pyobj_img, &hitThreshold, &pyobj_winStride, &pyobj_padding, &scale, &finalThreshold, &useMeanshiftGrouping) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        pyopencv_to(pyobj_padding, padding, ArgInfo("padding", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(img, foundLocations, foundWeights, hitThreshold, winStride, padding, scale, finalThreshold, useMeanshiftGrouping));
        return Py_BuildValue("(NN)", pyopencv_from(foundLocations), pyopencv_from(foundWeights));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;
    vector_Rect foundLocations;
    vector_double foundWeights;
    double hitThreshold=0;
    PyObject* pyobj_winStride = NULL;
    Size winStride;
    PyObject* pyobj_padding = NULL;
    Size padding;
    double scale=1.05;
    double finalThreshold=2.0;
    bool useMeanshiftGrouping=false;

    const char* keywords[] = { "img", "hitThreshold", "winStride", "padding", "scale", "finalThreshold", "useMeanshiftGrouping", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|dOOddb:HOGDescriptor.detectMultiScale", (char**)keywords, &pyobj_img, &hitThreshold, &pyobj_winStride, &pyobj_padding, &scale, &finalThreshold, &useMeanshiftGrouping) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_winStride, winStride, ArgInfo("winStride", 0)) &&
        pyopencv_to(pyobj_padding, padding, ArgInfo("padding", 0)) )
    {
        ERRWRAP2(_self_->detectMultiScale(img, foundLocations, foundWeights, hitThreshold, winStride, padding, scale, finalThreshold, useMeanshiftGrouping));
        return Py_BuildValue("(NN)", pyopencv_from(foundLocations), pyopencv_from(foundWeights));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_getDaimlerPeopleDetector_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    std::vector<float> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::HOGDescriptor::getDaimlerPeopleDetector());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_getDefaultPeopleDetector_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    std::vector<float> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::HOGDescriptor::getDefaultPeopleDetector());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_getDescriptorSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    size_t retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDescriptorSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_getWinSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWinSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_load(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;
    PyObject* pyobj_objname = NULL;
    String objname;
    bool retval;

    const char* keywords[] = { "filename", "objname", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:HOGDescriptor.load", (char**)keywords, &pyobj_filename, &pyobj_objname) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_objname, objname, ArgInfo("objname", 0)) )
    {
        ERRWRAP2(retval = _self_->load(filename, objname));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_save(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;
    PyObject* pyobj_objname = NULL;
    String objname;

    const char* keywords[] = { "filename", "objname", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:HOGDescriptor.save", (char**)keywords, &pyobj_filename, &pyobj_objname) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_objname, objname, ArgInfo("objname", 0)) )
    {
        ERRWRAP2(_self_->save(filename, objname));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_HOGDescriptor_setSVMDetector(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HOGDescriptor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HOGDescriptor_Type))
        _self_ = ((pyopencv_HOGDescriptor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HOGDescriptor' or its derivative)");
    {
    PyObject* pyobj__svmdetector = NULL;
    Mat _svmdetector;

    const char* keywords[] = { "_svmdetector", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:HOGDescriptor.setSVMDetector", (char**)keywords, &pyobj__svmdetector) &&
        pyopencv_to(pyobj__svmdetector, _svmdetector, ArgInfo("_svmdetector", 0)) )
    {
        ERRWRAP2(_self_->setSVMDetector(_svmdetector));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__svmdetector = NULL;
    UMat _svmdetector;

    const char* keywords[] = { "_svmdetector", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:HOGDescriptor.setSVMDetector", (char**)keywords, &pyobj__svmdetector) &&
        pyopencv_to(pyobj__svmdetector, _svmdetector, ArgInfo("_svmdetector", 0)) )
    {
        ERRWRAP2(_self_->setSVMDetector(_svmdetector));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_HOGDescriptor_methods[] =
{
    {"checkDetectorSize", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_checkDetectorSize, 0), "checkDetectorSize() -> retval\n.   @brief Checks if detector size equal to descriptor size."},
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_compute, 0), "compute(img[, winStride[, padding[, locations]]]) -> descriptors\n.   @brief Computes HOG descriptors of given image.\n.   @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.\n.   @param descriptors Matrix of the type CV_32F\n.   @param winStride Window stride. It must be a multiple of block stride.\n.   @param padding Padding\n.   @param locations Vector of Point"},
    {"computeGradient", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_computeGradient, 0), "computeGradient(img[, grad[, angleOfs[, paddingTL[, paddingBR]]]]) -> grad, angleOfs\n.   @brief  Computes gradients and quantized gradient orientations.\n.   @param img Matrix contains the image to be computed\n.   @param grad Matrix of type CV_32FC2 contains computed gradients\n.   @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations\n.   @param paddingTL Padding from top-left\n.   @param paddingBR Padding from bottom-right"},
    {"detect", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_detect, 0), "detect(img[, hitThreshold[, winStride[, padding[, searchLocations]]]]) -> foundLocations, weights\n.   @brief Performs object detection without a multi-scale window.\n.   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n.   @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.\n.   @param weights Vector that will contain confidence values for each detected object.\n.   @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n.   Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n.   But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n.   @param winStride Window stride. It must be a multiple of block stride.\n.   @param padding Padding\n.   @param searchLocations Vector of Point includes set of requested locations to be evaluated."},
    {"detectMultiScale", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_detectMultiScale, 0), "detectMultiScale(img[, hitThreshold[, winStride[, padding[, scale[, finalThreshold[, useMeanshiftGrouping]]]]]]) -> foundLocations, foundWeights\n.   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n.   of rectangles.\n.   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n.   @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n.   @param foundWeights Vector that will contain confidence values for each detected object.\n.   @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n.   Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n.   But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n.   @param winStride Window stride. It must be a multiple of block stride.\n.   @param padding Padding\n.   @param scale Coefficient of the detection window increase.\n.   @param finalThreshold Final threshold\n.   @param useMeanshiftGrouping indicates grouping algorithm"},
    {"getDaimlerPeopleDetector", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getDaimlerPeopleDetector_cls, METH_CLASS), "getDaimlerPeopleDetector() -> retval\n.   @brief Returns coefficients of the classifier trained for people detection (for 48x96 windows)."},
    {"getDefaultPeopleDetector", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getDefaultPeopleDetector_cls, METH_CLASS), "getDefaultPeopleDetector() -> retval\n.   @brief Returns coefficients of the classifier trained for people detection (for 64x128 windows)."},
    {"getDescriptorSize", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getDescriptorSize, 0), "getDescriptorSize() -> retval\n.   @brief Returns the number of coefficients required for the classification."},
    {"getWinSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_getWinSigma, 0), "getWinSigma() -> retval\n.   @brief Returns winSigma value"},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_load, 0), "load(filename[, objname]) -> retval\n.   @brief loads coefficients for the linear SVM classifier from a file\n.   @param filename Name of the file to read.\n.   @param objname The optional name of the node to read (if empty, the first top-level node will be used)."},
    {"save", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_save, 0), "save(filename[, objname]) -> None\n.   @brief saves coefficients for the linear SVM classifier to a file\n.   @param filename File name\n.   @param objname Object name"},
    {"setSVMDetector", CV_PY_FN_WITH_KW_(pyopencv_cv_HOGDescriptor_setSVMDetector, 0), "setSVMDetector(_svmdetector) -> None\n.   @brief Sets coefficients for the linear SVM classifier.\n.   @param _svmdetector coefficients for the linear SVM classifier."},

    {NULL,          NULL}
};

static void pyopencv_HOGDescriptor_specials(void)
{
    pyopencv_HOGDescriptor_Type.tp_base = NULL;
    pyopencv_HOGDescriptor_Type.tp_dealloc = pyopencv_HOGDescriptor_dealloc;
    pyopencv_HOGDescriptor_Type.tp_repr = pyopencv_HOGDescriptor_repr;
    pyopencv_HOGDescriptor_Type.tp_getset = pyopencv_HOGDescriptor_getseters;
    pyopencv_HOGDescriptor_Type.tp_init = (initproc)pyopencv_cv_HOGDescriptor_HOGDescriptor;
    pyopencv_HOGDescriptor_Type.tp_methods = pyopencv_HOGDescriptor_methods;
}

static PyObject* pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<phase_unwrapping_HistogramPhaseUnwrapping %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_phase_unwrapping_phase_unwrapping_HistogramPhaseUnwrapping_getInverseReliabilityMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::phase_unwrapping;

    cv::phase_unwrapping::HistogramPhaseUnwrapping* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type))
        _self_ = dynamic_cast<cv::phase_unwrapping::HistogramPhaseUnwrapping*>(((pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'phase_unwrapping_HistogramPhaseUnwrapping' or its derivative)");
    {
    PyObject* pyobj_reliabilityMap = NULL;
    Mat reliabilityMap;

    const char* keywords[] = { "reliabilityMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:phase_unwrapping_HistogramPhaseUnwrapping.getInverseReliabilityMap", (char**)keywords, &pyobj_reliabilityMap) &&
        pyopencv_to(pyobj_reliabilityMap, reliabilityMap, ArgInfo("reliabilityMap", 1)) )
    {
        ERRWRAP2(_self_->getInverseReliabilityMap(reliabilityMap));
        return pyopencv_from(reliabilityMap);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_reliabilityMap = NULL;
    UMat reliabilityMap;

    const char* keywords[] = { "reliabilityMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:phase_unwrapping_HistogramPhaseUnwrapping.getInverseReliabilityMap", (char**)keywords, &pyobj_reliabilityMap) &&
        pyopencv_to(pyobj_reliabilityMap, reliabilityMap, ArgInfo("reliabilityMap", 1)) )
    {
        ERRWRAP2(_self_->getInverseReliabilityMap(reliabilityMap));
        return pyopencv_from(reliabilityMap);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_methods[] =
{
    {"getInverseReliabilityMap", CV_PY_FN_WITH_KW_(pyopencv_cv_phase_unwrapping_phase_unwrapping_HistogramPhaseUnwrapping_getInverseReliabilityMap, 0), "getInverseReliabilityMap([, reliabilityMap]) -> reliabilityMap\n.   * @brief Get the reliability map computed from the wrapped phase map.\n.   \n.   * @param reliabilityMap Image where the reliability map is stored."},

    {NULL,          NULL}
};

static void pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_specials(void)
{
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type.tp_base = &pyopencv_phase_unwrapping_PhaseUnwrapping_Type;
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type.tp_dealloc = pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_dealloc;
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type.tp_repr = pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_repr;
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type.tp_getset = pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_getseters;
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type.tp_init = (initproc)0;
    pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_Type.tp_methods = pyopencv_phase_unwrapping_HistogramPhaseUnwrapping_methods;
}

static PyObject* pyopencv_phase_unwrapping_PhaseUnwrapping_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<phase_unwrapping_PhaseUnwrapping %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_phase_unwrapping_PhaseUnwrapping_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_phase_unwrapping_phase_unwrapping_PhaseUnwrapping_unwrapPhaseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::phase_unwrapping;

    cv::phase_unwrapping::PhaseUnwrapping* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_phase_unwrapping_PhaseUnwrapping_Type))
        _self_ = dynamic_cast<cv::phase_unwrapping::PhaseUnwrapping*>(((pyopencv_phase_unwrapping_PhaseUnwrapping_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'phase_unwrapping_PhaseUnwrapping' or its derivative)");
    {
    PyObject* pyobj_wrappedPhaseMap = NULL;
    Mat wrappedPhaseMap;
    PyObject* pyobj_unwrappedPhaseMap = NULL;
    Mat unwrappedPhaseMap;
    PyObject* pyobj_shadowMask = NULL;
    Mat shadowMask;

    const char* keywords[] = { "wrappedPhaseMap", "unwrappedPhaseMap", "shadowMask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OO:phase_unwrapping_PhaseUnwrapping.unwrapPhaseMap", (char**)keywords, &pyobj_wrappedPhaseMap, &pyobj_unwrappedPhaseMap, &pyobj_shadowMask) &&
        pyopencv_to(pyobj_wrappedPhaseMap, wrappedPhaseMap, ArgInfo("wrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_unwrappedPhaseMap, unwrappedPhaseMap, ArgInfo("unwrappedPhaseMap", 1)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 0)) )
    {
        ERRWRAP2(_self_->unwrapPhaseMap(wrappedPhaseMap, unwrappedPhaseMap, shadowMask));
        return pyopencv_from(unwrappedPhaseMap);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_wrappedPhaseMap = NULL;
    UMat wrappedPhaseMap;
    PyObject* pyobj_unwrappedPhaseMap = NULL;
    UMat unwrappedPhaseMap;
    PyObject* pyobj_shadowMask = NULL;
    UMat shadowMask;

    const char* keywords[] = { "wrappedPhaseMap", "unwrappedPhaseMap", "shadowMask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OO:phase_unwrapping_PhaseUnwrapping.unwrapPhaseMap", (char**)keywords, &pyobj_wrappedPhaseMap, &pyobj_unwrappedPhaseMap, &pyobj_shadowMask) &&
        pyopencv_to(pyobj_wrappedPhaseMap, wrappedPhaseMap, ArgInfo("wrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_unwrappedPhaseMap, unwrappedPhaseMap, ArgInfo("unwrappedPhaseMap", 1)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 0)) )
    {
        ERRWRAP2(_self_->unwrapPhaseMap(wrappedPhaseMap, unwrappedPhaseMap, shadowMask));
        return pyopencv_from(unwrappedPhaseMap);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_phase_unwrapping_PhaseUnwrapping_methods[] =
{
    {"unwrapPhaseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_phase_unwrapping_phase_unwrapping_PhaseUnwrapping_unwrapPhaseMap, 0), "unwrapPhaseMap(wrappedPhaseMap[, unwrappedPhaseMap[, shadowMask]]) -> unwrappedPhaseMap\n.   * @brief Unwraps a 2D phase map.\n.   \n.   * @param wrappedPhaseMap The wrapped phase map that needs to be unwrapped.\n.   * @param unwrappedPhaseMap The unwrapped phase map.\n.   * @param shadowMask Optional parameter used when some pixels do not hold any phase information in the wrapped phase map."},

    {NULL,          NULL}
};

static void pyopencv_phase_unwrapping_PhaseUnwrapping_specials(void)
{
    pyopencv_phase_unwrapping_PhaseUnwrapping_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_phase_unwrapping_PhaseUnwrapping_Type.tp_dealloc = pyopencv_phase_unwrapping_PhaseUnwrapping_dealloc;
    pyopencv_phase_unwrapping_PhaseUnwrapping_Type.tp_repr = pyopencv_phase_unwrapping_PhaseUnwrapping_repr;
    pyopencv_phase_unwrapping_PhaseUnwrapping_Type.tp_getset = pyopencv_phase_unwrapping_PhaseUnwrapping_getseters;
    pyopencv_phase_unwrapping_PhaseUnwrapping_Type.tp_init = (initproc)0;
    pyopencv_phase_unwrapping_PhaseUnwrapping_Type.tp_methods = pyopencv_phase_unwrapping_PhaseUnwrapping_methods;
}

static PyObject* pyopencv_Tonemap_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<Tonemap %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_Tonemap_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_Tonemap_getGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Tonemap* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Tonemap_Type))
        _self_ = dynamic_cast<cv::Tonemap*>(((pyopencv_Tonemap_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Tonemap' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGamma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Tonemap_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Tonemap* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Tonemap_Type))
        _self_ = dynamic_cast<cv::Tonemap*>(((pyopencv_Tonemap_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Tonemap' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Tonemap.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->process(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Tonemap.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->process(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Tonemap_setGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Tonemap* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Tonemap_Type))
        _self_ = dynamic_cast<cv::Tonemap*>(((pyopencv_Tonemap_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Tonemap' or its derivative)");
    float gamma=0.f;

    const char* keywords[] = { "gamma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:Tonemap.setGamma", (char**)keywords, &gamma) )
    {
        ERRWRAP2(_self_->setGamma(gamma));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_Tonemap_methods[] =
{
    {"getGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_Tonemap_getGamma, 0), "getGamma() -> retval\n."},
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_Tonemap_process, 0), "process(src[, dst]) -> dst\n.   @brief Tonemaps image\n.   \n.   @param src source image - 32-bit 3-channel Mat\n.   @param dst destination image - 32-bit 3-channel Mat with values in [0, 1] range"},
    {"setGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_Tonemap_setGamma, 0), "setGamma(gamma) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_Tonemap_specials(void)
{
    pyopencv_Tonemap_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_Tonemap_Type.tp_dealloc = pyopencv_Tonemap_dealloc;
    pyopencv_Tonemap_Type.tp_repr = pyopencv_Tonemap_repr;
    pyopencv_Tonemap_Type.tp_getset = pyopencv_Tonemap_getseters;
    pyopencv_Tonemap_Type.tp_init = (initproc)0;
    pyopencv_Tonemap_Type.tp_methods = pyopencv_Tonemap_methods;
}

static PyObject* pyopencv_TonemapDrago_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TonemapDrago %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TonemapDrago_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TonemapDrago_getBias(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDrago* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDrago_Type))
        _self_ = dynamic_cast<cv::TonemapDrago*>(((pyopencv_TonemapDrago_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBias());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDrago_getSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDrago* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDrago_Type))
        _self_ = dynamic_cast<cv::TonemapDrago*>(((pyopencv_TonemapDrago_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSaturation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDrago_setBias(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDrago* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDrago_Type))
        _self_ = dynamic_cast<cv::TonemapDrago*>(((pyopencv_TonemapDrago_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    float bias=0.f;

    const char* keywords[] = { "bias", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapDrago.setBias", (char**)keywords, &bias) )
    {
        ERRWRAP2(_self_->setBias(bias));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDrago_setSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDrago* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDrago_Type))
        _self_ = dynamic_cast<cv::TonemapDrago*>(((pyopencv_TonemapDrago_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDrago' or its derivative)");
    float saturation=0.f;

    const char* keywords[] = { "saturation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapDrago.setSaturation", (char**)keywords, &saturation) )
    {
        ERRWRAP2(_self_->setSaturation(saturation));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_TonemapDrago_methods[] =
{
    {"getBias", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDrago_getBias, 0), "getBias() -> retval\n."},
    {"getSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDrago_getSaturation, 0), "getSaturation() -> retval\n."},
    {"setBias", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDrago_setBias, 0), "setBias(bias) -> None\n."},
    {"setSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDrago_setSaturation, 0), "setSaturation(saturation) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_TonemapDrago_specials(void)
{
    pyopencv_TonemapDrago_Type.tp_base = &pyopencv_Tonemap_Type;
    pyopencv_TonemapDrago_Type.tp_dealloc = pyopencv_TonemapDrago_dealloc;
    pyopencv_TonemapDrago_Type.tp_repr = pyopencv_TonemapDrago_repr;
    pyopencv_TonemapDrago_Type.tp_getset = pyopencv_TonemapDrago_getseters;
    pyopencv_TonemapDrago_Type.tp_init = (initproc)0;
    pyopencv_TonemapDrago_Type.tp_methods = pyopencv_TonemapDrago_methods;
}

static PyObject* pyopencv_TonemapDurand_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TonemapDurand %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TonemapDurand_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TonemapDurand_getContrast(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getContrast());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_getSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSaturation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_getSigmaColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigmaColor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_getSigmaSpace(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigmaSpace());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_setContrast(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float contrast=0.f;

    const char* keywords[] = { "contrast", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapDurand.setContrast", (char**)keywords, &contrast) )
    {
        ERRWRAP2(_self_->setContrast(contrast));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_setSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float saturation=0.f;

    const char* keywords[] = { "saturation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapDurand.setSaturation", (char**)keywords, &saturation) )
    {
        ERRWRAP2(_self_->setSaturation(saturation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_setSigmaColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float sigma_color=0.f;

    const char* keywords[] = { "sigma_color", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapDurand.setSigmaColor", (char**)keywords, &sigma_color) )
    {
        ERRWRAP2(_self_->setSigmaColor(sigma_color));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapDurand_setSigmaSpace(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapDurand* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapDurand_Type))
        _self_ = dynamic_cast<cv::TonemapDurand*>(((pyopencv_TonemapDurand_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapDurand' or its derivative)");
    float sigma_space=0.f;

    const char* keywords[] = { "sigma_space", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapDurand.setSigmaSpace", (char**)keywords, &sigma_space) )
    {
        ERRWRAP2(_self_->setSigmaSpace(sigma_space));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_TonemapDurand_methods[] =
{
    {"getContrast", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_getContrast, 0), "getContrast() -> retval\n."},
    {"getSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_getSaturation, 0), "getSaturation() -> retval\n."},
    {"getSigmaColor", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_getSigmaColor, 0), "getSigmaColor() -> retval\n."},
    {"getSigmaSpace", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_getSigmaSpace, 0), "getSigmaSpace() -> retval\n."},
    {"setContrast", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_setContrast, 0), "setContrast(contrast) -> None\n."},
    {"setSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_setSaturation, 0), "setSaturation(saturation) -> None\n."},
    {"setSigmaColor", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_setSigmaColor, 0), "setSigmaColor(sigma_color) -> None\n."},
    {"setSigmaSpace", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapDurand_setSigmaSpace, 0), "setSigmaSpace(sigma_space) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_TonemapDurand_specials(void)
{
    pyopencv_TonemapDurand_Type.tp_base = &pyopencv_Tonemap_Type;
    pyopencv_TonemapDurand_Type.tp_dealloc = pyopencv_TonemapDurand_dealloc;
    pyopencv_TonemapDurand_Type.tp_repr = pyopencv_TonemapDurand_repr;
    pyopencv_TonemapDurand_Type.tp_getset = pyopencv_TonemapDurand_getseters;
    pyopencv_TonemapDurand_Type.tp_init = (initproc)0;
    pyopencv_TonemapDurand_Type.tp_methods = pyopencv_TonemapDurand_methods;
}

static PyObject* pyopencv_TonemapReinhard_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TonemapReinhard %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TonemapReinhard_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TonemapReinhard_getColorAdaptation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapReinhard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapReinhard_Type))
        _self_ = dynamic_cast<cv::TonemapReinhard*>(((pyopencv_TonemapReinhard_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getColorAdaptation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapReinhard_getIntensity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapReinhard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapReinhard_Type))
        _self_ = dynamic_cast<cv::TonemapReinhard*>(((pyopencv_TonemapReinhard_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIntensity());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapReinhard_getLightAdaptation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapReinhard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapReinhard_Type))
        _self_ = dynamic_cast<cv::TonemapReinhard*>(((pyopencv_TonemapReinhard_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLightAdaptation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapReinhard_setColorAdaptation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapReinhard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapReinhard_Type))
        _self_ = dynamic_cast<cv::TonemapReinhard*>(((pyopencv_TonemapReinhard_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    float color_adapt=0.f;

    const char* keywords[] = { "color_adapt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapReinhard.setColorAdaptation", (char**)keywords, &color_adapt) )
    {
        ERRWRAP2(_self_->setColorAdaptation(color_adapt));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapReinhard_setIntensity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapReinhard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapReinhard_Type))
        _self_ = dynamic_cast<cv::TonemapReinhard*>(((pyopencv_TonemapReinhard_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    float intensity=0.f;

    const char* keywords[] = { "intensity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapReinhard.setIntensity", (char**)keywords, &intensity) )
    {
        ERRWRAP2(_self_->setIntensity(intensity));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapReinhard_setLightAdaptation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapReinhard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapReinhard_Type))
        _self_ = dynamic_cast<cv::TonemapReinhard*>(((pyopencv_TonemapReinhard_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapReinhard' or its derivative)");
    float light_adapt=0.f;

    const char* keywords[] = { "light_adapt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapReinhard.setLightAdaptation", (char**)keywords, &light_adapt) )
    {
        ERRWRAP2(_self_->setLightAdaptation(light_adapt));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_TonemapReinhard_methods[] =
{
    {"getColorAdaptation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_getColorAdaptation, 0), "getColorAdaptation() -> retval\n."},
    {"getIntensity", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_getIntensity, 0), "getIntensity() -> retval\n."},
    {"getLightAdaptation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_getLightAdaptation, 0), "getLightAdaptation() -> retval\n."},
    {"setColorAdaptation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_setColorAdaptation, 0), "setColorAdaptation(color_adapt) -> None\n."},
    {"setIntensity", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_setIntensity, 0), "setIntensity(intensity) -> None\n."},
    {"setLightAdaptation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapReinhard_setLightAdaptation, 0), "setLightAdaptation(light_adapt) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_TonemapReinhard_specials(void)
{
    pyopencv_TonemapReinhard_Type.tp_base = &pyopencv_Tonemap_Type;
    pyopencv_TonemapReinhard_Type.tp_dealloc = pyopencv_TonemapReinhard_dealloc;
    pyopencv_TonemapReinhard_Type.tp_repr = pyopencv_TonemapReinhard_repr;
    pyopencv_TonemapReinhard_Type.tp_getset = pyopencv_TonemapReinhard_getseters;
    pyopencv_TonemapReinhard_Type.tp_init = (initproc)0;
    pyopencv_TonemapReinhard_Type.tp_methods = pyopencv_TonemapReinhard_methods;
}

static PyObject* pyopencv_TonemapMantiuk_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TonemapMantiuk %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TonemapMantiuk_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TonemapMantiuk_getSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapMantiuk* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapMantiuk_Type))
        _self_ = dynamic_cast<cv::TonemapMantiuk*>(((pyopencv_TonemapMantiuk_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSaturation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapMantiuk_getScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapMantiuk* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapMantiuk_Type))
        _self_ = dynamic_cast<cv::TonemapMantiuk*>(((pyopencv_TonemapMantiuk_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScale());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapMantiuk_setSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapMantiuk* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapMantiuk_Type))
        _self_ = dynamic_cast<cv::TonemapMantiuk*>(((pyopencv_TonemapMantiuk_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    float saturation=0.f;

    const char* keywords[] = { "saturation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapMantiuk.setSaturation", (char**)keywords, &saturation) )
    {
        ERRWRAP2(_self_->setSaturation(saturation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_TonemapMantiuk_setScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::TonemapMantiuk* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_TonemapMantiuk_Type))
        _self_ = dynamic_cast<cv::TonemapMantiuk*>(((pyopencv_TonemapMantiuk_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'TonemapMantiuk' or its derivative)");
    float scale=0.f;

    const char* keywords[] = { "scale", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:TonemapMantiuk.setScale", (char**)keywords, &scale) )
    {
        ERRWRAP2(_self_->setScale(scale));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_TonemapMantiuk_methods[] =
{
    {"getSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_getSaturation, 0), "getSaturation() -> retval\n."},
    {"getScale", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_getScale, 0), "getScale() -> retval\n."},
    {"setSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_setSaturation, 0), "setSaturation(saturation) -> None\n."},
    {"setScale", CV_PY_FN_WITH_KW_(pyopencv_cv_TonemapMantiuk_setScale, 0), "setScale(scale) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_TonemapMantiuk_specials(void)
{
    pyopencv_TonemapMantiuk_Type.tp_base = &pyopencv_Tonemap_Type;
    pyopencv_TonemapMantiuk_Type.tp_dealloc = pyopencv_TonemapMantiuk_dealloc;
    pyopencv_TonemapMantiuk_Type.tp_repr = pyopencv_TonemapMantiuk_repr;
    pyopencv_TonemapMantiuk_Type.tp_getset = pyopencv_TonemapMantiuk_getseters;
    pyopencv_TonemapMantiuk_Type.tp_init = (initproc)0;
    pyopencv_TonemapMantiuk_Type.tp_methods = pyopencv_TonemapMantiuk_methods;
}

static PyObject* pyopencv_AlignExposures_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<AlignExposures %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_AlignExposures_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_AlignExposures_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignExposures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignExposures_Type))
        _self_ = dynamic_cast<cv::AlignExposures*>(((pyopencv_AlignExposures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignExposures' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    vector_Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;
    PyObject* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:AlignExposures.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    vector_Mat dst;
    PyObject* pyobj_times = NULL;
    UMat times;
    PyObject* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:AlignExposures.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_AlignExposures_methods[] =
{
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignExposures_process, 0), "process(src, dst, times, response) -> None\n.   @brief Aligns images\n.   \n.   @param src vector of input images\n.   @param dst vector of aligned images\n.   @param times vector of exposure time values for each image\n.   @param response 256x1 matrix with inverse camera response function for each pixel value, it should\n.   have the same number of channels as images."},

    {NULL,          NULL}
};

static void pyopencv_AlignExposures_specials(void)
{
    pyopencv_AlignExposures_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_AlignExposures_Type.tp_dealloc = pyopencv_AlignExposures_dealloc;
    pyopencv_AlignExposures_Type.tp_repr = pyopencv_AlignExposures_repr;
    pyopencv_AlignExposures_Type.tp_getset = pyopencv_AlignExposures_getseters;
    pyopencv_AlignExposures_Type.tp_init = (initproc)0;
    pyopencv_AlignExposures_Type.tp_methods = pyopencv_AlignExposures_methods;
}

static PyObject* pyopencv_AlignMTB_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<AlignMTB %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_AlignMTB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_AlignMTB_calculateShift(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    {
    PyObject* pyobj_img0 = NULL;
    Mat img0;
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    Point retval;

    const char* keywords[] = { "img0", "img1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:AlignMTB.calculateShift", (char**)keywords, &pyobj_img0, &pyobj_img1) &&
        pyopencv_to(pyobj_img0, img0, ArgInfo("img0", 0)) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) )
    {
        ERRWRAP2(retval = _self_->calculateShift(img0, img1));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img0 = NULL;
    UMat img0;
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    Point retval;

    const char* keywords[] = { "img0", "img1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:AlignMTB.calculateShift", (char**)keywords, &pyobj_img0, &pyobj_img1) &&
        pyopencv_to(pyobj_img0, img0, ArgInfo("img0", 0)) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) )
    {
        ERRWRAP2(retval = _self_->calculateShift(img0, img1));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_computeBitmaps(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    PyObject* pyobj_tb = NULL;
    Mat tb;
    PyObject* pyobj_eb = NULL;
    Mat eb;

    const char* keywords[] = { "img", "tb", "eb", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OO:AlignMTB.computeBitmaps", (char**)keywords, &pyobj_img, &pyobj_tb, &pyobj_eb) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_tb, tb, ArgInfo("tb", 1)) &&
        pyopencv_to(pyobj_eb, eb, ArgInfo("eb", 1)) )
    {
        ERRWRAP2(_self_->computeBitmaps(img, tb, eb));
        return Py_BuildValue("(NN)", pyopencv_from(tb), pyopencv_from(eb));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;
    PyObject* pyobj_tb = NULL;
    UMat tb;
    PyObject* pyobj_eb = NULL;
    UMat eb;

    const char* keywords[] = { "img", "tb", "eb", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OO:AlignMTB.computeBitmaps", (char**)keywords, &pyobj_img, &pyobj_tb, &pyobj_eb) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_tb, tb, ArgInfo("tb", 1)) &&
        pyopencv_to(pyobj_eb, eb, ArgInfo("eb", 1)) )
    {
        ERRWRAP2(_self_->computeBitmaps(img, tb, eb));
        return Py_BuildValue("(NN)", pyopencv_from(tb), pyopencv_from(eb));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_getCut(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCut());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_getExcludeRange(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getExcludeRange());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_getMaxBits(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxBits());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    vector_Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;
    PyObject* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    vector_Mat dst;
    PyObject* pyobj_times = NULL;
    UMat times;
    PyObject* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "dst", "times", "response", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_times, &pyobj_response) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    vector_Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    vector_Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:AlignMTB.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_setCut(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    bool value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:AlignMTB.setCut", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setCut(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_setExcludeRange(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    int exclude_range=0;

    const char* keywords[] = { "exclude_range", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AlignMTB.setExcludeRange", (char**)keywords, &exclude_range) )
    {
        ERRWRAP2(_self_->setExcludeRange(exclude_range));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_setMaxBits(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    int max_bits=0;

    const char* keywords[] = { "max_bits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AlignMTB.setMaxBits", (char**)keywords, &max_bits) )
    {
        ERRWRAP2(_self_->setMaxBits(max_bits));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AlignMTB_shiftMat(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AlignMTB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AlignMTB_Type))
        _self_ = dynamic_cast<cv::AlignMTB*>(((pyopencv_AlignMTB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AlignMTB' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_shift = NULL;
    Point shift;

    const char* keywords[] = { "src", "shift", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:AlignMTB.shiftMat", (char**)keywords, &pyobj_src, &pyobj_shift, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 0)) )
    {
        ERRWRAP2(_self_->shiftMat(src, dst, shift));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_shift = NULL;
    Point shift;

    const char* keywords[] = { "src", "shift", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:AlignMTB.shiftMat", (char**)keywords, &pyobj_src, &pyobj_shift, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 0)) )
    {
        ERRWRAP2(_self_->shiftMat(src, dst, shift));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_AlignMTB_methods[] =
{
    {"calculateShift", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_calculateShift, 0), "calculateShift(img0, img1) -> retval\n.   @brief Calculates shift between two images, i. e. how to shift the second image to correspond it with the\n.   first.\n.   \n.   @param img0 first image\n.   @param img1 second image"},
    {"computeBitmaps", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_computeBitmaps, 0), "computeBitmaps(img[, tb[, eb]]) -> tb, eb\n.   @brief Computes median threshold and exclude bitmaps of given image.\n.   \n.   @param img input image\n.   @param tb median threshold bitmap\n.   @param eb exclude bitmap"},
    {"getCut", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_getCut, 0), "getCut() -> retval\n."},
    {"getExcludeRange", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_getExcludeRange, 0), "getExcludeRange() -> retval\n."},
    {"getMaxBits", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_getMaxBits, 0), "getMaxBits() -> retval\n."},
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_process, 0), "process(src, dst, times, response) -> None\n.   \n\n\n\nprocess(src, dst) -> None\n.   @brief Short version of process, that doesn't take extra arguments.\n.   \n.   @param src vector of input images\n.   @param dst vector of aligned images"},
    {"setCut", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_setCut, 0), "setCut(value) -> None\n."},
    {"setExcludeRange", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_setExcludeRange, 0), "setExcludeRange(exclude_range) -> None\n."},
    {"setMaxBits", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_setMaxBits, 0), "setMaxBits(max_bits) -> None\n."},
    {"shiftMat", CV_PY_FN_WITH_KW_(pyopencv_cv_AlignMTB_shiftMat, 0), "shiftMat(src, shift[, dst]) -> dst\n.   @brief Helper function, that shift Mat filling new regions with zeros.\n.   \n.   @param src input image\n.   @param dst result image\n.   @param shift shift value"},

    {NULL,          NULL}
};

static void pyopencv_AlignMTB_specials(void)
{
    pyopencv_AlignMTB_Type.tp_base = &pyopencv_AlignExposures_Type;
    pyopencv_AlignMTB_Type.tp_dealloc = pyopencv_AlignMTB_dealloc;
    pyopencv_AlignMTB_Type.tp_repr = pyopencv_AlignMTB_repr;
    pyopencv_AlignMTB_Type.tp_getset = pyopencv_AlignMTB_getseters;
    pyopencv_AlignMTB_Type.tp_init = (initproc)0;
    pyopencv_AlignMTB_Type.tp_methods = pyopencv_AlignMTB_methods;
}

static PyObject* pyopencv_CalibrateCRF_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CalibrateCRF %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_CalibrateCRF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_CalibrateCRF_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateCRF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateCRF_Type))
        _self_ = dynamic_cast<cv::CalibrateCRF*>(((pyopencv_CalibrateCRF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateCRF' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:CalibrateCRF.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:CalibrateCRF.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_CalibrateCRF_methods[] =
{
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateCRF_process, 0), "process(src, times[, dst]) -> dst\n.   @brief Recovers inverse camera response.\n.   \n.   @param src vector of input images\n.   @param dst 256x1 matrix with inverse camera response function\n.   @param times vector of exposure time values for each image"},

    {NULL,          NULL}
};

static void pyopencv_CalibrateCRF_specials(void)
{
    pyopencv_CalibrateCRF_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_CalibrateCRF_Type.tp_dealloc = pyopencv_CalibrateCRF_dealloc;
    pyopencv_CalibrateCRF_Type.tp_repr = pyopencv_CalibrateCRF_repr;
    pyopencv_CalibrateCRF_Type.tp_getset = pyopencv_CalibrateCRF_getseters;
    pyopencv_CalibrateCRF_Type.tp_init = (initproc)0;
    pyopencv_CalibrateCRF_Type.tp_methods = pyopencv_CalibrateCRF_methods;
}

static PyObject* pyopencv_CalibrateDebevec_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CalibrateDebevec %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_CalibrateDebevec_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_CalibrateDebevec_getLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateDebevec_Type))
        _self_ = dynamic_cast<cv::CalibrateDebevec*>(((pyopencv_CalibrateDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLambda());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateDebevec_getRandom(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateDebevec_Type))
        _self_ = dynamic_cast<cv::CalibrateDebevec*>(((pyopencv_CalibrateDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRandom());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateDebevec_getSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateDebevec_Type))
        _self_ = dynamic_cast<cv::CalibrateDebevec*>(((pyopencv_CalibrateDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateDebevec_setLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateDebevec_Type))
        _self_ = dynamic_cast<cv::CalibrateDebevec*>(((pyopencv_CalibrateDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    float lambda=0.f;

    const char* keywords[] = { "lambda", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:CalibrateDebevec.setLambda", (char**)keywords, &lambda) )
    {
        ERRWRAP2(_self_->setLambda(lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateDebevec_setRandom(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateDebevec_Type))
        _self_ = dynamic_cast<cv::CalibrateDebevec*>(((pyopencv_CalibrateDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    bool random=0;

    const char* keywords[] = { "random", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:CalibrateDebevec.setRandom", (char**)keywords, &random) )
    {
        ERRWRAP2(_self_->setRandom(random));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateDebevec_setSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateDebevec_Type))
        _self_ = dynamic_cast<cv::CalibrateDebevec*>(((pyopencv_CalibrateDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateDebevec' or its derivative)");
    int samples=0;

    const char* keywords[] = { "samples", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:CalibrateDebevec.setSamples", (char**)keywords, &samples) )
    {
        ERRWRAP2(_self_->setSamples(samples));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_CalibrateDebevec_methods[] =
{
    {"getLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_getLambda, 0), "getLambda() -> retval\n."},
    {"getRandom", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_getRandom, 0), "getRandom() -> retval\n."},
    {"getSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_getSamples, 0), "getSamples() -> retval\n."},
    {"setLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_setLambda, 0), "setLambda(lambda) -> None\n."},
    {"setRandom", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_setRandom, 0), "setRandom(random) -> None\n."},
    {"setSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateDebevec_setSamples, 0), "setSamples(samples) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_CalibrateDebevec_specials(void)
{
    pyopencv_CalibrateDebevec_Type.tp_base = &pyopencv_CalibrateCRF_Type;
    pyopencv_CalibrateDebevec_Type.tp_dealloc = pyopencv_CalibrateDebevec_dealloc;
    pyopencv_CalibrateDebevec_Type.tp_repr = pyopencv_CalibrateDebevec_repr;
    pyopencv_CalibrateDebevec_Type.tp_getset = pyopencv_CalibrateDebevec_getseters;
    pyopencv_CalibrateDebevec_Type.tp_init = (initproc)0;
    pyopencv_CalibrateDebevec_Type.tp_methods = pyopencv_CalibrateDebevec_methods;
}

static PyObject* pyopencv_CalibrateRobertson_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CalibrateRobertson %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_CalibrateRobertson_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_CalibrateRobertson_getMaxIter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateRobertson* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateRobertson_Type))
        _self_ = dynamic_cast<cv::CalibrateRobertson*>(((pyopencv_CalibrateRobertson_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxIter());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateRobertson_getRadiance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateRobertson* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateRobertson_Type))
        _self_ = dynamic_cast<cv::CalibrateRobertson*>(((pyopencv_CalibrateRobertson_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRadiance());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateRobertson_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateRobertson* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateRobertson_Type))
        _self_ = dynamic_cast<cv::CalibrateRobertson*>(((pyopencv_CalibrateRobertson_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateRobertson_setMaxIter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateRobertson* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateRobertson_Type))
        _self_ = dynamic_cast<cv::CalibrateRobertson*>(((pyopencv_CalibrateRobertson_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    int max_iter=0;

    const char* keywords[] = { "max_iter", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:CalibrateRobertson.setMaxIter", (char**)keywords, &max_iter) )
    {
        ERRWRAP2(_self_->setMaxIter(max_iter));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_CalibrateRobertson_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::CalibrateRobertson* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_CalibrateRobertson_Type))
        _self_ = dynamic_cast<cv::CalibrateRobertson*>(((pyopencv_CalibrateRobertson_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'CalibrateRobertson' or its derivative)");
    float threshold=0.f;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:CalibrateRobertson.setThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_CalibrateRobertson_methods[] =
{
    {"getMaxIter", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_getMaxIter, 0), "getMaxIter() -> retval\n."},
    {"getRadiance", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_getRadiance, 0), "getRadiance() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_getThreshold, 0), "getThreshold() -> retval\n."},
    {"setMaxIter", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_setMaxIter, 0), "setMaxIter(max_iter) -> None\n."},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_CalibrateRobertson_setThreshold, 0), "setThreshold(threshold) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_CalibrateRobertson_specials(void)
{
    pyopencv_CalibrateRobertson_Type.tp_base = &pyopencv_CalibrateCRF_Type;
    pyopencv_CalibrateRobertson_Type.tp_dealloc = pyopencv_CalibrateRobertson_dealloc;
    pyopencv_CalibrateRobertson_Type.tp_repr = pyopencv_CalibrateRobertson_repr;
    pyopencv_CalibrateRobertson_Type.tp_getset = pyopencv_CalibrateRobertson_getseters;
    pyopencv_CalibrateRobertson_Type.tp_init = (initproc)0;
    pyopencv_CalibrateRobertson_Type.tp_methods = pyopencv_CalibrateRobertson_methods;
}

static PyObject* pyopencv_MergeExposures_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<MergeExposures %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_MergeExposures_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_MergeExposures_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeExposures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeExposures_Type))
        _self_ = dynamic_cast<cv::MergeExposures*>(((pyopencv_MergeExposures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeExposures' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;
    PyObject* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeExposures.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;
    PyObject* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeExposures.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_MergeExposures_methods[] =
{
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeExposures_process, 0), "process(src, times, response[, dst]) -> dst\n.   @brief Merges images.\n.   \n.   @param src vector of input images\n.   @param dst result image\n.   @param times vector of exposure time values for each image\n.   @param response 256x1 matrix with inverse camera response function for each pixel value, it should\n.   have the same number of channels as images."},

    {NULL,          NULL}
};

static void pyopencv_MergeExposures_specials(void)
{
    pyopencv_MergeExposures_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_MergeExposures_Type.tp_dealloc = pyopencv_MergeExposures_dealloc;
    pyopencv_MergeExposures_Type.tp_repr = pyopencv_MergeExposures_repr;
    pyopencv_MergeExposures_Type.tp_getset = pyopencv_MergeExposures_getseters;
    pyopencv_MergeExposures_Type.tp_init = (initproc)0;
    pyopencv_MergeExposures_Type.tp_methods = pyopencv_MergeExposures_methods;
}

static PyObject* pyopencv_MergeDebevec_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<MergeDebevec %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_MergeDebevec_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_MergeDebevec_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeDebevec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeDebevec_Type))
        _self_ = dynamic_cast<cv::MergeDebevec*>(((pyopencv_MergeDebevec_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeDebevec' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;
    PyObject* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;
    PyObject* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:MergeDebevec.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_MergeDebevec_methods[] =
{
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeDebevec_process, 0), "process(src, times, response[, dst]) -> dst\n.   \n\n\n\nprocess(src, times[, dst]) -> dst\n."},

    {NULL,          NULL}
};

static void pyopencv_MergeDebevec_specials(void)
{
    pyopencv_MergeDebevec_Type.tp_base = &pyopencv_MergeExposures_Type;
    pyopencv_MergeDebevec_Type.tp_dealloc = pyopencv_MergeDebevec_dealloc;
    pyopencv_MergeDebevec_Type.tp_repr = pyopencv_MergeDebevec_repr;
    pyopencv_MergeDebevec_Type.tp_getset = pyopencv_MergeDebevec_getseters;
    pyopencv_MergeDebevec_Type.tp_init = (initproc)0;
    pyopencv_MergeDebevec_Type.tp_methods = pyopencv_MergeDebevec_methods;
}

static PyObject* pyopencv_MergeMertens_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<MergeMertens %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_MergeMertens_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_MergeMertens_getContrastWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getContrastWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MergeMertens_getExposureWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getExposureWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MergeMertens_getSaturationWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSaturationWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MergeMertens_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;
    PyObject* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;
    PyObject* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->process(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:MergeMertens.process", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->process(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_MergeMertens_setContrastWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    float contrast_weiht=0.f;

    const char* keywords[] = { "contrast_weiht", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:MergeMertens.setContrastWeight", (char**)keywords, &contrast_weiht) )
    {
        ERRWRAP2(_self_->setContrastWeight(contrast_weiht));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_MergeMertens_setExposureWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    float exposure_weight=0.f;

    const char* keywords[] = { "exposure_weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:MergeMertens.setExposureWeight", (char**)keywords, &exposure_weight) )
    {
        ERRWRAP2(_self_->setExposureWeight(exposure_weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_MergeMertens_setSaturationWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeMertens* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeMertens_Type))
        _self_ = dynamic_cast<cv::MergeMertens*>(((pyopencv_MergeMertens_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeMertens' or its derivative)");
    float saturation_weight=0.f;

    const char* keywords[] = { "saturation_weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:MergeMertens.setSaturationWeight", (char**)keywords, &saturation_weight) )
    {
        ERRWRAP2(_self_->setSaturationWeight(saturation_weight));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_MergeMertens_methods[] =
{
    {"getContrastWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_getContrastWeight, 0), "getContrastWeight() -> retval\n."},
    {"getExposureWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_getExposureWeight, 0), "getExposureWeight() -> retval\n."},
    {"getSaturationWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_getSaturationWeight, 0), "getSaturationWeight() -> retval\n."},
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_process, 0), "process(src, times, response[, dst]) -> dst\n.   \n\n\n\nprocess(src[, dst]) -> dst\n.   @brief Short version of process, that doesn't take extra arguments.\n.   \n.   @param src vector of input images\n.   @param dst result image"},
    {"setContrastWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_setContrastWeight, 0), "setContrastWeight(contrast_weiht) -> None\n."},
    {"setExposureWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_setExposureWeight, 0), "setExposureWeight(exposure_weight) -> None\n."},
    {"setSaturationWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeMertens_setSaturationWeight, 0), "setSaturationWeight(saturation_weight) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_MergeMertens_specials(void)
{
    pyopencv_MergeMertens_Type.tp_base = &pyopencv_MergeExposures_Type;
    pyopencv_MergeMertens_Type.tp_dealloc = pyopencv_MergeMertens_dealloc;
    pyopencv_MergeMertens_Type.tp_repr = pyopencv_MergeMertens_repr;
    pyopencv_MergeMertens_Type.tp_getset = pyopencv_MergeMertens_getseters;
    pyopencv_MergeMertens_Type.tp_init = (initproc)0;
    pyopencv_MergeMertens_Type.tp_methods = pyopencv_MergeMertens_methods;
}

static PyObject* pyopencv_MergeRobertson_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<MergeRobertson %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_MergeRobertson_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_MergeRobertson_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MergeRobertson* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MergeRobertson_Type))
        _self_ = dynamic_cast<cv::MergeRobertson*>(((pyopencv_MergeRobertson_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MergeRobertson' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;
    PyObject* pyobj_response = NULL;
    Mat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;
    PyObject* pyobj_response = NULL;
    UMat response;

    const char* keywords[] = { "src", "times", "response", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_response, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) &&
        pyopencv_to(pyobj_response, response, ArgInfo("response", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times, response));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_times = NULL;
    Mat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_times = NULL;
    UMat times;

    const char* keywords[] = { "src", "times", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:MergeRobertson.process", (char**)keywords, &pyobj_src, &pyobj_times, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_times, times, ArgInfo("times", 0)) )
    {
        ERRWRAP2(_self_->process(src, dst, times));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_MergeRobertson_methods[] =
{
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_MergeRobertson_process, 0), "process(src, times, response[, dst]) -> dst\n.   \n\n\n\nprocess(src, times[, dst]) -> dst\n."},

    {NULL,          NULL}
};

static void pyopencv_MergeRobertson_specials(void)
{
    pyopencv_MergeRobertson_Type.tp_base = &pyopencv_MergeExposures_Type;
    pyopencv_MergeRobertson_Type.tp_dealloc = pyopencv_MergeRobertson_dealloc;
    pyopencv_MergeRobertson_Type.tp_repr = pyopencv_MergeRobertson_repr;
    pyopencv_MergeRobertson_Type.tp_getset = pyopencv_MergeRobertson_getseters;
    pyopencv_MergeRobertson_Type.tp_init = (initproc)0;
    pyopencv_MergeRobertson_Type.tp_methods = pyopencv_MergeRobertson_methods;
}

static PyObject* pyopencv_plot_Plot2d_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<plot_Plot2d %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_plot_Plot2d_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_plot_plot_Plot2d_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    {
    PyObject* pyobj_data = NULL;
    Mat data;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "data", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.create", (char**)keywords, &pyobj_data) &&
        pyopencv_to(pyobj_data, data, ArgInfo("data", 0)) )
    {
        ERRWRAP2(retval = cv::plot::Plot2d::create(data));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_data = NULL;
    UMat data;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "data", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.create", (char**)keywords, &pyobj_data) &&
        pyopencv_to(pyobj_data, data, ArgInfo("data", 0)) )
    {
        ERRWRAP2(retval = cv::plot::Plot2d::create(data));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_dataX = NULL;
    Mat dataX;
    PyObject* pyobj_dataY = NULL;
    Mat dataY;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "dataX", "dataY", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:plot_Plot2d.create", (char**)keywords, &pyobj_dataX, &pyobj_dataY) &&
        pyopencv_to(pyobj_dataX, dataX, ArgInfo("dataX", 0)) &&
        pyopencv_to(pyobj_dataY, dataY, ArgInfo("dataY", 0)) )
    {
        ERRWRAP2(retval = cv::plot::Plot2d::create(dataX, dataY));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_dataX = NULL;
    UMat dataX;
    PyObject* pyobj_dataY = NULL;
    UMat dataY;
    Ptr<Plot2d> retval;

    const char* keywords[] = { "dataX", "dataY", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:plot_Plot2d.create", (char**)keywords, &pyobj_dataX, &pyobj_dataY) &&
        pyopencv_to(pyobj_dataX, dataX, ArgInfo("dataX", 0)) &&
        pyopencv_to(pyobj_dataY, dataY, ArgInfo("dataY", 0)) )
    {
        ERRWRAP2(retval = cv::plot::Plot2d::create(dataX, dataY));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_render(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    {
    PyObject* pyobj__plotResult = NULL;
    Mat _plotResult;

    const char* keywords[] = { "_plotResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:plot_Plot2d.render", (char**)keywords, &pyobj__plotResult) &&
        pyopencv_to(pyobj__plotResult, _plotResult, ArgInfo("_plotResult", 1)) )
    {
        ERRWRAP2(_self_->render(_plotResult));
        return pyopencv_from(_plotResult);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__plotResult = NULL;
    UMat _plotResult;

    const char* keywords[] = { "_plotResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:plot_Plot2d.render", (char**)keywords, &pyobj__plotResult) &&
        pyopencv_to(pyobj__plotResult, _plotResult, ArgInfo("_plotResult", 1)) )
    {
        ERRWRAP2(_self_->render(_plotResult));
        return pyopencv_from(_plotResult);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setGridLinesNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    int gridLinesNumber=0;

    const char* keywords[] = { "gridLinesNumber", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:plot_Plot2d.setGridLinesNumber", (char**)keywords, &gridLinesNumber) )
    {
        ERRWRAP2(_self_->setGridLinesNumber(gridLinesNumber));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setInvertOrientation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    bool _invertOrientation=0;

    const char* keywords[] = { "_invertOrientation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:plot_Plot2d.setInvertOrientation", (char**)keywords, &_invertOrientation) )
    {
        ERRWRAP2(_self_->setInvertOrientation(_invertOrientation));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setMaxX(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    double _plotMaxX=0;

    const char* keywords[] = { "_plotMaxX", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:plot_Plot2d.setMaxX", (char**)keywords, &_plotMaxX) )
    {
        ERRWRAP2(_self_->setMaxX(_plotMaxX));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setMaxY(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    double _plotMaxY=0;

    const char* keywords[] = { "_plotMaxY", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:plot_Plot2d.setMaxY", (char**)keywords, &_plotMaxY) )
    {
        ERRWRAP2(_self_->setMaxY(_plotMaxY));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setMinX(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    double _plotMinX=0;

    const char* keywords[] = { "_plotMinX", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:plot_Plot2d.setMinX", (char**)keywords, &_plotMinX) )
    {
        ERRWRAP2(_self_->setMinX(_plotMinX));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setMinY(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    double _plotMinY=0;

    const char* keywords[] = { "_plotMinY", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:plot_Plot2d.setMinY", (char**)keywords, &_plotMinY) )
    {
        ERRWRAP2(_self_->setMinY(_plotMinY));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setNeedPlotLine(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    bool _needPlotLine=0;

    const char* keywords[] = { "_needPlotLine", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:plot_Plot2d.setNeedPlotLine", (char**)keywords, &_needPlotLine) )
    {
        ERRWRAP2(_self_->setNeedPlotLine(_needPlotLine));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotAxisColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    PyObject* pyobj__plotAxisColor = NULL;
    Scalar _plotAxisColor;

    const char* keywords[] = { "_plotAxisColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.setPlotAxisColor", (char**)keywords, &pyobj__plotAxisColor) &&
        pyopencv_to(pyobj__plotAxisColor, _plotAxisColor, ArgInfo("_plotAxisColor", 0)) )
    {
        ERRWRAP2(_self_->setPlotAxisColor(_plotAxisColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotBackgroundColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    PyObject* pyobj__plotBackgroundColor = NULL;
    Scalar _plotBackgroundColor;

    const char* keywords[] = { "_plotBackgroundColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.setPlotBackgroundColor", (char**)keywords, &pyobj__plotBackgroundColor) &&
        pyopencv_to(pyobj__plotBackgroundColor, _plotBackgroundColor, ArgInfo("_plotBackgroundColor", 0)) )
    {
        ERRWRAP2(_self_->setPlotBackgroundColor(_plotBackgroundColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotGridColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    PyObject* pyobj__plotGridColor = NULL;
    Scalar _plotGridColor;

    const char* keywords[] = { "_plotGridColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.setPlotGridColor", (char**)keywords, &pyobj__plotGridColor) &&
        pyopencv_to(pyobj__plotGridColor, _plotGridColor, ArgInfo("_plotGridColor", 0)) )
    {
        ERRWRAP2(_self_->setPlotGridColor(_plotGridColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotLineColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    PyObject* pyobj__plotLineColor = NULL;
    Scalar _plotLineColor;

    const char* keywords[] = { "_plotLineColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.setPlotLineColor", (char**)keywords, &pyobj__plotLineColor) &&
        pyopencv_to(pyobj__plotLineColor, _plotLineColor, ArgInfo("_plotLineColor", 0)) )
    {
        ERRWRAP2(_self_->setPlotLineColor(_plotLineColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotLineWidth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    int _plotLineWidth=0;

    const char* keywords[] = { "_plotLineWidth", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:plot_Plot2d.setPlotLineWidth", (char**)keywords, &_plotLineWidth) )
    {
        ERRWRAP2(_self_->setPlotLineWidth(_plotLineWidth));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    int _plotSizeWidth=0;
    int _plotSizeHeight=0;

    const char* keywords[] = { "_plotSizeWidth", "_plotSizeHeight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:plot_Plot2d.setPlotSize", (char**)keywords, &_plotSizeWidth, &_plotSizeHeight) )
    {
        ERRWRAP2(_self_->setPlotSize(_plotSizeWidth, _plotSizeHeight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPlotTextColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    PyObject* pyobj__plotTextColor = NULL;
    Scalar _plotTextColor;

    const char* keywords[] = { "_plotTextColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:plot_Plot2d.setPlotTextColor", (char**)keywords, &pyobj__plotTextColor) &&
        pyopencv_to(pyobj__plotTextColor, _plotTextColor, ArgInfo("_plotTextColor", 0)) )
    {
        ERRWRAP2(_self_->setPlotTextColor(_plotTextColor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setPointIdxToPrint(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    int pointIdx=0;

    const char* keywords[] = { "pointIdx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:plot_Plot2d.setPointIdxToPrint", (char**)keywords, &pointIdx) )
    {
        ERRWRAP2(_self_->setPointIdxToPrint(pointIdx));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setShowGrid(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    bool needShowGrid=0;

    const char* keywords[] = { "needShowGrid", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:plot_Plot2d.setShowGrid", (char**)keywords, &needShowGrid) )
    {
        ERRWRAP2(_self_->setShowGrid(needShowGrid));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_plot_plot_Plot2d_setShowText(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::plot;

    cv::plot::Plot2d* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_plot_Plot2d_Type))
        _self_ = dynamic_cast<cv::plot::Plot2d*>(((pyopencv_plot_Plot2d_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'plot_Plot2d' or its derivative)");
    bool needShowText=0;

    const char* keywords[] = { "needShowText", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:plot_Plot2d.setShowText", (char**)keywords, &needShowText) )
    {
        ERRWRAP2(_self_->setShowText(needShowText));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_plot_Plot2d_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_create_cls, METH_CLASS), "create(data) -> retval\n.   * @brief Creates Plot2d object\n.   *\n.   * @param data \\f$1xN\\f$ or \\f$Nx1\\f$ matrix containing \\f$Y\\f$ values of points to plot. \\f$X\\f$ values\n.   * will be equal to indexes of correspondind elements in data matrix.\n\n\n\ncreate(dataX, dataY) -> retval\n.   * @brief Creates Plot2d object\n.   *\n.   * @param dataX \\f$1xN\\f$ or \\f$Nx1\\f$ matrix \\f$X\\f$ values of points to plot.\n.   * @param dataY \\f$1xN\\f$ or \\f$Nx1\\f$ matrix containing \\f$Y\\f$ values of points to plot."},
    {"render", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_render, 0), "render([, _plotResult]) -> _plotResult\n."},
    {"setGridLinesNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setGridLinesNumber, 0), "setGridLinesNumber(gridLinesNumber) -> None\n."},
    {"setInvertOrientation", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setInvertOrientation, 0), "setInvertOrientation(_invertOrientation) -> None\n."},
    {"setMaxX", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMaxX, 0), "setMaxX(_plotMaxX) -> None\n."},
    {"setMaxY", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMaxY, 0), "setMaxY(_plotMaxY) -> None\n."},
    {"setMinX", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMinX, 0), "setMinX(_plotMinX) -> None\n."},
    {"setMinY", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setMinY, 0), "setMinY(_plotMinY) -> None\n."},
    {"setNeedPlotLine", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setNeedPlotLine, 0), "setNeedPlotLine(_needPlotLine) -> None\n.   * @brief Switches data visualization mode\n.   *\n.   * @param _needPlotLine if true then neighbour plot points will be connected by lines.\n.   * In other case data will be plotted as a set of standalone points."},
    {"setPlotAxisColor", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotAxisColor, 0), "setPlotAxisColor(_plotAxisColor) -> None\n."},
    {"setPlotBackgroundColor", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotBackgroundColor, 0), "setPlotBackgroundColor(_plotBackgroundColor) -> None\n."},
    {"setPlotGridColor", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotGridColor, 0), "setPlotGridColor(_plotGridColor) -> None\n."},
    {"setPlotLineColor", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotLineColor, 0), "setPlotLineColor(_plotLineColor) -> None\n."},
    {"setPlotLineWidth", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotLineWidth, 0), "setPlotLineWidth(_plotLineWidth) -> None\n."},
    {"setPlotSize", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotSize, 0), "setPlotSize(_plotSizeWidth, _plotSizeHeight) -> None\n."},
    {"setPlotTextColor", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPlotTextColor, 0), "setPlotTextColor(_plotTextColor) -> None\n."},
    {"setPointIdxToPrint", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setPointIdxToPrint, 0), "setPointIdxToPrint(pointIdx) -> None\n.   * @brief Sets the index of a point which coordinates will be printed on the top left corner of the plot (if ShowText flag is true).\n.   *\n.   * @param pointIdx index of the required point in data array."},
    {"setShowGrid", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setShowGrid, 0), "setShowGrid(needShowGrid) -> None\n."},
    {"setShowText", CV_PY_FN_WITH_KW_(pyopencv_cv_plot_plot_Plot2d_setShowText, 0), "setShowText(needShowText) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_plot_Plot2d_specials(void)
{
    pyopencv_plot_Plot2d_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_plot_Plot2d_Type.tp_dealloc = pyopencv_plot_Plot2d_dealloc;
    pyopencv_plot_Plot2d_Type.tp_repr = pyopencv_plot_Plot2d_repr;
    pyopencv_plot_Plot2d_Type.tp_getset = pyopencv_plot_Plot2d_getseters;
    pyopencv_plot_Plot2d_Type.tp_init = (initproc)0;
    pyopencv_plot_Plot2d_Type.tp_methods = pyopencv_plot_Plot2d_methods;
}

static PyObject* pyopencv_reg_Map_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_Map %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_Map_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_reg_reg_Map_compose(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Map* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Map_Type))
        _self_ = ((pyopencv_reg_Map_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Map' or its derivative)");
    PyObject* pyobj_map = NULL;
    Ptr<Map> map;

    const char* keywords[] = { "map", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_Map.compose", (char**)keywords, &pyobj_map) &&
        pyopencv_to(pyobj_map, map, ArgInfo("map", 0)) )
    {
        ERRWRAP2(_self_->compose(map));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_Map_inverseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Map* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Map_Type))
        _self_ = ((pyopencv_reg_Map_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Map' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->inverseMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_Map_inverseWarp(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Map* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Map_Type))
        _self_ = ((pyopencv_reg_Map_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Map' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_Map.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_Map.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_Map_scale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Map* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Map_Type))
        _self_ = ((pyopencv_reg_Map_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Map' or its derivative)");
    double factor=0;

    const char* keywords[] = { "factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:reg_Map.scale", (char**)keywords, &factor) )
    {
        ERRWRAP2(_self_->scale(factor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_Map_warp(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Map* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Map_Type))
        _self_ = ((pyopencv_reg_Map_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Map' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_Map.warp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->warp(img1, img2));
        return pyopencv_from(img2);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_Map.warp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->warp(img1, img2));
        return pyopencv_from(img2);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_Map_methods[] =
{
    {"compose", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Map_compose, 0), "compose(map) -> None\n."},
    {"inverseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Map_inverseMap, 0), "inverseMap() -> retval\n."},
    {"inverseWarp", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Map_inverseWarp, 0), "inverseWarp(img1[, img2]) -> img2\n."},
    {"scale", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Map_scale, 0), "scale(factor) -> None\n."},
    {"warp", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Map_warp, 0), "warp(img1[, img2]) -> img2\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_Map_specials(void)
{
    pyopencv_reg_Map_Type.tp_base = NULL;
    pyopencv_reg_Map_Type.tp_dealloc = pyopencv_reg_Map_dealloc;
    pyopencv_reg_Map_Type.tp_repr = pyopencv_reg_Map_repr;
    pyopencv_reg_Map_Type.tp_getset = pyopencv_reg_Map_getseters;
    pyopencv_reg_Map_Type.tp_init = (initproc)0;
    pyopencv_reg_Map_Type.tp_methods = pyopencv_reg_Map_methods;
}

static PyObject* pyopencv_reg_MapAffine_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapAffine %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapAffine_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapAffine_MapAffine(pyopencv_reg_MapAffine_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapAffine>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapAffine()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_linTr = NULL;
    Mat linTr;
    PyObject* pyobj_shift = NULL;
    Mat shift;

    const char* keywords[] = { "linTr", "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:MapAffine", (char**)keywords, &pyobj_linTr, &pyobj_shift) &&
        pyopencv_to(pyobj_linTr, linTr, ArgInfo("linTr", 0)) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapAffine>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapAffine(linTr, shift)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_linTr = NULL;
    UMat linTr;
    PyObject* pyobj_shift = NULL;
    UMat shift;

    const char* keywords[] = { "linTr", "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:MapAffine", (char**)keywords, &pyobj_linTr, &pyobj_shift) &&
        pyopencv_to(pyobj_linTr, linTr, ArgInfo("linTr", 0)) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapAffine>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapAffine(linTr, shift)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapAffine_compose(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapAffine_Type))
        _self_ = ((pyopencv_reg_MapAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapAffine' or its derivative)");
    PyObject* pyobj_map = NULL;
    Ptr<Map> map;

    const char* keywords[] = { "map", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_MapAffine.compose", (char**)keywords, &pyobj_map) &&
        pyopencv_to(pyobj_map, map, ArgInfo("map", 0)) )
    {
        ERRWRAP2(_self_->compose(map));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapAffine_getLinTr(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapAffine_Type))
        _self_ = ((pyopencv_reg_MapAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapAffine' or its derivative)");
    {
    PyObject* pyobj_linTr = NULL;
    Mat linTr;

    const char* keywords[] = { "linTr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapAffine.getLinTr", (char**)keywords, &pyobj_linTr) &&
        pyopencv_to(pyobj_linTr, linTr, ArgInfo("linTr", 1)) )
    {
        ERRWRAP2(_self_->getLinTr(linTr));
        return pyopencv_from(linTr);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_linTr = NULL;
    UMat linTr;

    const char* keywords[] = { "linTr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapAffine.getLinTr", (char**)keywords, &pyobj_linTr) &&
        pyopencv_to(pyobj_linTr, linTr, ArgInfo("linTr", 1)) )
    {
        ERRWRAP2(_self_->getLinTr(linTr));
        return pyopencv_from(linTr);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapAffine_getShift(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapAffine_Type))
        _self_ = ((pyopencv_reg_MapAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapAffine' or its derivative)");
    {
    PyObject* pyobj_shift = NULL;
    Mat shift;

    const char* keywords[] = { "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapAffine.getShift", (char**)keywords, &pyobj_shift) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 1)) )
    {
        ERRWRAP2(_self_->getShift(shift));
        return pyopencv_from(shift);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_shift = NULL;
    UMat shift;

    const char* keywords[] = { "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapAffine.getShift", (char**)keywords, &pyobj_shift) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 1)) )
    {
        ERRWRAP2(_self_->getShift(shift));
        return pyopencv_from(shift);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapAffine_inverseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapAffine_Type))
        _self_ = ((pyopencv_reg_MapAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapAffine' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->inverseMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapAffine_inverseWarp(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapAffine_Type))
        _self_ = ((pyopencv_reg_MapAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapAffine' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_MapAffine.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_MapAffine.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapAffine_scale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapAffine_Type))
        _self_ = ((pyopencv_reg_MapAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapAffine' or its derivative)");
    double factor=0;

    const char* keywords[] = { "factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:reg_MapAffine.scale", (char**)keywords, &factor) )
    {
        ERRWRAP2(_self_->scale(factor));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapAffine_methods[] =
{
    {"compose", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapAffine_compose, 0), "compose(map) -> None\n."},
    {"getLinTr", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapAffine_getLinTr, 0), "getLinTr([, linTr]) -> linTr\n."},
    {"getShift", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapAffine_getShift, 0), "getShift([, shift]) -> shift\n."},
    {"inverseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapAffine_inverseMap, 0), "inverseMap() -> retval\n."},
    {"inverseWarp", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapAffine_inverseWarp, 0), "inverseWarp(img1[, img2]) -> img2\n."},
    {"scale", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapAffine_scale, 0), "scale(factor) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapAffine_specials(void)
{
    pyopencv_reg_MapAffine_Type.tp_base = &pyopencv_reg_Map_Type;
    pyopencv_reg_MapAffine_Type.tp_dealloc = pyopencv_reg_MapAffine_dealloc;
    pyopencv_reg_MapAffine_Type.tp_repr = pyopencv_reg_MapAffine_repr;
    pyopencv_reg_MapAffine_Type.tp_getset = pyopencv_reg_MapAffine_getseters;
    pyopencv_reg_MapAffine_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapAffine_MapAffine;
    pyopencv_reg_MapAffine_Type.tp_methods = pyopencv_reg_MapAffine_methods;
}

static PyObject* pyopencv_reg_Mapper_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_Mapper %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_Mapper_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_reg_reg_Mapper_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Mapper* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Mapper_Type))
        _self_ = ((pyopencv_reg_Mapper_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Mapper' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_Mapper.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_Mapper.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_Mapper_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::Mapper* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_Mapper_Type))
        _self_ = ((pyopencv_reg_Mapper_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_Mapper' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_Mapper_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Mapper_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_Mapper_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_Mapper_specials(void)
{
    pyopencv_reg_Mapper_Type.tp_base = NULL;
    pyopencv_reg_Mapper_Type.tp_dealloc = pyopencv_reg_Mapper_dealloc;
    pyopencv_reg_Mapper_Type.tp_repr = pyopencv_reg_Mapper_repr;
    pyopencv_reg_Mapper_Type.tp_getset = pyopencv_reg_Mapper_getseters;
    pyopencv_reg_Mapper_Type.tp_init = (initproc)0;
    pyopencv_reg_Mapper_Type.tp_methods = pyopencv_reg_Mapper_methods;
}

static PyObject* pyopencv_reg_MapperGradAffine_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapperGradAffine %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapperGradAffine_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapperGradAffine_MapperGradAffine(pyopencv_reg_MapperGradAffine_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapperGradAffine>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapperGradAffine()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradAffine_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradAffine_Type))
        _self_ = ((pyopencv_reg_MapperGradAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradAffine' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradAffine.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradAffine.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradAffine_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradAffine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradAffine_Type))
        _self_ = ((pyopencv_reg_MapperGradAffine_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradAffine' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapperGradAffine_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradAffine_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradAffine_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapperGradAffine_specials(void)
{
    pyopencv_reg_MapperGradAffine_Type.tp_base = &pyopencv_reg_Mapper_Type;
    pyopencv_reg_MapperGradAffine_Type.tp_dealloc = pyopencv_reg_MapperGradAffine_dealloc;
    pyopencv_reg_MapperGradAffine_Type.tp_repr = pyopencv_reg_MapperGradAffine_repr;
    pyopencv_reg_MapperGradAffine_Type.tp_getset = pyopencv_reg_MapperGradAffine_getseters;
    pyopencv_reg_MapperGradAffine_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapperGradAffine_MapperGradAffine;
    pyopencv_reg_MapperGradAffine_Type.tp_methods = pyopencv_reg_MapperGradAffine_methods;
}

static PyObject* pyopencv_reg_MapperGradEuclid_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapperGradEuclid %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapperGradEuclid_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapperGradEuclid_MapperGradEuclid(pyopencv_reg_MapperGradEuclid_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapperGradEuclid>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapperGradEuclid()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradEuclid_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradEuclid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradEuclid_Type))
        _self_ = ((pyopencv_reg_MapperGradEuclid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradEuclid' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradEuclid.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradEuclid.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradEuclid_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradEuclid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradEuclid_Type))
        _self_ = ((pyopencv_reg_MapperGradEuclid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradEuclid' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapperGradEuclid_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradEuclid_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradEuclid_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapperGradEuclid_specials(void)
{
    pyopencv_reg_MapperGradEuclid_Type.tp_base = &pyopencv_reg_Mapper_Type;
    pyopencv_reg_MapperGradEuclid_Type.tp_dealloc = pyopencv_reg_MapperGradEuclid_dealloc;
    pyopencv_reg_MapperGradEuclid_Type.tp_repr = pyopencv_reg_MapperGradEuclid_repr;
    pyopencv_reg_MapperGradEuclid_Type.tp_getset = pyopencv_reg_MapperGradEuclid_getseters;
    pyopencv_reg_MapperGradEuclid_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapperGradEuclid_MapperGradEuclid;
    pyopencv_reg_MapperGradEuclid_Type.tp_methods = pyopencv_reg_MapperGradEuclid_methods;
}

static PyObject* pyopencv_reg_MapperGradProj_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapperGradProj %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapperGradProj_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapperGradProj_MapperGradProj(pyopencv_reg_MapperGradProj_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapperGradProj>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapperGradProj()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradProj_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradProj* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradProj_Type))
        _self_ = ((pyopencv_reg_MapperGradProj_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradProj' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradProj.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradProj.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradProj_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradProj* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradProj_Type))
        _self_ = ((pyopencv_reg_MapperGradProj_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradProj' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapperGradProj_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradProj_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradProj_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapperGradProj_specials(void)
{
    pyopencv_reg_MapperGradProj_Type.tp_base = &pyopencv_reg_Mapper_Type;
    pyopencv_reg_MapperGradProj_Type.tp_dealloc = pyopencv_reg_MapperGradProj_dealloc;
    pyopencv_reg_MapperGradProj_Type.tp_repr = pyopencv_reg_MapperGradProj_repr;
    pyopencv_reg_MapperGradProj_Type.tp_getset = pyopencv_reg_MapperGradProj_getseters;
    pyopencv_reg_MapperGradProj_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapperGradProj_MapperGradProj;
    pyopencv_reg_MapperGradProj_Type.tp_methods = pyopencv_reg_MapperGradProj_methods;
}

static PyObject* pyopencv_reg_MapperGradShift_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapperGradShift %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapperGradShift_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapperGradShift_MapperGradShift(pyopencv_reg_MapperGradShift_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapperGradShift>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapperGradShift()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradShift_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradShift_Type))
        _self_ = ((pyopencv_reg_MapperGradShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradShift' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradShift.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradShift.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradShift_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradShift_Type))
        _self_ = ((pyopencv_reg_MapperGradShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradShift' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapperGradShift_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradShift_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradShift_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapperGradShift_specials(void)
{
    pyopencv_reg_MapperGradShift_Type.tp_base = &pyopencv_reg_Mapper_Type;
    pyopencv_reg_MapperGradShift_Type.tp_dealloc = pyopencv_reg_MapperGradShift_dealloc;
    pyopencv_reg_MapperGradShift_Type.tp_repr = pyopencv_reg_MapperGradShift_repr;
    pyopencv_reg_MapperGradShift_Type.tp_getset = pyopencv_reg_MapperGradShift_getseters;
    pyopencv_reg_MapperGradShift_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapperGradShift_MapperGradShift;
    pyopencv_reg_MapperGradShift_Type.tp_methods = pyopencv_reg_MapperGradShift_methods;
}

static PyObject* pyopencv_reg_MapperGradSimilar_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapperGradSimilar %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapperGradSimilar_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapperGradSimilar_MapperGradSimilar(pyopencv_reg_MapperGradSimilar_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapperGradSimilar>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapperGradSimilar()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradSimilar_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradSimilar* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradSimilar_Type))
        _self_ = ((pyopencv_reg_MapperGradSimilar_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradSimilar' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradSimilar.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperGradSimilar.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapperGradSimilar_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperGradSimilar* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperGradSimilar_Type))
        _self_ = ((pyopencv_reg_MapperGradSimilar_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperGradSimilar' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapperGradSimilar_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradSimilar_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperGradSimilar_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapperGradSimilar_specials(void)
{
    pyopencv_reg_MapperGradSimilar_Type.tp_base = &pyopencv_reg_Mapper_Type;
    pyopencv_reg_MapperGradSimilar_Type.tp_dealloc = pyopencv_reg_MapperGradSimilar_dealloc;
    pyopencv_reg_MapperGradSimilar_Type.tp_repr = pyopencv_reg_MapperGradSimilar_repr;
    pyopencv_reg_MapperGradSimilar_Type.tp_getset = pyopencv_reg_MapperGradSimilar_getseters;
    pyopencv_reg_MapperGradSimilar_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapperGradSimilar_MapperGradSimilar;
    pyopencv_reg_MapperGradSimilar_Type.tp_methods = pyopencv_reg_MapperGradSimilar_methods;
}

static PyObject* pyopencv_reg_MapperPyramid_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapperPyramid %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_reg_MapperPyramid_get_numIterPerScale_(pyopencv_reg_MapperPyramid_t* p, void *closure)
{
    return pyopencv_from(p->v->numIterPerScale_);
}

static int pyopencv_reg_MapperPyramid_set_numIterPerScale_(pyopencv_reg_MapperPyramid_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the numIterPerScale_ attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->numIterPerScale_) ? 0 : -1;
}

static PyObject* pyopencv_reg_MapperPyramid_get_numLev_(pyopencv_reg_MapperPyramid_t* p, void *closure)
{
    return pyopencv_from(p->v->numLev_);
}

static int pyopencv_reg_MapperPyramid_set_numLev_(pyopencv_reg_MapperPyramid_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the numLev_ attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->numLev_) ? 0 : -1;
}


static PyGetSetDef pyopencv_reg_MapperPyramid_getseters[] =
{
    {(char*)"numIterPerScale_", (getter)pyopencv_reg_MapperPyramid_get_numIterPerScale_, (setter)pyopencv_reg_MapperPyramid_set_numIterPerScale_, (char*)"numIterPerScale_", NULL},
    {(char*)"numLev_", (getter)pyopencv_reg_MapperPyramid_get_numLev_, (setter)pyopencv_reg_MapperPyramid_set_numLev_, (char*)"numLev_", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapperPyramid_MapperPyramid(pyopencv_reg_MapperPyramid_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    PyObject* pyobj_baseMapper = NULL;
    Ptr<Mapper> baseMapper;

    const char* keywords[] = { "baseMapper", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MapperPyramid", (char**)keywords, &pyobj_baseMapper) &&
        pyopencv_to(pyobj_baseMapper, baseMapper, ArgInfo("baseMapper", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapperPyramid>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapperPyramid(baseMapper)));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapperPyramid_calculate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperPyramid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperPyramid_Type))
        _self_ = ((pyopencv_reg_MapperPyramid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperPyramid' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperPyramid.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;
    PyObject* pyobj_init = NULL;
    Ptr<Map> init=cv::Ptr<Map>();
    cv::Ptr<Map> retval;

    const char* keywords[] = { "img1", "img2", "init", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:reg_MapperPyramid.calculate", (char**)keywords, &pyobj_img1, &pyobj_img2, &pyobj_init) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 0)) &&
        pyopencv_to(pyobj_init, init, ArgInfo("init", 0)) )
    {
        ERRWRAP2(retval = _self_->calculate(img1, img2, init));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapperPyramid_getMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapperPyramid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapperPyramid_Type))
        _self_ = ((pyopencv_reg_MapperPyramid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapperPyramid' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMap());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapperPyramid_methods[] =
{
    {"calculate", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperPyramid_calculate, 0), "calculate(img1, img2[, init]) -> retval\n."},
    {"getMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapperPyramid_getMap, 0), "getMap() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapperPyramid_specials(void)
{
    pyopencv_reg_MapperPyramid_Type.tp_base = &pyopencv_reg_Mapper_Type;
    pyopencv_reg_MapperPyramid_Type.tp_dealloc = pyopencv_reg_MapperPyramid_dealloc;
    pyopencv_reg_MapperPyramid_Type.tp_repr = pyopencv_reg_MapperPyramid_repr;
    pyopencv_reg_MapperPyramid_Type.tp_getset = pyopencv_reg_MapperPyramid_getseters;
    pyopencv_reg_MapperPyramid_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapperPyramid_MapperPyramid;
    pyopencv_reg_MapperPyramid_Type.tp_methods = pyopencv_reg_MapperPyramid_methods;
}

static PyObject* pyopencv_reg_MapTypeCaster_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapTypeCaster %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapTypeCaster_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_reg_reg_MapTypeCaster_toAffine_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    PyObject* pyobj_sourceMap = NULL;
    Ptr<Map> sourceMap;
    Ptr<MapAffine> retval;

    const char* keywords[] = { "sourceMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_MapTypeCaster.toAffine", (char**)keywords, &pyobj_sourceMap) &&
        pyopencv_to(pyobj_sourceMap, sourceMap, ArgInfo("sourceMap", 0)) )
    {
        ERRWRAP2(retval = cv::reg::MapTypeCaster::toAffine(sourceMap));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapTypeCaster_toProjec_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    PyObject* pyobj_sourceMap = NULL;
    Ptr<Map> sourceMap;
    Ptr<MapProjec> retval;

    const char* keywords[] = { "sourceMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_MapTypeCaster.toProjec", (char**)keywords, &pyobj_sourceMap) &&
        pyopencv_to(pyobj_sourceMap, sourceMap, ArgInfo("sourceMap", 0)) )
    {
        ERRWRAP2(retval = cv::reg::MapTypeCaster::toProjec(sourceMap));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapTypeCaster_toShift_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    PyObject* pyobj_sourceMap = NULL;
    Ptr<Map> sourceMap;
    Ptr<MapShift> retval;

    const char* keywords[] = { "sourceMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_MapTypeCaster.toShift", (char**)keywords, &pyobj_sourceMap) &&
        pyopencv_to(pyobj_sourceMap, sourceMap, ArgInfo("sourceMap", 0)) )
    {
        ERRWRAP2(retval = cv::reg::MapTypeCaster::toShift(sourceMap));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapTypeCaster_methods[] =
{
    {"toAffine", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapTypeCaster_toAffine_cls, METH_CLASS), "toAffine(sourceMap) -> retval\n."},
    {"toProjec", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapTypeCaster_toProjec_cls, METH_CLASS), "toProjec(sourceMap) -> retval\n."},
    {"toShift", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapTypeCaster_toShift_cls, METH_CLASS), "toShift(sourceMap) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapTypeCaster_specials(void)
{
    pyopencv_reg_MapTypeCaster_Type.tp_base = NULL;
    pyopencv_reg_MapTypeCaster_Type.tp_dealloc = pyopencv_reg_MapTypeCaster_dealloc;
    pyopencv_reg_MapTypeCaster_Type.tp_repr = pyopencv_reg_MapTypeCaster_repr;
    pyopencv_reg_MapTypeCaster_Type.tp_getset = pyopencv_reg_MapTypeCaster_getseters;
    pyopencv_reg_MapTypeCaster_Type.tp_init = (initproc)0;
    pyopencv_reg_MapTypeCaster_Type.tp_methods = pyopencv_reg_MapTypeCaster_methods;
}

static PyObject* pyopencv_reg_MapProjec_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapProjec %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapProjec_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapProjec_MapProjec(pyopencv_reg_MapProjec_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapProjec>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapProjec()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_projTr = NULL;
    Mat projTr;

    const char* keywords[] = { "projTr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MapProjec", (char**)keywords, &pyobj_projTr) &&
        pyopencv_to(pyobj_projTr, projTr, ArgInfo("projTr", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapProjec>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapProjec(projTr)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_projTr = NULL;
    UMat projTr;

    const char* keywords[] = { "projTr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MapProjec", (char**)keywords, &pyobj_projTr) &&
        pyopencv_to(pyobj_projTr, projTr, ArgInfo("projTr", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapProjec>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapProjec(projTr)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapProjec_compose(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapProjec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapProjec_Type))
        _self_ = ((pyopencv_reg_MapProjec_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapProjec' or its derivative)");
    PyObject* pyobj_map = NULL;
    Ptr<Map> map;

    const char* keywords[] = { "map", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_MapProjec.compose", (char**)keywords, &pyobj_map) &&
        pyopencv_to(pyobj_map, map, ArgInfo("map", 0)) )
    {
        ERRWRAP2(_self_->compose(map));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapProjec_getProjTr(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapProjec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapProjec_Type))
        _self_ = ((pyopencv_reg_MapProjec_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapProjec' or its derivative)");
    {
    PyObject* pyobj_projTr = NULL;
    Mat projTr;

    const char* keywords[] = { "projTr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapProjec.getProjTr", (char**)keywords, &pyobj_projTr) &&
        pyopencv_to(pyobj_projTr, projTr, ArgInfo("projTr", 1)) )
    {
        ERRWRAP2(_self_->getProjTr(projTr));
        return pyopencv_from(projTr);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_projTr = NULL;
    UMat projTr;

    const char* keywords[] = { "projTr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapProjec.getProjTr", (char**)keywords, &pyobj_projTr) &&
        pyopencv_to(pyobj_projTr, projTr, ArgInfo("projTr", 1)) )
    {
        ERRWRAP2(_self_->getProjTr(projTr));
        return pyopencv_from(projTr);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapProjec_inverseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapProjec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapProjec_Type))
        _self_ = ((pyopencv_reg_MapProjec_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapProjec' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->inverseMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapProjec_inverseWarp(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapProjec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapProjec_Type))
        _self_ = ((pyopencv_reg_MapProjec_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapProjec' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_MapProjec.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_MapProjec.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapProjec_normalize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapProjec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapProjec_Type))
        _self_ = ((pyopencv_reg_MapProjec_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapProjec' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->normalize());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapProjec_scale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapProjec* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapProjec_Type))
        _self_ = ((pyopencv_reg_MapProjec_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapProjec' or its derivative)");
    double factor=0;

    const char* keywords[] = { "factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:reg_MapProjec.scale", (char**)keywords, &factor) )
    {
        ERRWRAP2(_self_->scale(factor));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapProjec_methods[] =
{
    {"compose", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapProjec_compose, 0), "compose(map) -> None\n."},
    {"getProjTr", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapProjec_getProjTr, 0), "getProjTr([, projTr]) -> projTr\n."},
    {"inverseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapProjec_inverseMap, 0), "inverseMap() -> retval\n."},
    {"inverseWarp", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapProjec_inverseWarp, 0), "inverseWarp(img1[, img2]) -> img2\n."},
    {"normalize", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapProjec_normalize, 0), "normalize() -> None\n."},
    {"scale", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapProjec_scale, 0), "scale(factor) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapProjec_specials(void)
{
    pyopencv_reg_MapProjec_Type.tp_base = &pyopencv_reg_Map_Type;
    pyopencv_reg_MapProjec_Type.tp_dealloc = pyopencv_reg_MapProjec_dealloc;
    pyopencv_reg_MapProjec_Type.tp_repr = pyopencv_reg_MapProjec_repr;
    pyopencv_reg_MapProjec_Type.tp_getset = pyopencv_reg_MapProjec_getseters;
    pyopencv_reg_MapProjec_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapProjec_MapProjec;
    pyopencv_reg_MapProjec_Type.tp_methods = pyopencv_reg_MapProjec_methods;
}

static PyObject* pyopencv_reg_MapShift_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<reg_MapShift %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_reg_MapShift_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_reg_reg_MapShift_MapShift(pyopencv_reg_MapShift_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::reg::MapShift>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapShift()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_shift = NULL;
    Mat shift;

    const char* keywords[] = { "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MapShift", (char**)keywords, &pyobj_shift) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapShift>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapShift(shift)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_shift = NULL;
    UMat shift;

    const char* keywords[] = { "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MapShift", (char**)keywords, &pyobj_shift) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 0)) )
    {
        new (&(self->v)) Ptr<cv::reg::MapShift>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::reg::MapShift(shift)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_reg_reg_MapShift_compose(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapShift_Type))
        _self_ = ((pyopencv_reg_MapShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapShift' or its derivative)");
    PyObject* pyobj_map = NULL;
    Ptr<Map> map;

    const char* keywords[] = { "map", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:reg_MapShift.compose", (char**)keywords, &pyobj_map) &&
        pyopencv_to(pyobj_map, map, ArgInfo("map", 0)) )
    {
        ERRWRAP2(_self_->compose(map));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapShift_getShift(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapShift_Type))
        _self_ = ((pyopencv_reg_MapShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapShift' or its derivative)");
    {
    PyObject* pyobj_shift = NULL;
    Mat shift;

    const char* keywords[] = { "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapShift.getShift", (char**)keywords, &pyobj_shift) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 1)) )
    {
        ERRWRAP2(_self_->getShift(shift));
        return pyopencv_from(shift);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_shift = NULL;
    UMat shift;

    const char* keywords[] = { "shift", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:reg_MapShift.getShift", (char**)keywords, &pyobj_shift) &&
        pyopencv_to(pyobj_shift, shift, ArgInfo("shift", 1)) )
    {
        ERRWRAP2(_self_->getShift(shift));
        return pyopencv_from(shift);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapShift_inverseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapShift_Type))
        _self_ = ((pyopencv_reg_MapShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapShift' or its derivative)");
    cv::Ptr<Map> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->inverseMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapShift_inverseWarp(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapShift_Type))
        _self_ = ((pyopencv_reg_MapShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapShift' or its derivative)");
    {
    PyObject* pyobj_img1 = NULL;
    Mat img1;
    PyObject* pyobj_img2 = NULL;
    Mat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_MapShift.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img1 = NULL;
    UMat img1;
    PyObject* pyobj_img2 = NULL;
    UMat img2;

    const char* keywords[] = { "img1", "img2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:reg_MapShift.inverseWarp", (char**)keywords, &pyobj_img1, &pyobj_img2) &&
        pyopencv_to(pyobj_img1, img1, ArgInfo("img1", 0)) &&
        pyopencv_to(pyobj_img2, img2, ArgInfo("img2", 1)) )
    {
        ERRWRAP2(_self_->inverseWarp(img1, img2));
        return pyopencv_from(img2);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_reg_reg_MapShift_scale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::reg;

    cv::reg::MapShift* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_reg_MapShift_Type))
        _self_ = ((pyopencv_reg_MapShift_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'reg_MapShift' or its derivative)");
    double factor=0;

    const char* keywords[] = { "factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:reg_MapShift.scale", (char**)keywords, &factor) )
    {
        ERRWRAP2(_self_->scale(factor));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_reg_MapShift_methods[] =
{
    {"compose", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapShift_compose, 0), "compose(map) -> None\n."},
    {"getShift", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapShift_getShift, 0), "getShift([, shift]) -> shift\n."},
    {"inverseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapShift_inverseMap, 0), "inverseMap() -> retval\n."},
    {"inverseWarp", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapShift_inverseWarp, 0), "inverseWarp(img1[, img2]) -> img2\n."},
    {"scale", CV_PY_FN_WITH_KW_(pyopencv_cv_reg_reg_MapShift_scale, 0), "scale(factor) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_reg_MapShift_specials(void)
{
    pyopencv_reg_MapShift_Type.tp_base = &pyopencv_reg_Map_Type;
    pyopencv_reg_MapShift_Type.tp_dealloc = pyopencv_reg_MapShift_dealloc;
    pyopencv_reg_MapShift_Type.tp_repr = pyopencv_reg_MapShift_repr;
    pyopencv_reg_MapShift_Type.tp_getset = pyopencv_reg_MapShift_getseters;
    pyopencv_reg_MapShift_Type.tp_init = (initproc)pyopencv_cv_reg_reg_MapShift_MapShift;
    pyopencv_reg_MapShift_Type.tp_methods = pyopencv_reg_MapShift_methods;
}

static PyObject* pyopencv_ppf_match_3d_ICP_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ppf_match_3d_ICP %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ppf_match_3d_ICP_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_ppf_match_3d_ppf_match_3d_ICP_ICP(pyopencv_ppf_match_3d_ICP_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ppf_match_3d;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::ppf_match_3d::ICP>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::ppf_match_3d::ICP()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    int iterations=0;
    float tolerence=0.05f;
    float rejectionScale=2.5f;
    int numLevels=6;
    int sampleType=ICP::ICP_SAMPLING_TYPE_UNIFORM;
    int numMaxCorr=1;

    const char* keywords[] = { "iterations", "tolerence", "rejectionScale", "numLevels", "sampleType", "numMaxCorr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|ffiii:ICP", (char**)keywords, &iterations, &tolerence, &rejectionScale, &numLevels, &sampleType, &numMaxCorr) )
    {
        new (&(self->v)) Ptr<cv::ppf_match_3d::ICP>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::ppf_match_3d::ICP(iterations, tolerence, rejectionScale, numLevels, sampleType, numMaxCorr)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_ppf_match_3d_ppf_match_3d_ICP_registerModelToScene(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ppf_match_3d;

    cv::ppf_match_3d::ICP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ppf_match_3d_ICP_Type))
        _self_ = ((pyopencv_ppf_match_3d_ICP_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ppf_match_3d_ICP' or its derivative)");
    {
    PyObject* pyobj_srcPC = NULL;
    Mat srcPC;
    PyObject* pyobj_dstPC = NULL;
    Mat dstPC;
    double residual;
    Matx44d pose;
    int retval;

    const char* keywords[] = { "srcPC", "dstPC", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ppf_match_3d_ICP.registerModelToScene", (char**)keywords, &pyobj_srcPC, &pyobj_dstPC) &&
        pyopencv_to(pyobj_srcPC, srcPC, ArgInfo("srcPC", 0)) &&
        pyopencv_to(pyobj_dstPC, dstPC, ArgInfo("dstPC", 0)) )
    {
        ERRWRAP2(retval = _self_->registerModelToScene(srcPC, dstPC, residual, pose));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(residual), pyopencv_from(pose));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_srcPC = NULL;
    Mat srcPC;
    PyObject* pyobj_dstPC = NULL;
    Mat dstPC;
    double residual;
    Matx44d pose;
    int retval;

    const char* keywords[] = { "srcPC", "dstPC", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ppf_match_3d_ICP.registerModelToScene", (char**)keywords, &pyobj_srcPC, &pyobj_dstPC) &&
        pyopencv_to(pyobj_srcPC, srcPC, ArgInfo("srcPC", 0)) &&
        pyopencv_to(pyobj_dstPC, dstPC, ArgInfo("dstPC", 0)) )
    {
        ERRWRAP2(retval = _self_->registerModelToScene(srcPC, dstPC, residual, pose));
        return Py_BuildValue("(NNN)", pyopencv_from(retval), pyopencv_from(residual), pyopencv_from(pose));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ppf_match_3d_ICP_methods[] =
{
    {"registerModelToScene", CV_PY_FN_WITH_KW_(pyopencv_cv_ppf_match_3d_ppf_match_3d_ICP_registerModelToScene, 0), "registerModelToScene(srcPC, dstPC) -> retval, residual, pose\n.   *  \\brief Perform registration\n.   *\n.   *  @param [in] srcPC The input point cloud for the model. Expected to have the normals (Nx6). Currently,\n.   *  CV_32F is the only supported data type.\n.   *  @param [in] dstPC The input point cloud for the scene. It is assumed that the model is registered on the scene. Scene remains static. Expected to have the normals (Nx6). Currently, CV_32F is the only supported data type.\n.   *  @param [out] residual The output registration error.\n.   *  @param [out] pose Transformation between srcPC and dstPC.\n.   *  \\return On successful termination, the function returns 0.\n.   *\n.   *  \\details It is assumed that the model is registered on the scene. Scene remains static, while the model transforms. The output poses transform the models onto the scene. Because of the point to plane minimization, the scene is expected to have the normals available. Expected to have the normals (Nx6)."},

    {NULL,          NULL}
};

static void pyopencv_ppf_match_3d_ICP_specials(void)
{
    pyopencv_ppf_match_3d_ICP_Type.tp_base = NULL;
    pyopencv_ppf_match_3d_ICP_Type.tp_dealloc = pyopencv_ppf_match_3d_ICP_dealloc;
    pyopencv_ppf_match_3d_ICP_Type.tp_repr = pyopencv_ppf_match_3d_ICP_repr;
    pyopencv_ppf_match_3d_ICP_Type.tp_getset = pyopencv_ppf_match_3d_ICP_getseters;
    pyopencv_ppf_match_3d_ICP_Type.tp_init = (initproc)pyopencv_cv_ppf_match_3d_ppf_match_3d_ICP_ICP;
    pyopencv_ppf_match_3d_ICP_Type.tp_methods = pyopencv_ppf_match_3d_ICP_methods;
}

static PyObject* pyopencv_ppf_match_3d_PoseCluster3D_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ppf_match_3d_PoseCluster3D %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ppf_match_3d_PoseCluster3D_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ppf_match_3d_PoseCluster3D_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ppf_match_3d_PoseCluster3D_specials(void)
{
    pyopencv_ppf_match_3d_PoseCluster3D_Type.tp_base = NULL;
    pyopencv_ppf_match_3d_PoseCluster3D_Type.tp_dealloc = pyopencv_ppf_match_3d_PoseCluster3D_dealloc;
    pyopencv_ppf_match_3d_PoseCluster3D_Type.tp_repr = pyopencv_ppf_match_3d_PoseCluster3D_repr;
    pyopencv_ppf_match_3d_PoseCluster3D_Type.tp_getset = pyopencv_ppf_match_3d_PoseCluster3D_getseters;
    pyopencv_ppf_match_3d_PoseCluster3D_Type.tp_init = (initproc)0;
    pyopencv_ppf_match_3d_PoseCluster3D_Type.tp_methods = pyopencv_ppf_match_3d_PoseCluster3D_methods;
}

static PyObject* pyopencv_ppf_match_3d_PPF3DDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ppf_match_3d_PPF3DDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ppf_match_3d_PPF3DDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ppf_match_3d_PPF3DDetector_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ppf_match_3d_PPF3DDetector_specials(void)
{
    pyopencv_ppf_match_3d_PPF3DDetector_Type.tp_base = NULL;
    pyopencv_ppf_match_3d_PPF3DDetector_Type.tp_dealloc = pyopencv_ppf_match_3d_PPF3DDetector_dealloc;
    pyopencv_ppf_match_3d_PPF3DDetector_Type.tp_repr = pyopencv_ppf_match_3d_PPF3DDetector_repr;
    pyopencv_ppf_match_3d_PPF3DDetector_Type.tp_getset = pyopencv_ppf_match_3d_PPF3DDetector_getseters;
    pyopencv_ppf_match_3d_PPF3DDetector_Type.tp_init = (initproc)0;
    pyopencv_ppf_match_3d_PPF3DDetector_Type.tp_methods = pyopencv_ppf_match_3d_PPF3DDetector_methods;
}

static PyObject* pyopencv_BackgroundSubtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BackgroundSubtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BackgroundSubtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_BackgroundSubtractor_apply(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractor_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractor*>(((pyopencv_BackgroundSubtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractor' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_fgmask = NULL;
    Mat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:BackgroundSubtractor.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_fgmask = NULL;
    UMat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:BackgroundSubtractor.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractor_getBackgroundImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractor_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractor*>(((pyopencv_BackgroundSubtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractor' or its derivative)");
    {
    PyObject* pyobj_backgroundImage = NULL;
    Mat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:BackgroundSubtractor.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_backgroundImage = NULL;
    UMat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:BackgroundSubtractor.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_BackgroundSubtractor_methods[] =
{
    {"apply", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractor_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n.   @brief Computes a foreground mask.\n.   \n.   @param image Next video frame.\n.   @param fgmask The output foreground mask as an 8-bit binary image.\n.   @param learningRate The value between 0 and 1 that indicates how fast the background model is\n.   learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n.   rate. 0 means that the background model is not updated at all, 1 means that the background model\n.   is completely reinitialized from the last frame."},
    {"getBackgroundImage", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractor_getBackgroundImage, 0), "getBackgroundImage([, backgroundImage]) -> backgroundImage\n.   @brief Computes a background image.\n.   \n.   @param backgroundImage The output background image.\n.   \n.   @note Sometimes the background image can be very blurry, as it contain the average background\n.   statistics."},

    {NULL,          NULL}
};

static void pyopencv_BackgroundSubtractor_specials(void)
{
    pyopencv_BackgroundSubtractor_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_BackgroundSubtractor_Type.tp_dealloc = pyopencv_BackgroundSubtractor_dealloc;
    pyopencv_BackgroundSubtractor_Type.tp_repr = pyopencv_BackgroundSubtractor_repr;
    pyopencv_BackgroundSubtractor_Type.tp_getset = pyopencv_BackgroundSubtractor_getseters;
    pyopencv_BackgroundSubtractor_Type.tp_init = (initproc)0;
    pyopencv_BackgroundSubtractor_Type.tp_methods = pyopencv_BackgroundSubtractor_methods;
}

static PyObject* pyopencv_BackgroundSubtractorMOG2_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BackgroundSubtractorMOG2 %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BackgroundSubtractorMOG2_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_apply(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_fgmask = NULL;
    Mat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:BackgroundSubtractorMOG2.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_fgmask = NULL;
    UMat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:BackgroundSubtractorMOG2.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getBackgroundRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBackgroundRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getComplexityReductionThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getComplexityReductionThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getDetectShadows(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDetectShadows());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHistory());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getNMixtures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNMixtures());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getShadowThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getShadowThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getShadowValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getShadowValue());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getVarInit(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarInit());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getVarMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarMax());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getVarMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarMin());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getVarThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_getVarThresholdGen(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVarThresholdGen());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setBackgroundRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double ratio=0;

    const char* keywords[] = { "ratio", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setBackgroundRatio", (char**)keywords, &ratio) )
    {
        ERRWRAP2(_self_->setBackgroundRatio(ratio));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setComplexityReductionThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double ct=0;

    const char* keywords[] = { "ct", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setComplexityReductionThreshold", (char**)keywords, &ct) )
    {
        ERRWRAP2(_self_->setComplexityReductionThreshold(ct));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setDetectShadows(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    bool detectShadows=0;

    const char* keywords[] = { "detectShadows", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:BackgroundSubtractorMOG2.setDetectShadows", (char**)keywords, &detectShadows) )
    {
        ERRWRAP2(_self_->setDetectShadows(detectShadows));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    int history=0;

    const char* keywords[] = { "history", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorMOG2.setHistory", (char**)keywords, &history) )
    {
        ERRWRAP2(_self_->setHistory(history));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setNMixtures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    int nmixtures=0;

    const char* keywords[] = { "nmixtures", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorMOG2.setNMixtures", (char**)keywords, &nmixtures) )
    {
        ERRWRAP2(_self_->setNMixtures(nmixtures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setShadowThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setShadowThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setShadowThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setShadowValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    int value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorMOG2.setShadowValue", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setShadowValue(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setVarInit(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double varInit=0;

    const char* keywords[] = { "varInit", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setVarInit", (char**)keywords, &varInit) )
    {
        ERRWRAP2(_self_->setVarInit(varInit));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setVarMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double varMax=0;

    const char* keywords[] = { "varMax", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setVarMax", (char**)keywords, &varMax) )
    {
        ERRWRAP2(_self_->setVarMax(varMax));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setVarMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double varMin=0;

    const char* keywords[] = { "varMin", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setVarMin", (char**)keywords, &varMin) )
    {
        ERRWRAP2(_self_->setVarMin(varMin));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setVarThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double varThreshold=0;

    const char* keywords[] = { "varThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setVarThreshold", (char**)keywords, &varThreshold) )
    {
        ERRWRAP2(_self_->setVarThreshold(varThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorMOG2_setVarThresholdGen(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorMOG2* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorMOG2_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorMOG2*>(((pyopencv_BackgroundSubtractorMOG2_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorMOG2' or its derivative)");
    double varThresholdGen=0;

    const char* keywords[] = { "varThresholdGen", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorMOG2.setVarThresholdGen", (char**)keywords, &varThresholdGen) )
    {
        ERRWRAP2(_self_->setVarThresholdGen(varThresholdGen));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_BackgroundSubtractorMOG2_methods[] =
{
    {"apply", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n.   @brief Computes a foreground mask.\n.   \n.   @param image Next video frame. Floating point frame will be used without scaling and should be in range \\f$[0,255]\\f$.\n.   @param fgmask The output foreground mask as an 8-bit binary image.\n.   @param learningRate The value between 0 and 1 that indicates how fast the background model is\n.   learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n.   rate. 0 means that the background model is not updated at all, 1 means that the background model\n.   is completely reinitialized from the last frame."},
    {"getBackgroundRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getBackgroundRatio, 0), "getBackgroundRatio() -> retval\n.   @brief Returns the \"background ratio\" parameter of the algorithm\n.   \n.   If a foreground pixel keeps semi-constant value for about backgroundRatio\\*history frames, it's\n.   considered background and added to the model as a center of a new component. It corresponds to TB\n.   parameter in the paper."},
    {"getComplexityReductionThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getComplexityReductionThreshold, 0), "getComplexityReductionThreshold() -> retval\n.   @brief Returns the complexity reduction threshold\n.   \n.   This parameter defines the number of samples needed to accept to prove the component exists. CT=0.05\n.   is a default value for all the samples. By setting CT=0 you get an algorithm very similar to the\n.   standard Stauffer&Grimson algorithm."},
    {"getDetectShadows", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getDetectShadows, 0), "getDetectShadows() -> retval\n.   @brief Returns the shadow detection flag\n.   \n.   If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorMOG2 for\n.   details."},
    {"getHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getHistory, 0), "getHistory() -> retval\n.   @brief Returns the number of last frames that affect the background model"},
    {"getNMixtures", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getNMixtures, 0), "getNMixtures() -> retval\n.   @brief Returns the number of gaussian components in the background model"},
    {"getShadowThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getShadowThreshold, 0), "getShadowThreshold() -> retval\n.   @brief Returns the shadow threshold\n.   \n.   A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in\n.   the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel\n.   is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,\n.   *Detecting Moving Shadows...*, IEEE PAMI,2003."},
    {"getShadowValue", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getShadowValue, 0), "getShadowValue() -> retval\n.   @brief Returns the shadow value\n.   \n.   Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0\n.   in the mask always means background, 255 means foreground."},
    {"getVarInit", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarInit, 0), "getVarInit() -> retval\n.   @brief Returns the initial variance of each gaussian component"},
    {"getVarMax", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarMax, 0), "getVarMax() -> retval\n."},
    {"getVarMin", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarMin, 0), "getVarMin() -> retval\n."},
    {"getVarThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarThreshold, 0), "getVarThreshold() -> retval\n.   @brief Returns the variance threshold for the pixel-model match\n.   \n.   The main threshold on the squared Mahalanobis distance to decide if the sample is well described by\n.   the background model or not. Related to Cthr from the paper."},
    {"getVarThresholdGen", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_getVarThresholdGen, 0), "getVarThresholdGen() -> retval\n.   @brief Returns the variance threshold for the pixel-model match used for new mixture component generation\n.   \n.   Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the\n.   existing components (corresponds to Tg in the paper). If a pixel is not close to any component, it\n.   is considered foreground or added as a new component. 3 sigma =\\> Tg=3\\*3=9 is default. A smaller Tg\n.   value generates more components. A higher Tg value may result in a small number of components but\n.   they can grow too large."},
    {"setBackgroundRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setBackgroundRatio, 0), "setBackgroundRatio(ratio) -> None\n.   @brief Sets the \"background ratio\" parameter of the algorithm"},
    {"setComplexityReductionThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setComplexityReductionThreshold, 0), "setComplexityReductionThreshold(ct) -> None\n.   @brief Sets the complexity reduction threshold"},
    {"setDetectShadows", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setDetectShadows, 0), "setDetectShadows(detectShadows) -> None\n.   @brief Enables or disables shadow detection"},
    {"setHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setHistory, 0), "setHistory(history) -> None\n.   @brief Sets the number of last frames that affect the background model"},
    {"setNMixtures", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setNMixtures, 0), "setNMixtures(nmixtures) -> None\n.   @brief Sets the number of gaussian components in the background model.\n.   \n.   The model needs to be reinitalized to reserve memory."},
    {"setShadowThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setShadowThreshold, 0), "setShadowThreshold(threshold) -> None\n.   @brief Sets the shadow threshold"},
    {"setShadowValue", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setShadowValue, 0), "setShadowValue(value) -> None\n.   @brief Sets the shadow value"},
    {"setVarInit", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarInit, 0), "setVarInit(varInit) -> None\n.   @brief Sets the initial variance of each gaussian component"},
    {"setVarMax", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarMax, 0), "setVarMax(varMax) -> None\n."},
    {"setVarMin", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarMin, 0), "setVarMin(varMin) -> None\n."},
    {"setVarThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarThreshold, 0), "setVarThreshold(varThreshold) -> None\n.   @brief Sets the variance threshold for the pixel-model match"},
    {"setVarThresholdGen", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorMOG2_setVarThresholdGen, 0), "setVarThresholdGen(varThresholdGen) -> None\n.   @brief Sets the variance threshold for the pixel-model match used for new mixture component generation"},

    {NULL,          NULL}
};

static void pyopencv_BackgroundSubtractorMOG2_specials(void)
{
    pyopencv_BackgroundSubtractorMOG2_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_BackgroundSubtractorMOG2_Type.tp_dealloc = pyopencv_BackgroundSubtractorMOG2_dealloc;
    pyopencv_BackgroundSubtractorMOG2_Type.tp_repr = pyopencv_BackgroundSubtractorMOG2_repr;
    pyopencv_BackgroundSubtractorMOG2_Type.tp_getset = pyopencv_BackgroundSubtractorMOG2_getseters;
    pyopencv_BackgroundSubtractorMOG2_Type.tp_init = (initproc)0;
    pyopencv_BackgroundSubtractorMOG2_Type.tp_methods = pyopencv_BackgroundSubtractorMOG2_methods;
}

static PyObject* pyopencv_BackgroundSubtractorKNN_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BackgroundSubtractorKNN %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BackgroundSubtractorKNN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getDetectShadows(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDetectShadows());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getDist2Threshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDist2Threshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHistory());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getNSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getShadowThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getShadowThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getShadowValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getShadowValue());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_getkNNSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getkNNSamples());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setDetectShadows(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    bool detectShadows=0;

    const char* keywords[] = { "detectShadows", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:BackgroundSubtractorKNN.setDetectShadows", (char**)keywords, &detectShadows) )
    {
        ERRWRAP2(_self_->setDetectShadows(detectShadows));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setDist2Threshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    double _dist2Threshold=0;

    const char* keywords[] = { "_dist2Threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorKNN.setDist2Threshold", (char**)keywords, &_dist2Threshold) )
    {
        ERRWRAP2(_self_->setDist2Threshold(_dist2Threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int history=0;

    const char* keywords[] = { "history", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorKNN.setHistory", (char**)keywords, &history) )
    {
        ERRWRAP2(_self_->setHistory(history));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setNSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int _nN=0;

    const char* keywords[] = { "_nN", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorKNN.setNSamples", (char**)keywords, &_nN) )
    {
        ERRWRAP2(_self_->setNSamples(_nN));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setShadowThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:BackgroundSubtractorKNN.setShadowThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setShadowThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setShadowValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorKNN.setShadowValue", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setShadowValue(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BackgroundSubtractorKNN_setkNNSamples(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BackgroundSubtractorKNN* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BackgroundSubtractorKNN_Type))
        _self_ = dynamic_cast<cv::BackgroundSubtractorKNN*>(((pyopencv_BackgroundSubtractorKNN_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BackgroundSubtractorKNN' or its derivative)");
    int _nkNN=0;

    const char* keywords[] = { "_nkNN", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:BackgroundSubtractorKNN.setkNNSamples", (char**)keywords, &_nkNN) )
    {
        ERRWRAP2(_self_->setkNNSamples(_nkNN));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_BackgroundSubtractorKNN_methods[] =
{
    {"getDetectShadows", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getDetectShadows, 0), "getDetectShadows() -> retval\n.   @brief Returns the shadow detection flag\n.   \n.   If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorKNN for\n.   details."},
    {"getDist2Threshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getDist2Threshold, 0), "getDist2Threshold() -> retval\n.   @brief Returns the threshold on the squared distance between the pixel and the sample\n.   \n.   The threshold on the squared distance between the pixel and the sample to decide whether a pixel is\n.   close to a data sample."},
    {"getHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getHistory, 0), "getHistory() -> retval\n.   @brief Returns the number of last frames that affect the background model"},
    {"getNSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getNSamples, 0), "getNSamples() -> retval\n.   @brief Returns the number of data samples in the background model"},
    {"getShadowThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getShadowThreshold, 0), "getShadowThreshold() -> retval\n.   @brief Returns the shadow threshold\n.   \n.   A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in\n.   the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel\n.   is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,\n.   *Detecting Moving Shadows...*, IEEE PAMI,2003."},
    {"getShadowValue", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getShadowValue, 0), "getShadowValue() -> retval\n.   @brief Returns the shadow value\n.   \n.   Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0\n.   in the mask always means background, 255 means foreground."},
    {"getkNNSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_getkNNSamples, 0), "getkNNSamples() -> retval\n.   @brief Returns the number of neighbours, the k in the kNN.\n.   \n.   K is the number of samples that need to be within dist2Threshold in order to decide that that\n.   pixel is matching the kNN background model."},
    {"setDetectShadows", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setDetectShadows, 0), "setDetectShadows(detectShadows) -> None\n.   @brief Enables or disables shadow detection"},
    {"setDist2Threshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setDist2Threshold, 0), "setDist2Threshold(_dist2Threshold) -> None\n.   @brief Sets the threshold on the squared distance"},
    {"setHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setHistory, 0), "setHistory(history) -> None\n.   @brief Sets the number of last frames that affect the background model"},
    {"setNSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setNSamples, 0), "setNSamples(_nN) -> None\n.   @brief Sets the number of data samples in the background model.\n.   \n.   The model needs to be reinitalized to reserve memory."},
    {"setShadowThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setShadowThreshold, 0), "setShadowThreshold(threshold) -> None\n.   @brief Sets the shadow threshold"},
    {"setShadowValue", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setShadowValue, 0), "setShadowValue(value) -> None\n.   @brief Sets the shadow value"},
    {"setkNNSamples", CV_PY_FN_WITH_KW_(pyopencv_cv_BackgroundSubtractorKNN_setkNNSamples, 0), "setkNNSamples(_nkNN) -> None\n.   @brief Sets the k in the kNN. How many nearest neighbours need to match."},

    {NULL,          NULL}
};

static void pyopencv_BackgroundSubtractorKNN_specials(void)
{
    pyopencv_BackgroundSubtractorKNN_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_BackgroundSubtractorKNN_Type.tp_dealloc = pyopencv_BackgroundSubtractorKNN_dealloc;
    pyopencv_BackgroundSubtractorKNN_Type.tp_repr = pyopencv_BackgroundSubtractorKNN_repr;
    pyopencv_BackgroundSubtractorKNN_Type.tp_getset = pyopencv_BackgroundSubtractorKNN_getseters;
    pyopencv_BackgroundSubtractorKNN_Type.tp_init = (initproc)0;
    pyopencv_BackgroundSubtractorKNN_Type.tp_methods = pyopencv_BackgroundSubtractorKNN_methods;
}

static PyObject* pyopencv_KalmanFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<KalmanFilter %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_KalmanFilter_get_controlMatrix(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->controlMatrix);
}

static int pyopencv_KalmanFilter_set_controlMatrix(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the controlMatrix attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->controlMatrix) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_errorCovPost(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->errorCovPost);
}

static int pyopencv_KalmanFilter_set_errorCovPost(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCovPost attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->errorCovPost) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_errorCovPre(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->errorCovPre);
}

static int pyopencv_KalmanFilter_set_errorCovPre(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCovPre attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->errorCovPre) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_gain(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->gain);
}

static int pyopencv_KalmanFilter_set_gain(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the gain attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->gain) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_measurementMatrix(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->measurementMatrix);
}

static int pyopencv_KalmanFilter_set_measurementMatrix(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the measurementMatrix attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->measurementMatrix) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_measurementNoiseCov(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->measurementNoiseCov);
}

static int pyopencv_KalmanFilter_set_measurementNoiseCov(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the measurementNoiseCov attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->measurementNoiseCov) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_processNoiseCov(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->processNoiseCov);
}

static int pyopencv_KalmanFilter_set_processNoiseCov(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the processNoiseCov attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->processNoiseCov) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_statePost(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->statePost);
}

static int pyopencv_KalmanFilter_set_statePost(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the statePost attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->statePost) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_statePre(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->statePre);
}

static int pyopencv_KalmanFilter_set_statePre(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the statePre attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->statePre) ? 0 : -1;
}

static PyObject* pyopencv_KalmanFilter_get_transitionMatrix(pyopencv_KalmanFilter_t* p, void *closure)
{
    return pyopencv_from(p->v->transitionMatrix);
}

static int pyopencv_KalmanFilter_set_transitionMatrix(pyopencv_KalmanFilter_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the transitionMatrix attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->transitionMatrix) ? 0 : -1;
}


static PyGetSetDef pyopencv_KalmanFilter_getseters[] =
{
    {(char*)"controlMatrix", (getter)pyopencv_KalmanFilter_get_controlMatrix, (setter)pyopencv_KalmanFilter_set_controlMatrix, (char*)"controlMatrix", NULL},
    {(char*)"errorCovPost", (getter)pyopencv_KalmanFilter_get_errorCovPost, (setter)pyopencv_KalmanFilter_set_errorCovPost, (char*)"errorCovPost", NULL},
    {(char*)"errorCovPre", (getter)pyopencv_KalmanFilter_get_errorCovPre, (setter)pyopencv_KalmanFilter_set_errorCovPre, (char*)"errorCovPre", NULL},
    {(char*)"gain", (getter)pyopencv_KalmanFilter_get_gain, (setter)pyopencv_KalmanFilter_set_gain, (char*)"gain", NULL},
    {(char*)"measurementMatrix", (getter)pyopencv_KalmanFilter_get_measurementMatrix, (setter)pyopencv_KalmanFilter_set_measurementMatrix, (char*)"measurementMatrix", NULL},
    {(char*)"measurementNoiseCov", (getter)pyopencv_KalmanFilter_get_measurementNoiseCov, (setter)pyopencv_KalmanFilter_set_measurementNoiseCov, (char*)"measurementNoiseCov", NULL},
    {(char*)"processNoiseCov", (getter)pyopencv_KalmanFilter_get_processNoiseCov, (setter)pyopencv_KalmanFilter_set_processNoiseCov, (char*)"processNoiseCov", NULL},
    {(char*)"statePost", (getter)pyopencv_KalmanFilter_get_statePost, (setter)pyopencv_KalmanFilter_set_statePost, (char*)"statePost", NULL},
    {(char*)"statePre", (getter)pyopencv_KalmanFilter_get_statePre, (setter)pyopencv_KalmanFilter_set_statePre, (char*)"statePre", NULL},
    {(char*)"transitionMatrix", (getter)pyopencv_KalmanFilter_get_transitionMatrix, (setter)pyopencv_KalmanFilter_set_transitionMatrix, (char*)"transitionMatrix", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_KalmanFilter_KalmanFilter(pyopencv_KalmanFilter_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::KalmanFilter>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::KalmanFilter()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    int dynamParams=0;
    int measureParams=0;
    int controlParams=0;
    int type=CV_32F;

    const char* keywords[] = { "dynamParams", "measureParams", "controlParams", "type", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii|ii:KalmanFilter", (char**)keywords, &dynamParams, &measureParams, &controlParams, &type) )
    {
        new (&(self->v)) Ptr<cv::KalmanFilter>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::KalmanFilter(dynamParams, measureParams, controlParams, type)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_KalmanFilter_correct(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KalmanFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KalmanFilter_Type))
        _self_ = ((pyopencv_KalmanFilter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KalmanFilter' or its derivative)");
    {
    PyObject* pyobj_measurement = NULL;
    Mat measurement;
    Mat retval;

    const char* keywords[] = { "measurement", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:KalmanFilter.correct", (char**)keywords, &pyobj_measurement) &&
        pyopencv_to(pyobj_measurement, measurement, ArgInfo("measurement", 0)) )
    {
        ERRWRAP2(retval = _self_->correct(measurement));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_measurement = NULL;
    Mat measurement;
    Mat retval;

    const char* keywords[] = { "measurement", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:KalmanFilter.correct", (char**)keywords, &pyobj_measurement) &&
        pyopencv_to(pyobj_measurement, measurement, ArgInfo("measurement", 0)) )
    {
        ERRWRAP2(retval = _self_->correct(measurement));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_KalmanFilter_predict(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KalmanFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KalmanFilter_Type))
        _self_ = ((pyopencv_KalmanFilter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KalmanFilter' or its derivative)");
    {
    PyObject* pyobj_control = NULL;
    Mat control;
    Mat retval;

    const char* keywords[] = { "control", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:KalmanFilter.predict", (char**)keywords, &pyobj_control) &&
        pyopencv_to(pyobj_control, control, ArgInfo("control", 0)) )
    {
        ERRWRAP2(retval = _self_->predict(control));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_control = NULL;
    Mat control;
    Mat retval;

    const char* keywords[] = { "control", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:KalmanFilter.predict", (char**)keywords, &pyobj_control) &&
        pyopencv_to(pyobj_control, control, ArgInfo("control", 0)) )
    {
        ERRWRAP2(retval = _self_->predict(control));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_KalmanFilter_methods[] =
{
    {"correct", CV_PY_FN_WITH_KW_(pyopencv_cv_KalmanFilter_correct, 0), "correct(measurement) -> retval\n.   @brief Updates the predicted state from the measurement.\n.   \n.   @param measurement The measured system parameters"},
    {"predict", CV_PY_FN_WITH_KW_(pyopencv_cv_KalmanFilter_predict, 0), "predict([, control]) -> retval\n.   @brief Computes a predicted state.\n.   \n.   @param control The optional input control"},

    {NULL,          NULL}
};

static void pyopencv_KalmanFilter_specials(void)
{
    pyopencv_KalmanFilter_Type.tp_base = NULL;
    pyopencv_KalmanFilter_Type.tp_dealloc = pyopencv_KalmanFilter_dealloc;
    pyopencv_KalmanFilter_Type.tp_repr = pyopencv_KalmanFilter_repr;
    pyopencv_KalmanFilter_Type.tp_getset = pyopencv_KalmanFilter_getseters;
    pyopencv_KalmanFilter_Type.tp_init = (initproc)pyopencv_cv_KalmanFilter_KalmanFilter;
    pyopencv_KalmanFilter_Type.tp_methods = pyopencv_KalmanFilter_methods;
}

static PyObject* pyopencv_DenseOpticalFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<DenseOpticalFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_DenseOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_DenseOpticalFlow_calc(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DenseOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DenseOpticalFlow_Type))
        _self_ = dynamic_cast<cv::DenseOpticalFlow*>(((pyopencv_DenseOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DenseOpticalFlow' or its derivative)");
    {
    PyObject* pyobj_I0 = NULL;
    Mat I0;
    PyObject* pyobj_I1 = NULL;
    Mat I1;
    PyObject* pyobj_flow = NULL;
    Mat flow;

    const char* keywords[] = { "I0", "I1", "flow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:DenseOpticalFlow.calc", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow) &&
        pyopencv_to(pyobj_I0, I0, ArgInfo("I0", 0)) &&
        pyopencv_to(pyobj_I1, I1, ArgInfo("I1", 0)) &&
        pyopencv_to(pyobj_flow, flow, ArgInfo("flow", 1)) )
    {
        ERRWRAP2(_self_->calc(I0, I1, flow));
        return pyopencv_from(flow);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_I0 = NULL;
    UMat I0;
    PyObject* pyobj_I1 = NULL;
    UMat I1;
    PyObject* pyobj_flow = NULL;
    UMat flow;

    const char* keywords[] = { "I0", "I1", "flow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:DenseOpticalFlow.calc", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow) &&
        pyopencv_to(pyobj_I0, I0, ArgInfo("I0", 0)) &&
        pyopencv_to(pyobj_I1, I1, ArgInfo("I1", 0)) &&
        pyopencv_to(pyobj_flow, flow, ArgInfo("flow", 1)) )
    {
        ERRWRAP2(_self_->calc(I0, I1, flow));
        return pyopencv_from(flow);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DenseOpticalFlow_collectGarbage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DenseOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DenseOpticalFlow_Type))
        _self_ = dynamic_cast<cv::DenseOpticalFlow*>(((pyopencv_DenseOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DenseOpticalFlow' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->collectGarbage());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_DenseOpticalFlow_methods[] =
{
    {"calc", CV_PY_FN_WITH_KW_(pyopencv_cv_DenseOpticalFlow_calc, 0), "calc(I0, I1, flow) -> flow\n.   @brief Calculates an optical flow.\n.   \n.   @param I0 first 8-bit single-channel input image.\n.   @param I1 second input image of the same size and the same type as prev.\n.   @param flow computed flow image that has the same size as prev and type CV_32FC2."},
    {"collectGarbage", CV_PY_FN_WITH_KW_(pyopencv_cv_DenseOpticalFlow_collectGarbage, 0), "collectGarbage() -> None\n.   @brief Releases all inner buffers."},

    {NULL,          NULL}
};

static void pyopencv_DenseOpticalFlow_specials(void)
{
    pyopencv_DenseOpticalFlow_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_DenseOpticalFlow_Type.tp_dealloc = pyopencv_DenseOpticalFlow_dealloc;
    pyopencv_DenseOpticalFlow_Type.tp_repr = pyopencv_DenseOpticalFlow_repr;
    pyopencv_DenseOpticalFlow_Type.tp_getset = pyopencv_DenseOpticalFlow_getseters;
    pyopencv_DenseOpticalFlow_Type.tp_init = (initproc)0;
    pyopencv_DenseOpticalFlow_Type.tp_methods = pyopencv_DenseOpticalFlow_methods;
}

static PyObject* pyopencv_SparseOpticalFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<SparseOpticalFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_SparseOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_SparseOpticalFlow_calc(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparseOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparseOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparseOpticalFlow*>(((pyopencv_SparseOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparseOpticalFlow' or its derivative)");
    {
    PyObject* pyobj_prevImg = NULL;
    Mat prevImg;
    PyObject* pyobj_nextImg = NULL;
    Mat nextImg;
    PyObject* pyobj_prevPts = NULL;
    Mat prevPts;
    PyObject* pyobj_nextPts = NULL;
    Mat nextPts;
    PyObject* pyobj_status = NULL;
    Mat status;
    PyObject* pyobj_err = NULL;
    Mat err;

    const char* keywords[] = { "prevImg", "nextImg", "prevPts", "nextPts", "status", "err", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO|OO:SparseOpticalFlow.calc", (char**)keywords, &pyobj_prevImg, &pyobj_nextImg, &pyobj_prevPts, &pyobj_nextPts, &pyobj_status, &pyobj_err) &&
        pyopencv_to(pyobj_prevImg, prevImg, ArgInfo("prevImg", 0)) &&
        pyopencv_to(pyobj_nextImg, nextImg, ArgInfo("nextImg", 0)) &&
        pyopencv_to(pyobj_prevPts, prevPts, ArgInfo("prevPts", 0)) &&
        pyopencv_to(pyobj_nextPts, nextPts, ArgInfo("nextPts", 1)) &&
        pyopencv_to(pyobj_status, status, ArgInfo("status", 1)) &&
        pyopencv_to(pyobj_err, err, ArgInfo("err", 1)) )
    {
        ERRWRAP2(_self_->calc(prevImg, nextImg, prevPts, nextPts, status, err));
        return Py_BuildValue("(NNN)", pyopencv_from(nextPts), pyopencv_from(status), pyopencv_from(err));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_prevImg = NULL;
    UMat prevImg;
    PyObject* pyobj_nextImg = NULL;
    UMat nextImg;
    PyObject* pyobj_prevPts = NULL;
    UMat prevPts;
    PyObject* pyobj_nextPts = NULL;
    UMat nextPts;
    PyObject* pyobj_status = NULL;
    UMat status;
    PyObject* pyobj_err = NULL;
    UMat err;

    const char* keywords[] = { "prevImg", "nextImg", "prevPts", "nextPts", "status", "err", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO|OO:SparseOpticalFlow.calc", (char**)keywords, &pyobj_prevImg, &pyobj_nextImg, &pyobj_prevPts, &pyobj_nextPts, &pyobj_status, &pyobj_err) &&
        pyopencv_to(pyobj_prevImg, prevImg, ArgInfo("prevImg", 0)) &&
        pyopencv_to(pyobj_nextImg, nextImg, ArgInfo("nextImg", 0)) &&
        pyopencv_to(pyobj_prevPts, prevPts, ArgInfo("prevPts", 0)) &&
        pyopencv_to(pyobj_nextPts, nextPts, ArgInfo("nextPts", 1)) &&
        pyopencv_to(pyobj_status, status, ArgInfo("status", 1)) &&
        pyopencv_to(pyobj_err, err, ArgInfo("err", 1)) )
    {
        ERRWRAP2(_self_->calc(prevImg, nextImg, prevPts, nextPts, status, err));
        return Py_BuildValue("(NNN)", pyopencv_from(nextPts), pyopencv_from(status), pyopencv_from(err));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_SparseOpticalFlow_methods[] =
{
    {"calc", CV_PY_FN_WITH_KW_(pyopencv_cv_SparseOpticalFlow_calc, 0), "calc(prevImg, nextImg, prevPts, nextPts[, status[, err]]) -> nextPts, status, err\n.   @brief Calculates a sparse optical flow.\n.   \n.   @param prevImg First input image.\n.   @param nextImg Second input image of the same size and the same type as prevImg.\n.   @param prevPts Vector of 2D points for which the flow needs to be found.\n.   @param nextPts Output vector of 2D points containing the calculated new positions of input features in the second image.\n.   @param status Output status vector. Each element of the vector is set to 1 if the\n.   flow for the corresponding features has been found. Otherwise, it is set to 0.\n.   @param err Optional output vector that contains error response for each point (inverse confidence)."},

    {NULL,          NULL}
};

static void pyopencv_SparseOpticalFlow_specials(void)
{
    pyopencv_SparseOpticalFlow_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_SparseOpticalFlow_Type.tp_dealloc = pyopencv_SparseOpticalFlow_dealloc;
    pyopencv_SparseOpticalFlow_Type.tp_repr = pyopencv_SparseOpticalFlow_repr;
    pyopencv_SparseOpticalFlow_Type.tp_getset = pyopencv_SparseOpticalFlow_getseters;
    pyopencv_SparseOpticalFlow_Type.tp_init = (initproc)0;
    pyopencv_SparseOpticalFlow_Type.tp_methods = pyopencv_SparseOpticalFlow_methods;
}

static PyObject* pyopencv_DualTVL1OpticalFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<DualTVL1OpticalFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_DualTVL1OpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    double tau=0.25;
    double lambda=0.15;
    double theta=0.3;
    int nscales=5;
    int warps=5;
    double epsilon=0.01;
    int innnerIterations=30;
    int outerIterations=10;
    double scaleStep=0.8;
    double gamma=0.0;
    int medianFiltering=5;
    bool useInitialFlow=false;
    Ptr<DualTVL1OpticalFlow> retval;

    const char* keywords[] = { "tau", "lambda", "theta", "nscales", "warps", "epsilon", "innnerIterations", "outerIterations", "scaleStep", "gamma", "medianFiltering", "useInitialFlow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|dddiidiiddib:DualTVL1OpticalFlow.create", (char**)keywords, &tau, &lambda, &theta, &nscales, &warps, &epsilon, &innnerIterations, &outerIterations, &scaleStep, &gamma, &medianFiltering, &useInitialFlow) )
    {
        ERRWRAP2(retval = cv::DualTVL1OpticalFlow::create(tau, lambda, theta, nscales, warps, epsilon, innnerIterations, outerIterations, scaleStep, gamma, medianFiltering, useInitialFlow));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getEpsilon(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEpsilon());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGamma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getInnerIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInnerIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLambda());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getMedianFiltering(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMedianFiltering());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getOuterIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOuterIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getScaleStep(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScaleStep());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getScalesNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScalesNumber());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getTau(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTau());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getTheta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTheta());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getUseInitialFlow(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseInitialFlow());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_getWarpingsNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWarpingsNumber());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setEpsilon(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DualTVL1OpticalFlow.setEpsilon", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setEpsilon(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DualTVL1OpticalFlow.setGamma", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setInnerIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DualTVL1OpticalFlow.setInnerIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setInnerIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DualTVL1OpticalFlow.setLambda", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setLambda(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setMedianFiltering(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DualTVL1OpticalFlow.setMedianFiltering", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMedianFiltering(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setOuterIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DualTVL1OpticalFlow.setOuterIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setOuterIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setScaleStep(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DualTVL1OpticalFlow.setScaleStep", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setScaleStep(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setScalesNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DualTVL1OpticalFlow.setScalesNumber", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setScalesNumber(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setTau(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DualTVL1OpticalFlow.setTau", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTau(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setTheta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DualTVL1OpticalFlow.setTheta", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTheta(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setUseInitialFlow(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:DualTVL1OpticalFlow.setUseInitialFlow", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setUseInitialFlow(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DualTVL1OpticalFlow_setWarpingsNumber(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DualTVL1OpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DualTVL1OpticalFlow_Type))
        _self_ = dynamic_cast<cv::DualTVL1OpticalFlow*>(((pyopencv_DualTVL1OpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DualTVL1OpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DualTVL1OpticalFlow.setWarpingsNumber", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setWarpingsNumber(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_DualTVL1OpticalFlow_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_create_cls, METH_CLASS), "create([, tau[, lambda[, theta[, nscales[, warps[, epsilon[, innnerIterations[, outerIterations[, scaleStep[, gamma[, medianFiltering[, useInitialFlow]]]]]]]]]]]]) -> retval\n.   @brief Creates instance of cv::DualTVL1OpticalFlow"},
    {"getEpsilon", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getEpsilon, 0), "getEpsilon() -> retval\n.   @see setEpsilon"},
    {"getGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getGamma, 0), "getGamma() -> retval\n.   @see setGamma"},
    {"getInnerIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getInnerIterations, 0), "getInnerIterations() -> retval\n.   @see setInnerIterations"},
    {"getLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getLambda, 0), "getLambda() -> retval\n.   @see setLambda"},
    {"getMedianFiltering", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getMedianFiltering, 0), "getMedianFiltering() -> retval\n.   @see setMedianFiltering"},
    {"getOuterIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getOuterIterations, 0), "getOuterIterations() -> retval\n.   @see setOuterIterations"},
    {"getScaleStep", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getScaleStep, 0), "getScaleStep() -> retval\n.   @see setScaleStep"},
    {"getScalesNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getScalesNumber, 0), "getScalesNumber() -> retval\n.   @see setScalesNumber"},
    {"getTau", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getTau, 0), "getTau() -> retval\n.   @see setTau"},
    {"getTheta", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getTheta, 0), "getTheta() -> retval\n.   @see setTheta"},
    {"getUseInitialFlow", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getUseInitialFlow, 0), "getUseInitialFlow() -> retval\n.   @see setUseInitialFlow"},
    {"getWarpingsNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_getWarpingsNumber, 0), "getWarpingsNumber() -> retval\n.   @see setWarpingsNumber"},
    {"setEpsilon", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setEpsilon, 0), "setEpsilon(val) -> None\n.   @copybrief getEpsilon @see getEpsilon"},
    {"setGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setGamma, 0), "setGamma(val) -> None\n.   @copybrief getGamma @see getGamma"},
    {"setInnerIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setInnerIterations, 0), "setInnerIterations(val) -> None\n.   @copybrief getInnerIterations @see getInnerIterations"},
    {"setLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setLambda, 0), "setLambda(val) -> None\n.   @copybrief getLambda @see getLambda"},
    {"setMedianFiltering", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setMedianFiltering, 0), "setMedianFiltering(val) -> None\n.   @copybrief getMedianFiltering @see getMedianFiltering"},
    {"setOuterIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setOuterIterations, 0), "setOuterIterations(val) -> None\n.   @copybrief getOuterIterations @see getOuterIterations"},
    {"setScaleStep", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setScaleStep, 0), "setScaleStep(val) -> None\n.   @copybrief getScaleStep @see getScaleStep"},
    {"setScalesNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setScalesNumber, 0), "setScalesNumber(val) -> None\n.   @copybrief getScalesNumber @see getScalesNumber"},
    {"setTau", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setTau, 0), "setTau(val) -> None\n.   @copybrief getTau @see getTau"},
    {"setTheta", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setTheta, 0), "setTheta(val) -> None\n.   @copybrief getTheta @see getTheta"},
    {"setUseInitialFlow", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setUseInitialFlow, 0), "setUseInitialFlow(val) -> None\n.   @copybrief getUseInitialFlow @see getUseInitialFlow"},
    {"setWarpingsNumber", CV_PY_FN_WITH_KW_(pyopencv_cv_DualTVL1OpticalFlow_setWarpingsNumber, 0), "setWarpingsNumber(val) -> None\n.   @copybrief getWarpingsNumber @see getWarpingsNumber"},

    {NULL,          NULL}
};

static void pyopencv_DualTVL1OpticalFlow_specials(void)
{
    pyopencv_DualTVL1OpticalFlow_Type.tp_base = &pyopencv_DenseOpticalFlow_Type;
    pyopencv_DualTVL1OpticalFlow_Type.tp_dealloc = pyopencv_DualTVL1OpticalFlow_dealloc;
    pyopencv_DualTVL1OpticalFlow_Type.tp_repr = pyopencv_DualTVL1OpticalFlow_repr;
    pyopencv_DualTVL1OpticalFlow_Type.tp_getset = pyopencv_DualTVL1OpticalFlow_getseters;
    pyopencv_DualTVL1OpticalFlow_Type.tp_init = (initproc)0;
    pyopencv_DualTVL1OpticalFlow_Type.tp_methods = pyopencv_DualTVL1OpticalFlow_methods;
}

static PyObject* pyopencv_FarnebackOpticalFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<FarnebackOpticalFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_FarnebackOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_FarnebackOpticalFlow_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int numLevels=5;
    double pyrScale=0.5;
    bool fastPyramids=false;
    int winSize=13;
    int numIters=10;
    int polyN=5;
    double polySigma=1.1;
    int flags=0;
    Ptr<FarnebackOpticalFlow> retval;

    const char* keywords[] = { "numLevels", "pyrScale", "fastPyramids", "winSize", "numIters", "polyN", "polySigma", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|idbiiidi:FarnebackOpticalFlow.create", (char**)keywords, &numLevels, &pyrScale, &fastPyramids, &winSize, &numIters, &polyN, &polySigma, &flags) )
    {
        ERRWRAP2(retval = cv::FarnebackOpticalFlow::create(numLevels, pyrScale, fastPyramids, winSize, numIters, polyN, polySigma, flags));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getFastPyramids(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFastPyramids());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getFlags(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFlags());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getNumIters(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumIters());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getNumLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumLevels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getPolyN(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPolyN());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getPolySigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPolySigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getPyrScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPyrScale());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_getWinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWinSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setFastPyramids(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    bool fastPyramids=0;

    const char* keywords[] = { "fastPyramids", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:FarnebackOpticalFlow.setFastPyramids", (char**)keywords, &fastPyramids) )
    {
        ERRWRAP2(_self_->setFastPyramids(fastPyramids));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setFlags(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int flags=0;

    const char* keywords[] = { "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FarnebackOpticalFlow.setFlags", (char**)keywords, &flags) )
    {
        ERRWRAP2(_self_->setFlags(flags));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setNumIters(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int numIters=0;

    const char* keywords[] = { "numIters", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FarnebackOpticalFlow.setNumIters", (char**)keywords, &numIters) )
    {
        ERRWRAP2(_self_->setNumIters(numIters));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setNumLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int numLevels=0;

    const char* keywords[] = { "numLevels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FarnebackOpticalFlow.setNumLevels", (char**)keywords, &numLevels) )
    {
        ERRWRAP2(_self_->setNumLevels(numLevels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setPolyN(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int polyN=0;

    const char* keywords[] = { "polyN", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FarnebackOpticalFlow.setPolyN", (char**)keywords, &polyN) )
    {
        ERRWRAP2(_self_->setPolyN(polyN));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setPolySigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    double polySigma=0;

    const char* keywords[] = { "polySigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:FarnebackOpticalFlow.setPolySigma", (char**)keywords, &polySigma) )
    {
        ERRWRAP2(_self_->setPolySigma(polySigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setPyrScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    double pyrScale=0;

    const char* keywords[] = { "pyrScale", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:FarnebackOpticalFlow.setPyrScale", (char**)keywords, &pyrScale) )
    {
        ERRWRAP2(_self_->setPyrScale(pyrScale));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FarnebackOpticalFlow_setWinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FarnebackOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FarnebackOpticalFlow_Type))
        _self_ = dynamic_cast<cv::FarnebackOpticalFlow*>(((pyopencv_FarnebackOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FarnebackOpticalFlow' or its derivative)");
    int winSize=0;

    const char* keywords[] = { "winSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FarnebackOpticalFlow.setWinSize", (char**)keywords, &winSize) )
    {
        ERRWRAP2(_self_->setWinSize(winSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_FarnebackOpticalFlow_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_create_cls, METH_CLASS), "create([, numLevels[, pyrScale[, fastPyramids[, winSize[, numIters[, polyN[, polySigma[, flags]]]]]]]]) -> retval\n."},
    {"getFastPyramids", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getFastPyramids, 0), "getFastPyramids() -> retval\n."},
    {"getFlags", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getFlags, 0), "getFlags() -> retval\n."},
    {"getNumIters", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getNumIters, 0), "getNumIters() -> retval\n."},
    {"getNumLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getNumLevels, 0), "getNumLevels() -> retval\n."},
    {"getPolyN", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getPolyN, 0), "getPolyN() -> retval\n."},
    {"getPolySigma", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getPolySigma, 0), "getPolySigma() -> retval\n."},
    {"getPyrScale", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getPyrScale, 0), "getPyrScale() -> retval\n."},
    {"getWinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_getWinSize, 0), "getWinSize() -> retval\n."},
    {"setFastPyramids", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setFastPyramids, 0), "setFastPyramids(fastPyramids) -> None\n."},
    {"setFlags", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setFlags, 0), "setFlags(flags) -> None\n."},
    {"setNumIters", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setNumIters, 0), "setNumIters(numIters) -> None\n."},
    {"setNumLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setNumLevels, 0), "setNumLevels(numLevels) -> None\n."},
    {"setPolyN", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setPolyN, 0), "setPolyN(polyN) -> None\n."},
    {"setPolySigma", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setPolySigma, 0), "setPolySigma(polySigma) -> None\n."},
    {"setPyrScale", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setPyrScale, 0), "setPyrScale(pyrScale) -> None\n."},
    {"setWinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_FarnebackOpticalFlow_setWinSize, 0), "setWinSize(winSize) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_FarnebackOpticalFlow_specials(void)
{
    pyopencv_FarnebackOpticalFlow_Type.tp_base = &pyopencv_DenseOpticalFlow_Type;
    pyopencv_FarnebackOpticalFlow_Type.tp_dealloc = pyopencv_FarnebackOpticalFlow_dealloc;
    pyopencv_FarnebackOpticalFlow_Type.tp_repr = pyopencv_FarnebackOpticalFlow_repr;
    pyopencv_FarnebackOpticalFlow_Type.tp_getset = pyopencv_FarnebackOpticalFlow_getseters;
    pyopencv_FarnebackOpticalFlow_Type.tp_init = (initproc)0;
    pyopencv_FarnebackOpticalFlow_Type.tp_methods = pyopencv_FarnebackOpticalFlow_methods;
}

static PyObject* pyopencv_SparsePyrLKOpticalFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<SparsePyrLKOpticalFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_SparsePyrLKOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_winSize = NULL;
    Size winSize=Size(21, 21);
    int maxLevel=3;
    PyObject* pyobj_crit = NULL;
    TermCriteria crit=TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01);
    int flags=0;
    double minEigThreshold=1e-4;
    Ptr<SparsePyrLKOpticalFlow> retval;

    const char* keywords[] = { "winSize", "maxLevel", "crit", "flags", "minEigThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OiOid:SparsePyrLKOpticalFlow.create", (char**)keywords, &pyobj_winSize, &maxLevel, &pyobj_crit, &flags, &minEigThreshold) &&
        pyopencv_to(pyobj_winSize, winSize, ArgInfo("winSize", 0)) &&
        pyopencv_to(pyobj_crit, crit, ArgInfo("crit", 0)) )
    {
        ERRWRAP2(retval = cv::SparsePyrLKOpticalFlow::create(winSize, maxLevel, crit, flags, minEigThreshold));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_getFlags(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFlags());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_getMaxLevel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxLevel());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_getMinEigThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinEigThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_getTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    TermCriteria retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTermCriteria());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_getWinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWinSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_setFlags(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    int flags=0;

    const char* keywords[] = { "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:SparsePyrLKOpticalFlow.setFlags", (char**)keywords, &flags) )
    {
        ERRWRAP2(_self_->setFlags(flags));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_setMaxLevel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    int maxLevel=0;

    const char* keywords[] = { "maxLevel", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:SparsePyrLKOpticalFlow.setMaxLevel", (char**)keywords, &maxLevel) )
    {
        ERRWRAP2(_self_->setMaxLevel(maxLevel));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_setMinEigThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    double minEigThreshold=0;

    const char* keywords[] = { "minEigThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:SparsePyrLKOpticalFlow.setMinEigThreshold", (char**)keywords, &minEigThreshold) )
    {
        ERRWRAP2(_self_->setMinEigThreshold(minEigThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_setTermCriteria(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    PyObject* pyobj_crit = NULL;
    TermCriteria crit;

    const char* keywords[] = { "crit", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:SparsePyrLKOpticalFlow.setTermCriteria", (char**)keywords, &pyobj_crit) &&
        pyopencv_to(pyobj_crit, crit, ArgInfo("crit", 0)) )
    {
        ERRWRAP2(_self_->setTermCriteria(crit));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_SparsePyrLKOpticalFlow_setWinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SparsePyrLKOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SparsePyrLKOpticalFlow_Type))
        _self_ = dynamic_cast<cv::SparsePyrLKOpticalFlow*>(((pyopencv_SparsePyrLKOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SparsePyrLKOpticalFlow' or its derivative)");
    PyObject* pyobj_winSize = NULL;
    Size winSize;

    const char* keywords[] = { "winSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:SparsePyrLKOpticalFlow.setWinSize", (char**)keywords, &pyobj_winSize) &&
        pyopencv_to(pyobj_winSize, winSize, ArgInfo("winSize", 0)) )
    {
        ERRWRAP2(_self_->setWinSize(winSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_SparsePyrLKOpticalFlow_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_create_cls, METH_CLASS), "create([, winSize[, maxLevel[, crit[, flags[, minEigThreshold]]]]]) -> retval\n."},
    {"getFlags", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getFlags, 0), "getFlags() -> retval\n."},
    {"getMaxLevel", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getMaxLevel, 0), "getMaxLevel() -> retval\n."},
    {"getMinEigThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getMinEigThreshold, 0), "getMinEigThreshold() -> retval\n."},
    {"getTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getTermCriteria, 0), "getTermCriteria() -> retval\n."},
    {"getWinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_getWinSize, 0), "getWinSize() -> retval\n."},
    {"setFlags", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setFlags, 0), "setFlags(flags) -> None\n."},
    {"setMaxLevel", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setMaxLevel, 0), "setMaxLevel(maxLevel) -> None\n."},
    {"setMinEigThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setMinEigThreshold, 0), "setMinEigThreshold(minEigThreshold) -> None\n."},
    {"setTermCriteria", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setTermCriteria, 0), "setTermCriteria(crit) -> None\n."},
    {"setWinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_SparsePyrLKOpticalFlow_setWinSize, 0), "setWinSize(winSize) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_SparsePyrLKOpticalFlow_specials(void)
{
    pyopencv_SparsePyrLKOpticalFlow_Type.tp_base = &pyopencv_SparseOpticalFlow_Type;
    pyopencv_SparsePyrLKOpticalFlow_Type.tp_dealloc = pyopencv_SparsePyrLKOpticalFlow_dealloc;
    pyopencv_SparsePyrLKOpticalFlow_Type.tp_repr = pyopencv_SparsePyrLKOpticalFlow_repr;
    pyopencv_SparsePyrLKOpticalFlow_Type.tp_getset = pyopencv_SparsePyrLKOpticalFlow_getseters;
    pyopencv_SparsePyrLKOpticalFlow_Type.tp_init = (initproc)0;
    pyopencv_SparsePyrLKOpticalFlow_Type.tp_methods = pyopencv_SparsePyrLKOpticalFlow_methods;
}

static PyObject* pyopencv_xphoto_WhiteBalancer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xphoto_WhiteBalancer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xphoto_WhiteBalancer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xphoto_xphoto_WhiteBalancer_balanceWhite(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::WhiteBalancer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_WhiteBalancer_Type))
        _self_ = dynamic_cast<cv::xphoto::WhiteBalancer*>(((pyopencv_xphoto_WhiteBalancer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_WhiteBalancer' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:xphoto_WhiteBalancer.balanceWhite", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->balanceWhite(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:xphoto_WhiteBalancer.balanceWhite", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->balanceWhite(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_xphoto_WhiteBalancer_methods[] =
{
    {"balanceWhite", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_WhiteBalancer_balanceWhite, 0), "balanceWhite(src[, dst]) -> dst\n.   @brief Applies white balancing to the input image\n.   \n.   @param src Input image\n.   @param dst White balancing result\n.   @sa cvtColor, equalizeHist"},

    {NULL,          NULL}
};

static void pyopencv_xphoto_WhiteBalancer_specials(void)
{
    pyopencv_xphoto_WhiteBalancer_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_xphoto_WhiteBalancer_Type.tp_dealloc = pyopencv_xphoto_WhiteBalancer_dealloc;
    pyopencv_xphoto_WhiteBalancer_Type.tp_repr = pyopencv_xphoto_WhiteBalancer_repr;
    pyopencv_xphoto_WhiteBalancer_Type.tp_getset = pyopencv_xphoto_WhiteBalancer_getseters;
    pyopencv_xphoto_WhiteBalancer_Type.tp_init = (initproc)0;
    pyopencv_xphoto_WhiteBalancer_Type.tp_methods = pyopencv_xphoto_WhiteBalancer_methods;
}

static PyObject* pyopencv_xphoto_SimpleWB_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xphoto_SimpleWB %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xphoto_SimpleWB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_getInputMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInputMax());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_getInputMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInputMin());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_getOutputMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOutputMax());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_getOutputMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOutputMin());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_getP(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getP());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_setInputMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_SimpleWB.setInputMax", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setInputMax(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_setInputMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_SimpleWB.setInputMin", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setInputMin(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_setOutputMax(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_SimpleWB.setOutputMax", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setOutputMax(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_setOutputMin(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_SimpleWB.setOutputMin", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setOutputMin(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_SimpleWB_setP(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::SimpleWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_SimpleWB_Type))
        _self_ = dynamic_cast<cv::xphoto::SimpleWB*>(((pyopencv_xphoto_SimpleWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_SimpleWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_SimpleWB.setP", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setP(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xphoto_SimpleWB_methods[] =
{
    {"getInputMax", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_getInputMax, 0), "getInputMax() -> retval\n.   @brief Input image range maximum value\n.   @see setInputMax"},
    {"getInputMin", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_getInputMin, 0), "getInputMin() -> retval\n.   @brief Input image range minimum value\n.   @see setInputMin"},
    {"getOutputMax", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_getOutputMax, 0), "getOutputMax() -> retval\n.   @brief Output image range maximum value\n.   @see setOutputMax"},
    {"getOutputMin", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_getOutputMin, 0), "getOutputMin() -> retval\n.   @brief Output image range minimum value\n.   @see setOutputMin"},
    {"getP", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_getP, 0), "getP() -> retval\n.   @brief Percent of top/bottom values to ignore\n.   @see setP"},
    {"setInputMax", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_setInputMax, 0), "setInputMax(val) -> None\n.   @copybrief getInputMax @see getInputMax"},
    {"setInputMin", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_setInputMin, 0), "setInputMin(val) -> None\n.   @copybrief getInputMin @see getInputMin"},
    {"setOutputMax", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_setOutputMax, 0), "setOutputMax(val) -> None\n.   @copybrief getOutputMax @see getOutputMax"},
    {"setOutputMin", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_setOutputMin, 0), "setOutputMin(val) -> None\n.   @copybrief getOutputMin @see getOutputMin"},
    {"setP", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_SimpleWB_setP, 0), "setP(val) -> None\n.   @copybrief getP @see getP"},

    {NULL,          NULL}
};

static void pyopencv_xphoto_SimpleWB_specials(void)
{
    pyopencv_xphoto_SimpleWB_Type.tp_base = &pyopencv_xphoto_WhiteBalancer_Type;
    pyopencv_xphoto_SimpleWB_Type.tp_dealloc = pyopencv_xphoto_SimpleWB_dealloc;
    pyopencv_xphoto_SimpleWB_Type.tp_repr = pyopencv_xphoto_SimpleWB_repr;
    pyopencv_xphoto_SimpleWB_Type.tp_getset = pyopencv_xphoto_SimpleWB_getseters;
    pyopencv_xphoto_SimpleWB_Type.tp_init = (initproc)0;
    pyopencv_xphoto_SimpleWB_Type.tp_methods = pyopencv_xphoto_SimpleWB_methods;
}

static PyObject* pyopencv_xphoto_GrayworldWB_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xphoto_GrayworldWB %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xphoto_GrayworldWB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xphoto_xphoto_GrayworldWB_getSaturationThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::GrayworldWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_GrayworldWB_Type))
        _self_ = dynamic_cast<cv::xphoto::GrayworldWB*>(((pyopencv_xphoto_GrayworldWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_GrayworldWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSaturationThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_GrayworldWB_setSaturationThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::GrayworldWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_GrayworldWB_Type))
        _self_ = dynamic_cast<cv::xphoto::GrayworldWB*>(((pyopencv_xphoto_GrayworldWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_GrayworldWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_GrayworldWB.setSaturationThreshold", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setSaturationThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xphoto_GrayworldWB_methods[] =
{
    {"getSaturationThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_GrayworldWB_getSaturationThreshold, 0), "getSaturationThreshold() -> retval\n.   @brief Maximum saturation for a pixel to be included in the\n.   gray-world assumption\n.   @see setSaturationThreshold"},
    {"setSaturationThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_GrayworldWB_setSaturationThreshold, 0), "setSaturationThreshold(val) -> None\n.   @copybrief getSaturationThreshold @see getSaturationThreshold"},

    {NULL,          NULL}
};

static void pyopencv_xphoto_GrayworldWB_specials(void)
{
    pyopencv_xphoto_GrayworldWB_Type.tp_base = &pyopencv_xphoto_WhiteBalancer_Type;
    pyopencv_xphoto_GrayworldWB_Type.tp_dealloc = pyopencv_xphoto_GrayworldWB_dealloc;
    pyopencv_xphoto_GrayworldWB_Type.tp_repr = pyopencv_xphoto_GrayworldWB_repr;
    pyopencv_xphoto_GrayworldWB_Type.tp_getset = pyopencv_xphoto_GrayworldWB_getseters;
    pyopencv_xphoto_GrayworldWB_Type.tp_init = (initproc)0;
    pyopencv_xphoto_GrayworldWB_Type.tp_methods = pyopencv_xphoto_GrayworldWB_methods;
}

static PyObject* pyopencv_xphoto_LearningBasedWB_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xphoto_LearningBasedWB %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xphoto_LearningBasedWB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_extractSimpleFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:xphoto_LearningBasedWB.extractSimpleFeatures", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->extractSimpleFeatures(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:xphoto_LearningBasedWB.extractSimpleFeatures", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->extractSimpleFeatures(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_getHistBinNum(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHistBinNum());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_getRangeMaxVal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRangeMaxVal());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_getSaturationThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSaturationThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_setHistBinNum(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xphoto_LearningBasedWB.setHistBinNum", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setHistBinNum(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_setRangeMaxVal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xphoto_LearningBasedWB.setRangeMaxVal", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRangeMaxVal(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xphoto_xphoto_LearningBasedWB_setSaturationThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xphoto;

    cv::xphoto::LearningBasedWB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xphoto_LearningBasedWB_Type))
        _self_ = dynamic_cast<cv::xphoto::LearningBasedWB*>(((pyopencv_xphoto_LearningBasedWB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xphoto_LearningBasedWB' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xphoto_LearningBasedWB.setSaturationThreshold", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setSaturationThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xphoto_LearningBasedWB_methods[] =
{
    {"extractSimpleFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_extractSimpleFeatures, 0), "extractSimpleFeatures(src[, dst]) -> dst\n.   @brief Implements the feature extraction part of the algorithm.\n.   \n.   In accordance with @cite Cheng2015 , computes the following features for the input image:\n.   1. Chromaticity of an average (R,G,B) tuple\n.   2. Chromaticity of the brightest (R,G,B) tuple (while ignoring saturated pixels)\n.   3. Chromaticity of the dominant (R,G,B) tuple (the one that has the highest value in the RGB histogram)\n.   4. Mode of the chromaticity palette, that is constructed by taking 300 most common colors according to\n.   the RGB histogram and projecting them on the chromaticity plane. Mode is the most high-density point\n.   of the palette, which is computed by a straightforward fixed-bandwidth kernel density estimator with\n.   a Epanechnikov kernel function.\n.   \n.   @param src Input three-channel image (BGR color space is assumed).\n.   @param dst An array of four (r,g) chromaticity tuples corresponding to the features listed above."},
    {"getHistBinNum", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_getHistBinNum, 0), "getHistBinNum() -> retval\n.   @brief Defines the size of one dimension of a three-dimensional RGB histogram that is used internally\n.   by the algorithm. It often makes sense to increase the number of bins for images with higher bit depth\n.   (e.g. 256 bins for a 12 bit image).\n.   @see setHistBinNum"},
    {"getRangeMaxVal", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_getRangeMaxVal, 0), "getRangeMaxVal() -> retval\n.   @brief Maximum possible value of the input image (e.g. 255 for 8 bit images,\n.   4095 for 12 bit images)\n.   @see setRangeMaxVal"},
    {"getSaturationThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_getSaturationThreshold, 0), "getSaturationThreshold() -> retval\n.   @brief Threshold that is used to determine saturated pixels, i.e. pixels where at least one of the\n.   channels exceeds \\f$\\texttt{saturation_threshold}\\times\\texttt{range_max_val}\\f$ are ignored.\n.   @see setSaturationThreshold"},
    {"setHistBinNum", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_setHistBinNum, 0), "setHistBinNum(val) -> None\n.   @copybrief getHistBinNum @see getHistBinNum"},
    {"setRangeMaxVal", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_setRangeMaxVal, 0), "setRangeMaxVal(val) -> None\n.   @copybrief getRangeMaxVal @see getRangeMaxVal"},
    {"setSaturationThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xphoto_xphoto_LearningBasedWB_setSaturationThreshold, 0), "setSaturationThreshold(val) -> None\n.   @copybrief getSaturationThreshold @see getSaturationThreshold"},

    {NULL,          NULL}
};

static void pyopencv_xphoto_LearningBasedWB_specials(void)
{
    pyopencv_xphoto_LearningBasedWB_Type.tp_base = &pyopencv_xphoto_WhiteBalancer_Type;
    pyopencv_xphoto_LearningBasedWB_Type.tp_dealloc = pyopencv_xphoto_LearningBasedWB_dealloc;
    pyopencv_xphoto_LearningBasedWB_Type.tp_repr = pyopencv_xphoto_LearningBasedWB_repr;
    pyopencv_xphoto_LearningBasedWB_Type.tp_getset = pyopencv_xphoto_LearningBasedWB_getseters;
    pyopencv_xphoto_LearningBasedWB_Type.tp_init = (initproc)0;
    pyopencv_xphoto_LearningBasedWB_Type.tp_methods = pyopencv_xphoto_LearningBasedWB_methods;
}

static PyObject* pyopencv_dnn_DictValue_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<dnn_DictValue %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_dnn_DictValue_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_dnn_dnn_DictValue_DictValue(pyopencv_dnn_DictValue_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    {
    int i=0;

    const char* keywords[] = { "i", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DictValue", (char**)keywords, &i) )
    {
        new (&(self->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::dnn::DictValue(i)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    double p=0;

    const char* keywords[] = { "p", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:DictValue", (char**)keywords, &p) )
    {
        new (&(self->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::dnn::DictValue(p)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_s = NULL;
    String s;

    const char* keywords[] = { "s", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DictValue", (char**)keywords, &pyobj_s) &&
        pyopencv_to(pyobj_s, s, ArgInfo("s", 0)) )
    {
        new (&(self->v)) Ptr<cv::dnn::DictValue>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::dnn::DictValue(s)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_dnn_dnn_DictValue_getIntValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::DictValue* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_DictValue_Type))
        _self_ = ((pyopencv_dnn_DictValue_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    int idx=-1;
    int retval;

    const char* keywords[] = { "idx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:dnn_DictValue.getIntValue", (char**)keywords, &idx) )
    {
        ERRWRAP2(retval = _self_->getIntValue(idx));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_DictValue_getRealValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::DictValue* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_DictValue_Type))
        _self_ = ((pyopencv_dnn_DictValue_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    int idx=-1;
    double retval;

    const char* keywords[] = { "idx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:dnn_DictValue.getRealValue", (char**)keywords, &idx) )
    {
        ERRWRAP2(retval = _self_->getRealValue(idx));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_DictValue_getStringValue(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::DictValue* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_DictValue_Type))
        _self_ = ((pyopencv_dnn_DictValue_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    int idx=-1;
    String retval;

    const char* keywords[] = { "idx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:dnn_DictValue.getStringValue", (char**)keywords, &idx) )
    {
        ERRWRAP2(retval = _self_->getStringValue(idx));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_DictValue_isInt(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::DictValue* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_DictValue_Type))
        _self_ = ((pyopencv_dnn_DictValue_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isInt());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_DictValue_isReal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::DictValue* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_DictValue_Type))
        _self_ = ((pyopencv_dnn_DictValue_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isReal());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_DictValue_isString(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::DictValue* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_DictValue_Type))
        _self_ = ((pyopencv_dnn_DictValue_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_DictValue' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isString());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_dnn_DictValue_methods[] =
{
    {"getIntValue", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_getIntValue, 0), "getIntValue([, idx]) -> retval\n."},
    {"getRealValue", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_getRealValue, 0), "getRealValue([, idx]) -> retval\n."},
    {"getStringValue", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_getStringValue, 0), "getStringValue([, idx]) -> retval\n."},
    {"isInt", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_isInt, 0), "isInt() -> retval\n."},
    {"isReal", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_isReal, 0), "isReal() -> retval\n."},
    {"isString", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_DictValue_isString, 0), "isString() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_dnn_DictValue_specials(void)
{
    pyopencv_dnn_DictValue_Type.tp_base = NULL;
    pyopencv_dnn_DictValue_Type.tp_dealloc = pyopencv_dnn_DictValue_dealloc;
    pyopencv_dnn_DictValue_Type.tp_repr = pyopencv_dnn_DictValue_repr;
    pyopencv_dnn_DictValue_Type.tp_getset = pyopencv_dnn_DictValue_getseters;
    pyopencv_dnn_DictValue_Type.tp_init = (initproc)pyopencv_cv_dnn_dnn_DictValue_DictValue;
    pyopencv_dnn_DictValue_Type.tp_methods = pyopencv_dnn_DictValue_methods;
}

static PyObject* pyopencv_dnn_Layer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<dnn_Layer %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_dnn_Layer_get_blobs(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return pyopencv_from(_self_->blobs);
}

static int pyopencv_dnn_Layer_set_blobs(pyopencv_dnn_Layer_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the blobs attribute");
        return -1;
    }
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (_self_ == NULL)
    {
        failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
        return -1;
    }
    return pyopencv_to(value, _self_->blobs) ? 0 : -1;
}

static PyObject* pyopencv_dnn_Layer_get_name(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return pyopencv_from(_self_->name);
}

static PyObject* pyopencv_dnn_Layer_get_preferableTarget(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return pyopencv_from(_self_->preferableTarget);
}

static PyObject* pyopencv_dnn_Layer_get_type(pyopencv_dnn_Layer_t* p, void *closure)
{
    cv::dnn::Layer* _self_ = dynamic_cast<cv::dnn::Layer*>(p->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of object (must be 'dnn_Layer' or its derivative)");
    return pyopencv_from(_self_->type);
}


static PyGetSetDef pyopencv_dnn_Layer_getseters[] =
{
    {(char*)"blobs", (getter)pyopencv_dnn_Layer_get_blobs, (setter)pyopencv_dnn_Layer_set_blobs, (char*)"blobs", NULL},
    {(char*)"name", (getter)pyopencv_dnn_Layer_get_name, NULL, (char*)"name", NULL},
    {(char*)"preferableTarget", (getter)pyopencv_dnn_Layer_get_preferableTarget, NULL, (char*)"preferableTarget", NULL},
    {(char*)"type", (getter)pyopencv_dnn_Layer_get_type, NULL, (char*)"type", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_dnn_dnn_Layer_finalize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Layer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Layer_Type))
        _self_ = dynamic_cast<cv::dnn::Layer*>(((pyopencv_dnn_Layer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Layer' or its derivative)");
    {
    PyObject* pyobj_inputs = NULL;
    vector_Mat inputs;
    PyObject* pyobj_outputs = NULL;
    vector_Mat outputs;

    const char* keywords[] = { "inputs", "outputs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:dnn_Layer.finalize", (char**)keywords, &pyobj_inputs, &pyobj_outputs) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        pyopencv_to(pyobj_outputs, outputs, ArgInfo("outputs", 1)) )
    {
        ERRWRAP2(_self_->finalize(inputs, outputs));
        return pyopencv_from(outputs);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputs = NULL;
    vector_Mat inputs;
    PyObject* pyobj_outputs = NULL;
    vector_Mat outputs;

    const char* keywords[] = { "inputs", "outputs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:dnn_Layer.finalize", (char**)keywords, &pyobj_inputs, &pyobj_outputs) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        pyopencv_to(pyobj_outputs, outputs, ArgInfo("outputs", 1)) )
    {
        ERRWRAP2(_self_->finalize(inputs, outputs));
        return pyopencv_from(outputs);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputs = NULL;
    vector_Mat inputs;
    std::vector<Mat> retval;

    const char* keywords[] = { "inputs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Layer.finalize", (char**)keywords, &pyobj_inputs) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) )
    {
        ERRWRAP2(retval = _self_->finalize(inputs));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputs = NULL;
    vector_Mat inputs;
    std::vector<Mat> retval;

    const char* keywords[] = { "inputs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Layer.finalize", (char**)keywords, &pyobj_inputs) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) )
    {
        ERRWRAP2(retval = _self_->finalize(inputs));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Layer_outputNameToIndex(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Layer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Layer_Type))
        _self_ = dynamic_cast<cv::dnn::Layer*>(((pyopencv_dnn_Layer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Layer' or its derivative)");
    PyObject* pyobj_outputName = NULL;
    String outputName;
    int retval;

    const char* keywords[] = { "outputName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Layer.outputNameToIndex", (char**)keywords, &pyobj_outputName) &&
        pyopencv_to(pyobj_outputName, outputName, ArgInfo("outputName", 0)) )
    {
        ERRWRAP2(retval = _self_->outputNameToIndex(outputName));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Layer_run(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Layer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Layer_Type))
        _self_ = dynamic_cast<cv::dnn::Layer*>(((pyopencv_dnn_Layer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Layer' or its derivative)");
    {
    PyObject* pyobj_inputs = NULL;
    vector_Mat inputs;
    PyObject* pyobj_outputs = NULL;
    vector_Mat outputs;
    PyObject* pyobj_internals = NULL;
    vector_Mat internals;

    const char* keywords[] = { "inputs", "internals", "outputs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:dnn_Layer.run", (char**)keywords, &pyobj_inputs, &pyobj_internals, &pyobj_outputs) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        pyopencv_to(pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        pyopencv_to(pyobj_internals, internals, ArgInfo("internals", 1)) )
    {
        ERRWRAP2(_self_->run(inputs, outputs, internals));
        return Py_BuildValue("(NN)", pyopencv_from(outputs), pyopencv_from(internals));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputs = NULL;
    vector_Mat inputs;
    PyObject* pyobj_outputs = NULL;
    vector_Mat outputs;
    PyObject* pyobj_internals = NULL;
    vector_Mat internals;

    const char* keywords[] = { "inputs", "internals", "outputs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:dnn_Layer.run", (char**)keywords, &pyobj_inputs, &pyobj_internals, &pyobj_outputs) &&
        pyopencv_to(pyobj_inputs, inputs, ArgInfo("inputs", 0)) &&
        pyopencv_to(pyobj_outputs, outputs, ArgInfo("outputs", 1)) &&
        pyopencv_to(pyobj_internals, internals, ArgInfo("internals", 1)) )
    {
        ERRWRAP2(_self_->run(inputs, outputs, internals));
        return Py_BuildValue("(NN)", pyopencv_from(outputs), pyopencv_from(internals));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_dnn_Layer_methods[] =
{
    {"finalize", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Layer_finalize, 0), "finalize(inputs[, outputs]) -> outputs\n.   @brief @overload\n\n\n\nfinalize(inputs) -> retval\n.   @brief @overload"},
    {"outputNameToIndex", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Layer_outputNameToIndex, 0), "outputNameToIndex(outputName) -> retval\n.   @brief Returns index of output blob in output array.\n.   *  @see inputNameToIndex()"},
    {"run", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Layer_run, 0), "run(inputs, internals[, outputs]) -> outputs, internals\n.   @brief Allocates layer and computes output."},

    {NULL,          NULL}
};

static void pyopencv_dnn_Layer_specials(void)
{
    pyopencv_dnn_Layer_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_dnn_Layer_Type.tp_dealloc = pyopencv_dnn_Layer_dealloc;
    pyopencv_dnn_Layer_Type.tp_repr = pyopencv_dnn_Layer_repr;
    pyopencv_dnn_Layer_Type.tp_getset = pyopencv_dnn_Layer_getseters;
    pyopencv_dnn_Layer_Type.tp_init = (initproc)0;
    pyopencv_dnn_Layer_Type.tp_methods = pyopencv_dnn_Layer_methods;
}

static PyObject* pyopencv_dnn_Net_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<dnn_Net %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_dnn_Net_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_dnn_dnn_Net_Net(pyopencv_dnn_Net_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::dnn::Net());
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_connect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_outPin = NULL;
    String outPin;
    PyObject* pyobj_inpPin = NULL;
    String inpPin;

    const char* keywords[] = { "outPin", "inpPin", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:dnn_Net.connect", (char**)keywords, &pyobj_outPin, &pyobj_inpPin) &&
        pyopencv_to(pyobj_outPin, outPin, ArgInfo("outPin", 0)) &&
        pyopencv_to(pyobj_inpPin, inpPin, ArgInfo("inpPin", 0)) )
    {
        ERRWRAP2(_self_->connect(outPin, inpPin));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_deleteLayer(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_layer = NULL;
    LayerId layer;

    const char* keywords[] = { "layer", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.deleteLayer", (char**)keywords, &pyobj_layer) &&
        pyopencv_to(pyobj_layer, layer, ArgInfo("layer", 0)) )
    {
        ERRWRAP2(_self_->deleteLayer(layer));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_enableFusion(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    bool fusion=0;

    const char* keywords[] = { "fusion", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:dnn_Net.enableFusion", (char**)keywords, &fusion) )
    {
        ERRWRAP2(_self_->enableFusion(fusion));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_forward(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    {
    PyObject* pyobj_outputName = NULL;
    String outputName;
    Mat retval;

    const char* keywords[] = { "outputName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:dnn_Net.forward", (char**)keywords, &pyobj_outputName) &&
        pyopencv_to(pyobj_outputName, outputName, ArgInfo("outputName", 0)) )
    {
        ERRWRAP2(retval = _self_->forward(outputName));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_outputBlobs = NULL;
    vector_Mat outputBlobs;
    PyObject* pyobj_outputName = NULL;
    String outputName;

    const char* keywords[] = { "outputBlobs", "outputName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:dnn_Net.forward", (char**)keywords, &pyobj_outputBlobs, &pyobj_outputName) &&
        pyopencv_to(pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        pyopencv_to(pyobj_outputName, outputName, ArgInfo("outputName", 0)) )
    {
        ERRWRAP2(_self_->forward(outputBlobs, outputName));
        return pyopencv_from(outputBlobs);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_outputBlobs = NULL;
    vector_Mat outputBlobs;
    PyObject* pyobj_outputName = NULL;
    String outputName;

    const char* keywords[] = { "outputBlobs", "outputName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:dnn_Net.forward", (char**)keywords, &pyobj_outputBlobs, &pyobj_outputName) &&
        pyopencv_to(pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        pyopencv_to(pyobj_outputName, outputName, ArgInfo("outputName", 0)) )
    {
        ERRWRAP2(_self_->forward(outputBlobs, outputName));
        return pyopencv_from(outputBlobs);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_outputBlobs = NULL;
    vector_Mat outputBlobs;
    PyObject* pyobj_outBlobNames = NULL;
    vector_String outBlobNames;

    const char* keywords[] = { "outBlobNames", "outputBlobs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:dnn_Net.forward", (char**)keywords, &pyobj_outBlobNames, &pyobj_outputBlobs) &&
        pyopencv_to(pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        pyopencv_to(pyobj_outBlobNames, outBlobNames, ArgInfo("outBlobNames", 0)) )
    {
        ERRWRAP2(_self_->forward(outputBlobs, outBlobNames));
        return pyopencv_from(outputBlobs);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_outputBlobs = NULL;
    vector_Mat outputBlobs;
    PyObject* pyobj_outBlobNames = NULL;
    vector_String outBlobNames;

    const char* keywords[] = { "outBlobNames", "outputBlobs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:dnn_Net.forward", (char**)keywords, &pyobj_outBlobNames, &pyobj_outputBlobs) &&
        pyopencv_to(pyobj_outputBlobs, outputBlobs, ArgInfo("outputBlobs", 1)) &&
        pyopencv_to(pyobj_outBlobNames, outBlobNames, ArgInfo("outBlobNames", 0)) )
    {
        ERRWRAP2(_self_->forward(outputBlobs, outBlobNames));
        return pyopencv_from(outputBlobs);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_forwardAndRetrieve(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    vector_vector_Mat outputBlobs;
    PyObject* pyobj_outBlobNames = NULL;
    vector_String outBlobNames;

    const char* keywords[] = { "outBlobNames", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.forwardAndRetrieve", (char**)keywords, &pyobj_outBlobNames) &&
        pyopencv_to(pyobj_outBlobNames, outBlobNames, ArgInfo("outBlobNames", 0)) )
    {
        ERRWRAP2(_self_->forward(outputBlobs, outBlobNames));
        return pyopencv_from(outputBlobs);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getFLOPS(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    {
    PyObject* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    int64 retval;

    const char* keywords[] = { "netInputShapes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getFLOPS", (char**)keywords, &pyobj_netInputShapes) &&
        pyopencv_to(pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)) )
    {
        ERRWRAP2(retval = _self_->getFLOPS(netInputShapes));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    int64 retval;

    const char* keywords[] = { "netInputShape", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getFLOPS", (char**)keywords, &pyobj_netInputShape) &&
        pyopencv_to(pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)) )
    {
        ERRWRAP2(retval = _self_->getFLOPS(netInputShape));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int layerId=0;
    PyObject* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    int64 retval;

    const char* keywords[] = { "layerId", "netInputShapes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iO:dnn_Net.getFLOPS", (char**)keywords, &layerId, &pyobj_netInputShapes) &&
        pyopencv_to(pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)) )
    {
        ERRWRAP2(retval = _self_->getFLOPS(layerId, netInputShapes));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int layerId=0;
    PyObject* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    int64 retval;

    const char* keywords[] = { "layerId", "netInputShape", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iO:dnn_Net.getFLOPS", (char**)keywords, &layerId, &pyobj_netInputShape) &&
        pyopencv_to(pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)) )
    {
        ERRWRAP2(retval = _self_->getFLOPS(layerId, netInputShape));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getLayer(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_layerId = NULL;
    LayerId layerId;
    Ptr<Layer> retval;

    const char* keywords[] = { "layerId", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getLayer", (char**)keywords, &pyobj_layerId) &&
        pyopencv_to(pyobj_layerId, layerId, ArgInfo("layerId", 0)) )
    {
        ERRWRAP2(retval = _self_->getLayer(layerId));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getLayerId(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_layer = NULL;
    String layer;
    int retval;

    const char* keywords[] = { "layer", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getLayerId", (char**)keywords, &pyobj_layer) &&
        pyopencv_to(pyobj_layer, layer, ArgInfo("layer", 0)) )
    {
        ERRWRAP2(retval = _self_->getLayerId(layer));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getLayerNames(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    std::vector<String> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLayerNames());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getLayerTypes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    vector_String layersTypes;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->getLayerTypes(layersTypes));
        return pyopencv_from(layersTypes);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getLayersCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_layerType = NULL;
    String layerType;
    int retval;

    const char* keywords[] = { "layerType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getLayersCount", (char**)keywords, &pyobj_layerType) &&
        pyopencv_to(pyobj_layerType, layerType, ArgInfo("layerType", 0)) )
    {
        ERRWRAP2(retval = _self_->getLayersCount(layerType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getLayersShapes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    {
    PyObject* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    vector_int layersIds;
    vector_vector_MatShape inLayersShapes;
    vector_vector_MatShape outLayersShapes;

    const char* keywords[] = { "netInputShapes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getLayersShapes", (char**)keywords, &pyobj_netInputShapes) &&
        pyopencv_to(pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)) )
    {
        ERRWRAP2(_self_->getLayersShapes(netInputShapes, layersIds, inLayersShapes, outLayersShapes));
        return Py_BuildValue("(NNN)", pyopencv_from(layersIds), pyopencv_from(inLayersShapes), pyopencv_from(outLayersShapes));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    vector_int layersIds;
    vector_vector_MatShape inLayersShapes;
    vector_vector_MatShape outLayersShapes;

    const char* keywords[] = { "netInputShape", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getLayersShapes", (char**)keywords, &pyobj_netInputShape) &&
        pyopencv_to(pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)) )
    {
        ERRWRAP2(_self_->getLayersShapes(netInputShape, layersIds, inLayersShapes, outLayersShapes));
        return Py_BuildValue("(NNN)", pyopencv_from(layersIds), pyopencv_from(inLayersShapes), pyopencv_from(outLayersShapes));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getMemoryConsumption(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    {
    PyObject* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    size_t weights;
    size_t blobs;

    const char* keywords[] = { "netInputShape", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.getMemoryConsumption", (char**)keywords, &pyobj_netInputShape) &&
        pyopencv_to(pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)) )
    {
        ERRWRAP2(_self_->getMemoryConsumption(netInputShape, weights, blobs));
        return Py_BuildValue("(NN)", pyopencv_from(weights), pyopencv_from(blobs));
    }
    }
    PyErr_Clear();

    {
    int layerId=0;
    PyObject* pyobj_netInputShapes = NULL;
    vector_MatShape netInputShapes;
    size_t weights;
    size_t blobs;

    const char* keywords[] = { "layerId", "netInputShapes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iO:dnn_Net.getMemoryConsumption", (char**)keywords, &layerId, &pyobj_netInputShapes) &&
        pyopencv_to(pyobj_netInputShapes, netInputShapes, ArgInfo("netInputShapes", 0)) )
    {
        ERRWRAP2(_self_->getMemoryConsumption(layerId, netInputShapes, weights, blobs));
        return Py_BuildValue("(NN)", pyopencv_from(weights), pyopencv_from(blobs));
    }
    }
    PyErr_Clear();

    {
    int layerId=0;
    PyObject* pyobj_netInputShape = NULL;
    MatShape netInputShape;
    size_t weights;
    size_t blobs;

    const char* keywords[] = { "layerId", "netInputShape", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iO:dnn_Net.getMemoryConsumption", (char**)keywords, &layerId, &pyobj_netInputShape) &&
        pyopencv_to(pyobj_netInputShape, netInputShape, ArgInfo("netInputShape", 0)) )
    {
        ERRWRAP2(_self_->getMemoryConsumption(layerId, netInputShape, weights, blobs));
        return Py_BuildValue("(NN)", pyopencv_from(weights), pyopencv_from(blobs));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getParam(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_layer = NULL;
    LayerId layer;
    int numParam=0;
    Mat retval;

    const char* keywords[] = { "layer", "numParam", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|i:dnn_Net.getParam", (char**)keywords, &pyobj_layer, &numParam) &&
        pyopencv_to(pyobj_layer, layer, ArgInfo("layer", 0)) )
    {
        ERRWRAP2(retval = _self_->getParam(layer, numParam));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getPerfProfile(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    vector_double timings;
    int64 retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPerfProfile(timings));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(timings));
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_getUnconnectedOutLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    std::vector<int> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUnconnectedOutLayers());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_readFromModelOptimizer_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    PyObject* pyobj_xml = NULL;
    String xml;
    PyObject* pyobj_bin = NULL;
    String bin;
    Net retval;

    const char* keywords[] = { "xml", "bin", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:dnn_Net.readFromModelOptimizer", (char**)keywords, &pyobj_xml, &pyobj_bin) &&
        pyopencv_to(pyobj_xml, xml, ArgInfo("xml", 0)) &&
        pyopencv_to(pyobj_bin, bin, ArgInfo("bin", 0)) )
    {
        ERRWRAP2(retval = cv::dnn::Net::readFromModelOptimizer(xml, bin));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_setHalideScheduler(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_scheduler = NULL;
    String scheduler;

    const char* keywords[] = { "scheduler", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.setHalideScheduler", (char**)keywords, &pyobj_scheduler) &&
        pyopencv_to(pyobj_scheduler, scheduler, ArgInfo("scheduler", 0)) )
    {
        ERRWRAP2(_self_->setHalideScheduler(scheduler));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_setInput(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    {
    PyObject* pyobj_blob = NULL;
    Mat blob;
    PyObject* pyobj_name = NULL;
    String name="";
    double scalefactor=1.0;
    PyObject* pyobj_mean = NULL;
    Scalar mean;

    const char* keywords[] = { "blob", "name", "scalefactor", "mean", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OdO:dnn_Net.setInput", (char**)keywords, &pyobj_blob, &pyobj_name, &scalefactor, &pyobj_mean) &&
        pyopencv_to(pyobj_blob, blob, ArgInfo("blob", 0)) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) &&
        pyopencv_to(pyobj_mean, mean, ArgInfo("mean", 0)) )
    {
        ERRWRAP2(_self_->setInput(blob, name, scalefactor, mean));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_blob = NULL;
    UMat blob;
    PyObject* pyobj_name = NULL;
    String name="";
    double scalefactor=1.0;
    PyObject* pyobj_mean = NULL;
    Scalar mean;

    const char* keywords[] = { "blob", "name", "scalefactor", "mean", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OdO:dnn_Net.setInput", (char**)keywords, &pyobj_blob, &pyobj_name, &scalefactor, &pyobj_mean) &&
        pyopencv_to(pyobj_blob, blob, ArgInfo("blob", 0)) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) &&
        pyopencv_to(pyobj_mean, mean, ArgInfo("mean", 0)) )
    {
        ERRWRAP2(_self_->setInput(blob, name, scalefactor, mean));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_setInputsNames(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    PyObject* pyobj_inputBlobNames = NULL;
    vector_String inputBlobNames;

    const char* keywords[] = { "inputBlobNames", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:dnn_Net.setInputsNames", (char**)keywords, &pyobj_inputBlobNames) &&
        pyopencv_to(pyobj_inputBlobNames, inputBlobNames, ArgInfo("inputBlobNames", 0)) )
    {
        ERRWRAP2(_self_->setInputsNames(inputBlobNames));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_setParam(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    {
    PyObject* pyobj_layer = NULL;
    LayerId layer;
    int numParam=0;
    PyObject* pyobj_blob = NULL;
    Mat blob;

    const char* keywords[] = { "layer", "numParam", "blob", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO:dnn_Net.setParam", (char**)keywords, &pyobj_layer, &numParam, &pyobj_blob) &&
        pyopencv_to(pyobj_layer, layer, ArgInfo("layer", 0)) &&
        pyopencv_to(pyobj_blob, blob, ArgInfo("blob", 0)) )
    {
        ERRWRAP2(_self_->setParam(layer, numParam, blob));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_layer = NULL;
    LayerId layer;
    int numParam=0;
    PyObject* pyobj_blob = NULL;
    Mat blob;

    const char* keywords[] = { "layer", "numParam", "blob", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiO:dnn_Net.setParam", (char**)keywords, &pyobj_layer, &numParam, &pyobj_blob) &&
        pyopencv_to(pyobj_layer, layer, ArgInfo("layer", 0)) &&
        pyopencv_to(pyobj_blob, blob, ArgInfo("blob", 0)) )
    {
        ERRWRAP2(_self_->setParam(layer, numParam, blob));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_setPreferableBackend(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    int backendId=0;

    const char* keywords[] = { "backendId", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:dnn_Net.setPreferableBackend", (char**)keywords, &backendId) )
    {
        ERRWRAP2(_self_->setPreferableBackend(backendId));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_dnn_dnn_Net_setPreferableTarget(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::dnn;

    cv::dnn::Net* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_dnn_Net_Type))
        _self_ = &((pyopencv_dnn_Net_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'dnn_Net' or its derivative)");
    int targetId=0;

    const char* keywords[] = { "targetId", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:dnn_Net.setPreferableTarget", (char**)keywords, &targetId) )
    {
        ERRWRAP2(_self_->setPreferableTarget(targetId));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_dnn_Net_methods[] =
{
    {"connect", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_connect, 0), "connect(outPin, inpPin) -> None\n.   @brief Connects output of the first layer to input of the second layer.\n.   *  @param outPin descriptor of the first layer output.\n.   *  @param inpPin descriptor of the second layer input.\n.   *\n.   * Descriptors have the following template <DFN>&lt;layer_name&gt;[.input_number]</DFN>:\n.   * - the first part of the template <DFN>layer_name</DFN> is sting name of the added layer.\n.   *   If this part is empty then the network input pseudo layer will be used;\n.   * - the second optional part of the template <DFN>input_number</DFN>\n.   *   is either number of the layer input, either label one.\n.   *   If this part is omitted then the first layer input will be used.\n.   *\n.   *  @see setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()"},
    {"deleteLayer", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_deleteLayer, 0), "deleteLayer(layer) -> None\n.   @brief Delete layer for the network (not implemented yet)"},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_empty, 0), "empty() -> retval\n.   Returns true if there are no layers in the network."},
    {"enableFusion", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_enableFusion, 0), "enableFusion(fusion) -> None\n.   @brief Enables or disables layer fusion in the network.\n.   * @param fusion true to enable the fusion, false to disable. The fusion is enabled by default."},
    {"forward", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_forward, 0), "forward([, outputName]) -> retval\n.   @brief Runs forward pass to compute output of layer with name @p outputName.\n.   *  @param outputName name for layer which output is needed to get\n.   *  @return blob for first output of specified layer.\n.   *  @details By default runs forward pass for the whole network.\n\n\n\nforward([, outputBlobs[, outputName]]) -> outputBlobs\n.   @brief Runs forward pass to compute output of layer with name @p outputName.\n.   *  @param outputBlobs contains all output blobs for specified layer.\n.   *  @param outputName name for layer which output is needed to get\n.   *  @details If @p outputName is empty, runs forward pass for the whole network.\n\n\n\nforward(outBlobNames[, outputBlobs]) -> outputBlobs\n.   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n.   *  @param outputBlobs contains blobs for first outputs of specified layers.\n.   *  @param outBlobNames names for layers which outputs are needed to get"},
    {"forwardAndRetrieve", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_forwardAndRetrieve, 0), "forwardAndRetrieve(outBlobNames) -> outputBlobs\n.   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n.   *  @param outputBlobs contains all output blobs for each layer specified in @p outBlobNames.\n.   *  @param outBlobNames names for layers which outputs are needed to get"},
    {"getFLOPS", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getFLOPS, 0), "getFLOPS(netInputShapes) -> retval\n.   @brief Computes FLOP for whole loaded model with specified input shapes.\n.   * @param netInputShapes vector of shapes for all net inputs.\n.   * @returns computed FLOP.\n\n\n\ngetFLOPS(netInputShape) -> retval\n.   @overload\n\n\n\ngetFLOPS(layerId, netInputShapes) -> retval\n.   @overload\n\n\n\ngetFLOPS(layerId, netInputShape) -> retval\n.   @overload"},
    {"getLayer", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayer, 0), "getLayer(layerId) -> retval\n.   @brief Returns pointer to layer with specified id or name which the network use."},
    {"getLayerId", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayerId, 0), "getLayerId(layer) -> retval\n.   @brief Converts string name of the layer to the integer identifier.\n.   *  @returns id of the layer, or -1 if the layer wasn't found."},
    {"getLayerNames", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayerNames, 0), "getLayerNames() -> retval\n."},
    {"getLayerTypes", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayerTypes, 0), "getLayerTypes() -> layersTypes\n.   @brief Returns list of types for layer used in model.\n.   * @param layersTypes output parameter for returning types."},
    {"getLayersCount", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayersCount, 0), "getLayersCount(layerType) -> retval\n.   @brief Returns count of layers of specified type.\n.   * @param layerType type.\n.   * @returns count of layers"},
    {"getLayersShapes", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getLayersShapes, 0), "getLayersShapes(netInputShapes) -> layersIds, inLayersShapes, outLayersShapes\n.   @brief Returns input and output shapes for all layers in loaded model;\n.   *  preliminary inferencing isn't necessary.\n.   *  @param netInputShapes shapes for all input blobs in net input layer.\n.   *  @param layersIds output parameter for layer IDs.\n.   *  @param inLayersShapes output parameter for input layers shapes;\n.   * order is the same as in layersIds\n.   *  @param outLayersShapes output parameter for output layers shapes;\n.   * order is the same as in layersIds\n\n\n\ngetLayersShapes(netInputShape) -> layersIds, inLayersShapes, outLayersShapes\n.   @overload"},
    {"getMemoryConsumption", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getMemoryConsumption, 0), "getMemoryConsumption(netInputShape) -> weights, blobs\n.   @overload\n\n\n\ngetMemoryConsumption(layerId, netInputShapes) -> weights, blobs\n.   @overload\n\n\n\ngetMemoryConsumption(layerId, netInputShape) -> weights, blobs\n.   @overload"},
    {"getParam", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getParam, 0), "getParam(layer[, numParam]) -> retval\n.   @brief Returns parameter blob of the layer.\n.   *  @param layer name or id of the layer.\n.   *  @param numParam index of the layer parameter in the Layer::blobs array.\n.   *  @see Layer::blobs"},
    {"getPerfProfile", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getPerfProfile, 0), "getPerfProfile() -> retval, timings\n.   @brief Returns overall time for inference and timings (in ticks) for layers.\n.   * Indexes in returned vector correspond to layers ids. Some layers can be fused with others,\n.   * in this case zero ticks count will be return for that skipped layers.\n.   * @param timings vector for tick timings for all layers.\n.   * @return overall ticks for model inference."},
    {"getUnconnectedOutLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_getUnconnectedOutLayers, 0), "getUnconnectedOutLayers() -> retval\n.   @brief Returns indexes of layers with unconnected outputs."},
    {"readFromModelOptimizer", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_readFromModelOptimizer_cls, METH_CLASS), "readFromModelOptimizer(xml, bin) -> retval\n.   @brief Create a network from Intel's Model Optimizer intermediate representation.\n.   *  @param[in] xml XML configuration file with network's topology.\n.   *  @param[in] bin Binary file with trained weights.\n.   *  Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine\n.   *  backend."},
    {"setHalideScheduler", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setHalideScheduler, 0), "setHalideScheduler(scheduler) -> None\n.   * @brief Compile Halide layers.\n.   * @param[in] scheduler Path to YAML file with scheduling directives.\n.   * @see setPreferableBackend\n.   *\n.   * Schedule layers that support Halide backend. Then compile them for\n.   * specific target. For layers that not represented in scheduling file\n.   * or if no manual scheduling used at all, automatic scheduling will be applied."},
    {"setInput", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setInput, 0), "setInput(blob[, name[, scalefactor[, mean]]]) -> None\n.   @brief Sets the new input value for the network\n.   *  @param blob        A new blob. Should have CV_32F or CV_8U depth.\n.   *  @param name        A name of input layer.\n.   *  @param scalefactor An optional normalization scale.\n.   *  @param mean        An optional mean subtraction values.\n.   *  @see connect(String, String) to know format of the descriptor.\n.   *\n.   *  If scale or mean values are specified, a final input blob is computed\n.   *  as:\n.   * \\f[input(n,c,h,w) = scalefactor \\times (blob(n,c,h,w) - mean_c)\\f]"},
    {"setInputsNames", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setInputsNames, 0), "setInputsNames(inputBlobNames) -> None\n.   @brief Sets outputs names of the network input pseudo layer.\n.   *\n.   * Each net always has special own the network input pseudo layer with id=0.\n.   * This layer stores the user blobs only and don't make any computations.\n.   * In fact, this layer provides the only way to pass user data into the network.\n.   * As any other layer, this layer can label its outputs and this function provides an easy way to do this."},
    {"setParam", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setParam, 0), "setParam(layer, numParam, blob) -> None\n.   @brief Sets the new value for the learned param of the layer.\n.   *  @param layer name or id of the layer.\n.   *  @param numParam index of the layer parameter in the Layer::blobs array.\n.   *  @param blob the new value.\n.   *  @see Layer::blobs\n.   *  @note If shape of the new blob differs from the previous shape,\n.   *  then the following forward pass may fail."},
    {"setPreferableBackend", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setPreferableBackend, 0), "setPreferableBackend(backendId) -> None\n.   * @brief Ask network to use specific computation backend where it supported.\n.   * @param[in] backendId backend identifier.\n.   * @see Backend\n.   *\n.   * If OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT\n.   * means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV."},
    {"setPreferableTarget", CV_PY_FN_WITH_KW_(pyopencv_cv_dnn_dnn_Net_setPreferableTarget, 0), "setPreferableTarget(targetId) -> None\n.   * @brief Ask network to make computations on specific target device.\n.   * @param[in] targetId target identifier.\n.   * @see Target\n.   *\n.   * List of supported combinations backend / target:\n.   * |                        | DNN_BACKEND_OPENCV | DNN_BACKEND_INFERENCE_ENGINE | DNN_BACKEND_HALIDE |\n.   * |------------------------|--------------------|------------------------------|--------------------|\n.   * | DNN_TARGET_CPU         |                  + |                            + |                  + |\n.   * | DNN_TARGET_OPENCL      |                  + |                            + |                  + |\n.   * | DNN_TARGET_OPENCL_FP16 |                  + |                            + |                    |\n.   * | DNN_TARGET_MYRIAD      |                    |                            + |                    |"},

    {NULL,          NULL}
};

static void pyopencv_dnn_Net_specials(void)
{
    pyopencv_dnn_Net_Type.tp_base = NULL;
    pyopencv_dnn_Net_Type.tp_dealloc = pyopencv_dnn_Net_dealloc;
    pyopencv_dnn_Net_Type.tp_repr = pyopencv_dnn_Net_repr;
    pyopencv_dnn_Net_Type.tp_getset = pyopencv_dnn_Net_getseters;
    pyopencv_dnn_Net_Type.tp_init = (initproc)pyopencv_cv_dnn_dnn_Net_Net;
    pyopencv_dnn_Net_Type.tp_methods = pyopencv_dnn_Net_methods;
}

static PyObject* pyopencv_face_FaceRecognizer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_FaceRecognizer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_FaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_FaceRecognizer_getLabelInfo(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    int label=0;
    String retval;

    const char* keywords[] = { "label", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:face_FaceRecognizer.getLabelInfo", (char**)keywords, &label) )
    {
        ERRWRAP2(retval = _self_->getLabelInfo(label));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_getLabelsByString(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    PyObject* pyobj_str = NULL;
    String str;
    std::vector<int> retval;

    const char* keywords[] = { "str", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.getLabelsByString", (char**)keywords, &pyobj_str) &&
        pyopencv_to(pyobj_str, str, ArgInfo("str", 0)) )
    {
        ERRWRAP2(retval = _self_->getLabelsByString(str));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_predict(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    int label;
    double confidence;

    const char* keywords[] = { "src", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.predict", (char**)keywords, &pyobj_src) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(_self_->predict(src, label, confidence));
        return Py_BuildValue("(NN)", pyopencv_from(label), pyopencv_from(confidence));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    int label;
    double confidence;

    const char* keywords[] = { "src", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.predict", (char**)keywords, &pyobj_src) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(_self_->predict(src, label, confidence));
        return Py_BuildValue("(NN)", pyopencv_from(label), pyopencv_from(confidence));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_predict_collect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_collector = NULL;
    Ptr<PredictCollector> collector;

    const char* keywords[] = { "src", "collector", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:face_FaceRecognizer.predict_collect", (char**)keywords, &pyobj_src, &pyobj_collector) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_collector, collector, ArgInfo("collector", 0)) )
    {
        ERRWRAP2(_self_->predict(src, collector));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_collector = NULL;
    Ptr<PredictCollector> collector;

    const char* keywords[] = { "src", "collector", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:face_FaceRecognizer.predict_collect", (char**)keywords, &pyobj_src, &pyobj_collector) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_collector, collector, ArgInfo("collector", 0)) )
    {
        ERRWRAP2(_self_->predict(src, collector));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_predict_label(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    int retval;

    const char* keywords[] = { "src", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.predict_label", (char**)keywords, &pyobj_src) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(retval = _self_->predict(src));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    int retval;

    const char* keywords[] = { "src", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.predict_label", (char**)keywords, &pyobj_src) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(retval = _self_->predict(src));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.read", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(_self_->read(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_setLabelInfo(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    int label=0;
    PyObject* pyobj_strInfo = NULL;
    String strInfo;

    const char* keywords[] = { "label", "strInfo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iO:face_FaceRecognizer.setLabelInfo", (char**)keywords, &label, &pyobj_strInfo) &&
        pyopencv_to(pyobj_strInfo, strInfo, ArgInfo("strInfo", 0)) )
    {
        ERRWRAP2(_self_->setLabelInfo(label, strInfo));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_train(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_labels = NULL;
    Mat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:face_FaceRecognizer.train", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 0)) )
    {
        ERRWRAP2(_self_->train(src, labels));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_labels = NULL;
    UMat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:face_FaceRecognizer.train", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 0)) )
    {
        ERRWRAP2(_self_->train(src, labels));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_update(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_labels = NULL;
    Mat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:face_FaceRecognizer.update", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 0)) )
    {
        ERRWRAP2(_self_->update(src, labels));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    vector_Mat src;
    PyObject* pyobj_labels = NULL;
    UMat labels;

    const char* keywords[] = { "src", "labels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:face_FaceRecognizer.update", (char**)keywords, &pyobj_src, &pyobj_labels) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_labels, labels, ArgInfo("labels", 0)) )
    {
        ERRWRAP2(_self_->update(src, labels));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_FaceRecognizer_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::FaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_FaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::FaceRecognizer*>(((pyopencv_face_FaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_FaceRecognizer' or its derivative)");
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_FaceRecognizer.write", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(_self_->write(filename));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_face_FaceRecognizer_methods[] =
{
    {"getLabelInfo", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_getLabelInfo, 0), "getLabelInfo(label) -> retval\n.   @brief Gets string information by label.\n.   \n.   If an unknown label id is provided or there is no label information associated with the specified\n.   label id the method returns an empty string."},
    {"getLabelsByString", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_getLabelsByString, 0), "getLabelsByString(str) -> retval\n.   @brief Gets vector of labels by string.\n.   \n.   The function searches for the labels containing the specified sub-string in the associated string\n.   info."},
    {"predict", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_predict, 0), "predict(src) -> label, confidence\n.   @brief Predicts a label and associated confidence (e.g. distance) for a given input image.\n.   \n.   @param src Sample image to get a prediction from.\n.   @param label The predicted label for the given image.\n.   @param confidence Associated confidence (e.g. distance) for the predicted label.\n.   \n.   The suffix const means that prediction does not affect the internal model state, so the method can\n.   be safely called from within different threads.\n.   \n.   The following example shows how to get a prediction from a trained model:\n.   \n.   @code\n.   using namespace cv;\n.   // Do your initialization here (create the cv::FaceRecognizer model) ...\n.   // ...\n.   // Read in a sample image:\n.   Mat img = imread(\"person1/3.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\n.   // And get a prediction from the cv::FaceRecognizer:\n.   int predicted = model->predict(img);\n.   @endcode\n.   \n.   Or to get a prediction and the associated confidence (e.g. distance):\n.   \n.   @code\n.   using namespace cv;\n.   // Do your initialization here (create the cv::FaceRecognizer model) ...\n.   // ...\n.   Mat img = imread(\"person1/3.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\n.   // Some variables for the predicted label and associated confidence (e.g. distance):\n.   int predicted_label = -1;\n.   double predicted_confidence = 0.0;\n.   // Get the prediction and associated confidence from the model\n.   model->predict(img, predicted_label, predicted_confidence);\n.   @endcode"},
    {"predict_collect", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_predict_collect, 0), "predict_collect(src, collector) -> None\n.   @brief - if implemented - send all result of prediction to collector that can be used for somehow custom result handling\n.   @param src Sample image to get a prediction from.\n.   @param collector User-defined collector object that accepts all results\n.   \n.   To implement this method u just have to do same internal cycle as in predict(InputArray src, CV_OUT int &label, CV_OUT double &confidence) but\n.   not try to get \"best@ result, just resend it to caller side with given collector"},
    {"predict_label", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_predict_label, 0), "predict_label(src) -> retval\n.   @overload"},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_read, 0), "read(filename) -> None\n.   @brief Loads a FaceRecognizer and its model state.\n.   \n.   Loads a persisted model and state from a given XML or YAML file . Every FaceRecognizer has to\n.   overwrite FaceRecognizer::load(FileStorage& fs) to enable loading the model state.\n.   FaceRecognizer::load(FileStorage& fs) in turn gets called by\n.   FaceRecognizer::load(const String& filename), to ease saving a model."},
    {"setLabelInfo", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_setLabelInfo, 0), "setLabelInfo(label, strInfo) -> None\n.   @brief Sets string info for the specified model's label.\n.   \n.   The string info is replaced by the provided value if it was set before for the specified label."},
    {"train", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_train, 0), "train(src, labels) -> None\n.   @brief Trains a FaceRecognizer with given data and associated labels.\n.   \n.   @param src The training images, that means the faces you want to learn. The data has to be\n.   given as a vector\\<Mat\\>.\n.   @param labels The labels corresponding to the images have to be given either as a vector\\<int\\>\n.   or a\n.   \n.   The following source code snippet shows you how to learn a Fisherfaces model on a given set of\n.   images. The images are read with imread and pushed into a std::vector\\<Mat\\>. The labels of each\n.   image are stored within a std::vector\\<int\\> (you could also use a Mat of type CV_32SC1). Think of\n.   the label as the subject (the person) this image belongs to, so same subjects (persons) should have\n.   the same label. For the available FaceRecognizer you don't have to pay any attention to the order of\n.   the labels, just make sure same persons have the same label:\n.   \n.   @code\n.   // holds images and labels\n.   vector<Mat> images;\n.   vector<int> labels;\n.   // images for first person\n.   images.push_back(imread(\"person0/0.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(0);\n.   images.push_back(imread(\"person0/1.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(0);\n.   images.push_back(imread(\"person0/2.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(0);\n.   // images for second person\n.   images.push_back(imread(\"person1/0.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);\n.   images.push_back(imread(\"person1/1.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);\n.   images.push_back(imread(\"person1/2.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);\n.   @endcode\n.   \n.   Now that you have read some images, we can create a new FaceRecognizer. In this example I'll create\n.   a Fisherfaces model and decide to keep all of the possible Fisherfaces:\n.   \n.   @code\n.   // Create a new Fisherfaces model and retain all available Fisherfaces,\n.   // this is the most common usage of this specific FaceRecognizer:\n.   //\n.   Ptr<FaceRecognizer> model =  FisherFaceRecognizer::create();\n.   @endcode\n.   \n.   And finally train it on the given dataset (the face images and labels):\n.   \n.   @code\n.   // This is the common interface to train all of the available cv::FaceRecognizer\n.   // implementations:\n.   //\n.   model->train(images, labels);\n.   @endcode"},
    {"update", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_update, 0), "update(src, labels) -> None\n.   @brief Updates a FaceRecognizer with given data and associated labels.\n.   \n.   @param src The training images, that means the faces you want to learn. The data has to be given\n.   as a vector\\<Mat\\>.\n.   @param labels The labels corresponding to the images have to be given either as a vector\\<int\\> or\n.   a\n.   \n.   This method updates a (probably trained) FaceRecognizer, but only if the algorithm supports it. The\n.   Local Binary Patterns Histograms (LBPH) recognizer (see createLBPHFaceRecognizer) can be updated.\n.   For the Eigenfaces and Fisherfaces method, this is algorithmically not possible and you have to\n.   re-estimate the model with FaceRecognizer::train. In any case, a call to train empties the existing\n.   model and learns a new model, while update does not delete any model data.\n.   \n.   @code\n.   // Create a new LBPH model (it can be updated) and use the default parameters,\n.   // this is the most common usage of this specific FaceRecognizer:\n.   //\n.   Ptr<FaceRecognizer> model =  LBPHFaceRecognizer::create();\n.   // This is the common interface to train all of the available cv::FaceRecognizer\n.   // implementations:\n.   //\n.   model->train(images, labels);\n.   // Some containers to hold new image:\n.   vector<Mat> newImages;\n.   vector<int> newLabels;\n.   // You should add some images to the containers:\n.   //\n.   // ...\n.   //\n.   // Now updating the model is as easy as calling:\n.   model->update(newImages,newLabels);\n.   // This will preserve the old model data and extend the existing model\n.   // with the new features extracted from newImages!\n.   @endcode\n.   \n.   Calling update on an Eigenfaces model (see EigenFaceRecognizer::create), which doesn't support\n.   updating, will throw an error similar to:\n.   \n.   @code\n.   OpenCV Error: The function/feature is not implemented (This FaceRecognizer (FaceRecognizer.Eigenfaces) does not support updating, you have to use FaceRecognizer::train to update it.) in update, file /home/philipp/git/opencv/modules/contrib/src/facerec.cpp, line 305\n.   terminate called after throwing an instance of 'cv::Exception'\n.   @endcode\n.   \n.   @note The FaceRecognizer does not store your training images, because this would be very\n.   memory intense and it's not the responsibility of te FaceRecognizer to do so. The caller is\n.   responsible for maintaining the dataset, he want to work with."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FaceRecognizer_write, 0), "write(filename) -> None\n.   @brief Saves a FaceRecognizer and its model state.\n.   \n.   Saves this model to a given filename, either as XML or YAML.\n.   @param filename The filename to store this FaceRecognizer to (either XML/YAML).\n.   \n.   Every FaceRecognizer overwrites FaceRecognizer::save(FileStorage& fs) to save the internal model\n.   state. FaceRecognizer::save(const String& filename) saves the state of a model to the given\n.   filename.\n.   \n.   The suffix const means that prediction does not affect the internal model state, so the method can\n.   be safely called from within different threads."},

    {NULL,          NULL}
};

static void pyopencv_face_FaceRecognizer_specials(void)
{
    pyopencv_face_FaceRecognizer_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_face_FaceRecognizer_Type.tp_dealloc = pyopencv_face_FaceRecognizer_dealloc;
    pyopencv_face_FaceRecognizer_Type.tp_repr = pyopencv_face_FaceRecognizer_repr;
    pyopencv_face_FaceRecognizer_Type.tp_getset = pyopencv_face_FaceRecognizer_getseters;
    pyopencv_face_FaceRecognizer_Type.tp_init = (initproc)0;
    pyopencv_face_FaceRecognizer_Type.tp_methods = pyopencv_face_FaceRecognizer_methods;
}

static PyObject* pyopencv_face_BIF_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_BIF %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_BIF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_BIF_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BIF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BIF_Type))
        _self_ = dynamic_cast<cv::face::BIF*>(((pyopencv_face_BIF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BIF' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_features = NULL;
    Mat features;

    const char* keywords[] = { "image", "features", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:face_BIF.compute", (char**)keywords, &pyobj_image, &pyobj_features) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 1)) )
    {
        ERRWRAP2(_self_->compute(image, features));
        return pyopencv_from(features);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_features = NULL;
    UMat features;

    const char* keywords[] = { "image", "features", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:face_BIF.compute", (char**)keywords, &pyobj_image, &pyobj_features) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 1)) )
    {
        ERRWRAP2(_self_->compute(image, features));
        return pyopencv_from(features);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BIF_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    int num_bands=8;
    int num_rotations=12;
    Ptr<BIF> retval;

    const char* keywords[] = { "num_bands", "num_rotations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ii:face_BIF.create", (char**)keywords, &num_bands, &num_rotations) )
    {
        ERRWRAP2(retval = cv::face::BIF::create(num_bands, num_rotations));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BIF_getNumBands(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BIF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BIF_Type))
        _self_ = dynamic_cast<cv::face::BIF*>(((pyopencv_face_BIF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BIF' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumBands());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BIF_getNumRotations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BIF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BIF_Type))
        _self_ = dynamic_cast<cv::face::BIF*>(((pyopencv_face_BIF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BIF' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumRotations());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_face_BIF_methods[] =
{
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BIF_compute, 0), "compute(image[, features]) -> features\n.   Computes features sby input image.\n.   *  @param image Input image (CV_32FC1).\n.   *  @param features Feature vector (CV_32FC1)."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BIF_create_cls, METH_CLASS), "create([, num_bands[, num_rotations]]) -> retval\n.   * @param num_bands The number of filter bands (<=8) used for computing BIF.\n.   * @param num_rotations The number of image rotations for computing BIF.\n.   * @returns Object for computing BIF."},
    {"getNumBands", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BIF_getNumBands, 0), "getNumBands() -> retval\n.   @returns The number of filter bands used for computing BIF."},
    {"getNumRotations", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BIF_getNumRotations, 0), "getNumRotations() -> retval\n.   @returns The number of image rotations."},

    {NULL,          NULL}
};

static void pyopencv_face_BIF_specials(void)
{
    pyopencv_face_BIF_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_face_BIF_Type.tp_dealloc = pyopencv_face_BIF_dealloc;
    pyopencv_face_BIF_Type.tp_repr = pyopencv_face_BIF_repr;
    pyopencv_face_BIF_Type.tp_getset = pyopencv_face_BIF_getseters;
    pyopencv_face_BIF_Type.tp_init = (initproc)0;
    pyopencv_face_BIF_Type.tp_methods = pyopencv_face_BIF_methods;
}

static PyObject* pyopencv_face_FacemarkKazemi_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_FacemarkKazemi %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_FacemarkKazemi_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_face_FacemarkKazemi_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_face_FacemarkKazemi_specials(void)
{
    pyopencv_face_FacemarkKazemi_Type.tp_base = &pyopencv_face_Facemark_Type;
    pyopencv_face_FacemarkKazemi_Type.tp_dealloc = pyopencv_face_FacemarkKazemi_dealloc;
    pyopencv_face_FacemarkKazemi_Type.tp_repr = pyopencv_face_FacemarkKazemi_repr;
    pyopencv_face_FacemarkKazemi_Type.tp_getset = pyopencv_face_FacemarkKazemi_getseters;
    pyopencv_face_FacemarkKazemi_Type.tp_init = (initproc)0;
    pyopencv_face_FacemarkKazemi_Type.tp_methods = pyopencv_face_FacemarkKazemi_methods;
}

static PyObject* pyopencv_face_Facemark_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_Facemark %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_Facemark_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_Facemark_fit(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::Facemark* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_Facemark_Type))
        _self_ = dynamic_cast<cv::face::Facemark*>(((pyopencv_face_Facemark_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_Facemark' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_faces = NULL;
    Mat faces;
    PyObject* pyobj_landmarks = NULL;
    vector_Mat landmarks;
    bool retval;

    const char* keywords[] = { "image", "faces", "landmarks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:face_Facemark.fit", (char**)keywords, &pyobj_image, &pyobj_faces, &pyobj_landmarks) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_faces, faces, ArgInfo("faces", 0)) &&
        pyopencv_to(pyobj_landmarks, landmarks, ArgInfo("landmarks", 1)) )
    {
        ERRWRAP2(retval = _self_->fit(image, faces, landmarks));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(landmarks));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_faces = NULL;
    UMat faces;
    PyObject* pyobj_landmarks = NULL;
    vector_Mat landmarks;
    bool retval;

    const char* keywords[] = { "image", "faces", "landmarks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:face_Facemark.fit", (char**)keywords, &pyobj_image, &pyobj_faces, &pyobj_landmarks) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_faces, faces, ArgInfo("faces", 0)) &&
        pyopencv_to(pyobj_landmarks, landmarks, ArgInfo("landmarks", 1)) )
    {
        ERRWRAP2(retval = _self_->fit(image, faces, landmarks));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(landmarks));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_Facemark_loadModel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::Facemark* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_Facemark_Type))
        _self_ = dynamic_cast<cv::face::Facemark*>(((pyopencv_face_Facemark_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_Facemark' or its derivative)");
    PyObject* pyobj_model = NULL;
    String model;

    const char* keywords[] = { "model", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_Facemark.loadModel", (char**)keywords, &pyobj_model) &&
        pyopencv_to(pyobj_model, model, ArgInfo("model", 0)) )
    {
        ERRWRAP2(_self_->loadModel(model));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_face_Facemark_methods[] =
{
    {"fit", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_Facemark_fit, 0), "fit(image, faces[, landmarks]) -> retval, landmarks\n.   @brief Detect facial landmarks from an image.\n.   @param image Input image.\n.   @param faces Output of the function which represent region of interest of the detected faces.\n.   Each face is stored in cv::Rect container.\n.   @param landmarks The detected landmark points for each faces.\n.   \n.   <B>Example of usage</B>\n.   @code\n.   Mat image = imread(\"image.jpg\");\n.   std::vector<Rect> faces;\n.   std::vector<std::vector<Point2f> > landmarks;\n.   facemark->fit(image, faces, landmarks);\n.   @endcode"},
    {"loadModel", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_Facemark_loadModel, 0), "loadModel(model) -> None\n.   @brief A function to load the trained model before the fitting process.\n.   @param model A string represent the filename of a trained model.\n.   \n.   <B>Example of usage</B>\n.   @code\n.   facemark->loadModel(\"../data/lbf.model\");\n.   @endcode"},

    {NULL,          NULL}
};

static void pyopencv_face_Facemark_specials(void)
{
    pyopencv_face_Facemark_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_face_Facemark_Type.tp_dealloc = pyopencv_face_Facemark_dealloc;
    pyopencv_face_Facemark_Type.tp_repr = pyopencv_face_Facemark_repr;
    pyopencv_face_Facemark_Type.tp_getset = pyopencv_face_Facemark_getseters;
    pyopencv_face_Facemark_Type.tp_init = (initproc)0;
    pyopencv_face_Facemark_Type.tp_methods = pyopencv_face_Facemark_methods;
}

static PyObject* pyopencv_face_FacemarkAAM_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_FacemarkAAM %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_FacemarkAAM_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_face_FacemarkAAM_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_face_FacemarkAAM_specials(void)
{
    pyopencv_face_FacemarkAAM_Type.tp_base = &pyopencv_face_FacemarkTrain_Type;
    pyopencv_face_FacemarkAAM_Type.tp_dealloc = pyopencv_face_FacemarkAAM_dealloc;
    pyopencv_face_FacemarkAAM_Type.tp_repr = pyopencv_face_FacemarkAAM_repr;
    pyopencv_face_FacemarkAAM_Type.tp_getset = pyopencv_face_FacemarkAAM_getseters;
    pyopencv_face_FacemarkAAM_Type.tp_init = (initproc)0;
    pyopencv_face_FacemarkAAM_Type.tp_methods = pyopencv_face_FacemarkAAM_methods;
}

static PyObject* pyopencv_face_FacemarkLBF_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_FacemarkLBF %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_FacemarkLBF_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_face_FacemarkLBF_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_face_FacemarkLBF_specials(void)
{
    pyopencv_face_FacemarkLBF_Type.tp_base = &pyopencv_face_FacemarkTrain_Type;
    pyopencv_face_FacemarkLBF_Type.tp_dealloc = pyopencv_face_FacemarkLBF_dealloc;
    pyopencv_face_FacemarkLBF_Type.tp_repr = pyopencv_face_FacemarkLBF_repr;
    pyopencv_face_FacemarkLBF_Type.tp_getset = pyopencv_face_FacemarkLBF_getseters;
    pyopencv_face_FacemarkLBF_Type.tp_init = (initproc)0;
    pyopencv_face_FacemarkLBF_Type.tp_methods = pyopencv_face_FacemarkLBF_methods;
}

static PyObject* pyopencv_face_FacemarkTrain_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_FacemarkTrain %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_FacemarkTrain_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_face_FacemarkTrain_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_face_FacemarkTrain_specials(void)
{
    pyopencv_face_FacemarkTrain_Type.tp_base = &pyopencv_face_Facemark_Type;
    pyopencv_face_FacemarkTrain_Type.tp_dealloc = pyopencv_face_FacemarkTrain_dealloc;
    pyopencv_face_FacemarkTrain_Type.tp_repr = pyopencv_face_FacemarkTrain_repr;
    pyopencv_face_FacemarkTrain_Type.tp_getset = pyopencv_face_FacemarkTrain_getseters;
    pyopencv_face_FacemarkTrain_Type.tp_init = (initproc)0;
    pyopencv_face_FacemarkTrain_Type.tp_methods = pyopencv_face_FacemarkTrain_methods;
}

static PyObject* pyopencv_face_BasicFaceRecognizer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_BasicFaceRecognizer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_BasicFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getEigenValues(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEigenValues());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getEigenVectors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEigenVectors());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getLabels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLabels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getMean(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMean());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getNumComponents(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumComponents());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getProjections(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    std::vector<cv::Mat> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getProjections());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_setNumComponents(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:face_BasicFaceRecognizer.setNumComponents", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setNumComponents(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_BasicFaceRecognizer_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::BasicFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_BasicFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::BasicFaceRecognizer*>(((pyopencv_face_BasicFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_BasicFaceRecognizer' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:face_BasicFaceRecognizer.setThreshold", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_face_BasicFaceRecognizer_methods[] =
{
    {"getEigenValues", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getEigenValues, 0), "getEigenValues() -> retval\n."},
    {"getEigenVectors", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getEigenVectors, 0), "getEigenVectors() -> retval\n."},
    {"getLabels", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getLabels, 0), "getLabels() -> retval\n."},
    {"getMean", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getMean, 0), "getMean() -> retval\n."},
    {"getNumComponents", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getNumComponents, 0), "getNumComponents() -> retval\n.   @see setNumComponents"},
    {"getProjections", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getProjections, 0), "getProjections() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_getThreshold, 0), "getThreshold() -> retval\n.   @see setThreshold"},
    {"setNumComponents", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_setNumComponents, 0), "setNumComponents(val) -> None\n.   @copybrief getNumComponents @see getNumComponents"},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_BasicFaceRecognizer_setThreshold, 0), "setThreshold(val) -> None\n.   @copybrief getThreshold @see getThreshold"},

    {NULL,          NULL}
};

static void pyopencv_face_BasicFaceRecognizer_specials(void)
{
    pyopencv_face_BasicFaceRecognizer_Type.tp_base = &pyopencv_face_FaceRecognizer_Type;
    pyopencv_face_BasicFaceRecognizer_Type.tp_dealloc = pyopencv_face_BasicFaceRecognizer_dealloc;
    pyopencv_face_BasicFaceRecognizer_Type.tp_repr = pyopencv_face_BasicFaceRecognizer_repr;
    pyopencv_face_BasicFaceRecognizer_Type.tp_getset = pyopencv_face_BasicFaceRecognizer_getseters;
    pyopencv_face_BasicFaceRecognizer_Type.tp_init = (initproc)0;
    pyopencv_face_BasicFaceRecognizer_Type.tp_methods = pyopencv_face_BasicFaceRecognizer_methods;
}

static PyObject* pyopencv_face_EigenFaceRecognizer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_EigenFaceRecognizer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_EigenFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_EigenFaceRecognizer_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    int num_components=0;
    double threshold=DBL_MAX;
    Ptr<EigenFaceRecognizer> retval;

    const char* keywords[] = { "num_components", "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|id:face_EigenFaceRecognizer.create", (char**)keywords, &num_components, &threshold) )
    {
        ERRWRAP2(retval = cv::face::EigenFaceRecognizer::create(num_components, threshold));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_face_EigenFaceRecognizer_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_EigenFaceRecognizer_create_cls, METH_CLASS), "create([, num_components[, threshold]]) -> retval\n.   @param num_components The number of components (read: Eigenfaces) kept for this Principal\n.   Component Analysis. As a hint: There's no rule how many components (read: Eigenfaces) should be\n.   kept for good reconstruction capabilities. It is based on your input data, so experiment with the\n.   number. Keeping 80 components should almost always be sufficient.\n.   @param threshold The threshold applied in the prediction.\n.   \n.   ### Notes:\n.   \n.   -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n.   color spaces.\n.   -   **THE EIGENFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n.   SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n.   input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n.   the images.\n.   -   This model does not support updating.\n.   \n.   ### Model internal data:\n.   \n.   -   num_components see EigenFaceRecognizer::create.\n.   -   threshold see EigenFaceRecognizer::create.\n.   -   eigenvalues The eigenvalues for this Principal Component Analysis (ordered descending).\n.   -   eigenvectors The eigenvectors for this Principal Component Analysis (ordered by their\n.   eigenvalue).\n.   -   mean The sample mean calculated from the training data.\n.   -   projections The projections of the training data.\n.   -   labels The threshold applied in the prediction. If the distance to the nearest neighbor is\n.   larger than the threshold, this method returns -1."},

    {NULL,          NULL}
};

static void pyopencv_face_EigenFaceRecognizer_specials(void)
{
    pyopencv_face_EigenFaceRecognizer_Type.tp_base = &pyopencv_face_BasicFaceRecognizer_Type;
    pyopencv_face_EigenFaceRecognizer_Type.tp_dealloc = pyopencv_face_EigenFaceRecognizer_dealloc;
    pyopencv_face_EigenFaceRecognizer_Type.tp_repr = pyopencv_face_EigenFaceRecognizer_repr;
    pyopencv_face_EigenFaceRecognizer_Type.tp_getset = pyopencv_face_EigenFaceRecognizer_getseters;
    pyopencv_face_EigenFaceRecognizer_Type.tp_init = (initproc)0;
    pyopencv_face_EigenFaceRecognizer_Type.tp_methods = pyopencv_face_EigenFaceRecognizer_methods;
}

static PyObject* pyopencv_face_FisherFaceRecognizer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_FisherFaceRecognizer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_FisherFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_FisherFaceRecognizer_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    int num_components=0;
    double threshold=DBL_MAX;
    Ptr<FisherFaceRecognizer> retval;

    const char* keywords[] = { "num_components", "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|id:face_FisherFaceRecognizer.create", (char**)keywords, &num_components, &threshold) )
    {
        ERRWRAP2(retval = cv::face::FisherFaceRecognizer::create(num_components, threshold));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_face_FisherFaceRecognizer_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_FisherFaceRecognizer_create_cls, METH_CLASS), "create([, num_components[, threshold]]) -> retval\n.   @param num_components The number of components (read: Fisherfaces) kept for this Linear\n.   Discriminant Analysis with the Fisherfaces criterion. It's useful to keep all components, that\n.   means the number of your classes c (read: subjects, persons you want to recognize). If you leave\n.   this at the default (0) or set it to a value less-equal 0 or greater (c-1), it will be set to the\n.   correct number (c-1) automatically.\n.   @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n.   is larger than the threshold, this method returns -1.\n.   \n.   ### Notes:\n.   \n.   -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n.   color spaces.\n.   -   **THE FISHERFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n.   SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n.   input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n.   the images.\n.   -   This model does not support updating.\n.   \n.   ### Model internal data:\n.   \n.   -   num_components see FisherFaceRecognizer::create.\n.   -   threshold see FisherFaceRecognizer::create.\n.   -   eigenvalues The eigenvalues for this Linear Discriminant Analysis (ordered descending).\n.   -   eigenvectors The eigenvectors for this Linear Discriminant Analysis (ordered by their\n.   eigenvalue).\n.   -   mean The sample mean calculated from the training data.\n.   -   projections The projections of the training data.\n.   -   labels The labels corresponding to the projections."},

    {NULL,          NULL}
};

static void pyopencv_face_FisherFaceRecognizer_specials(void)
{
    pyopencv_face_FisherFaceRecognizer_Type.tp_base = &pyopencv_face_BasicFaceRecognizer_Type;
    pyopencv_face_FisherFaceRecognizer_Type.tp_dealloc = pyopencv_face_FisherFaceRecognizer_dealloc;
    pyopencv_face_FisherFaceRecognizer_Type.tp_repr = pyopencv_face_FisherFaceRecognizer_repr;
    pyopencv_face_FisherFaceRecognizer_Type.tp_getset = pyopencv_face_FisherFaceRecognizer_getseters;
    pyopencv_face_FisherFaceRecognizer_Type.tp_init = (initproc)0;
    pyopencv_face_FisherFaceRecognizer_Type.tp_methods = pyopencv_face_FisherFaceRecognizer_methods;
}

static PyObject* pyopencv_face_LBPHFaceRecognizer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_LBPHFaceRecognizer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_LBPHFaceRecognizer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    int radius=1;
    int neighbors=8;
    int grid_x=8;
    int grid_y=8;
    double threshold=DBL_MAX;
    Ptr<LBPHFaceRecognizer> retval;

    const char* keywords[] = { "radius", "neighbors", "grid_x", "grid_y", "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiiid:face_LBPHFaceRecognizer.create", (char**)keywords, &radius, &neighbors, &grid_x, &grid_y, &threshold) )
    {
        ERRWRAP2(retval = cv::face::LBPHFaceRecognizer::create(radius, neighbors, grid_x, grid_y, threshold));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getGridX(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGridX());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getGridY(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGridY());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getHistograms(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    std::vector<cv::Mat> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHistograms());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getLabels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLabels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getNeighbors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNeighbors());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRadius());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_setGridX(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:face_LBPHFaceRecognizer.setGridX", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setGridX(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_setGridY(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:face_LBPHFaceRecognizer.setGridY", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setGridY(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_setNeighbors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:face_LBPHFaceRecognizer.setNeighbors", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setNeighbors(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_setRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:face_LBPHFaceRecognizer.setRadius", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRadius(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_LBPHFaceRecognizer_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::LBPHFaceRecognizer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_LBPHFaceRecognizer_Type))
        _self_ = dynamic_cast<cv::face::LBPHFaceRecognizer*>(((pyopencv_face_LBPHFaceRecognizer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_LBPHFaceRecognizer' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:face_LBPHFaceRecognizer.setThreshold", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_face_LBPHFaceRecognizer_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_create_cls, METH_CLASS), "create([, radius[, neighbors[, grid_x[, grid_y[, threshold]]]]]) -> retval\n.   @param radius The radius used for building the Circular Local Binary Pattern. The greater the\n.   radius, the smoother the image but more spatial information you can get.\n.   @param neighbors The number of sample points to build a Circular Local Binary Pattern from. An\n.   appropriate value is to use `8` sample points. Keep in mind: the more sample points you include,\n.   the higher the computational cost.\n.   @param grid_x The number of cells in the horizontal direction, 8 is a common value used in\n.   publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n.   feature vector.\n.   @param grid_y The number of cells in the vertical direction, 8 is a common value used in\n.   publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n.   feature vector.\n.   @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n.   is larger than the threshold, this method returns -1.\n.   \n.   ### Notes:\n.   \n.   -   The Circular Local Binary Patterns (used in training and prediction) expect the data given as\n.   grayscale images, use cvtColor to convert between the color spaces.\n.   -   This model supports updating.\n.   \n.   ### Model internal data:\n.   \n.   -   radius see LBPHFaceRecognizer::create.\n.   -   neighbors see LBPHFaceRecognizer::create.\n.   -   grid_x see LLBPHFaceRecognizer::create.\n.   -   grid_y see LBPHFaceRecognizer::create.\n.   -   threshold see LBPHFaceRecognizer::create.\n.   -   histograms Local Binary Patterns Histograms calculated from the given training data (empty if\n.   none was given).\n.   -   labels Labels corresponding to the calculated Local Binary Patterns Histograms."},
    {"getGridX", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getGridX, 0), "getGridX() -> retval\n.   @see setGridX"},
    {"getGridY", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getGridY, 0), "getGridY() -> retval\n.   @see setGridY"},
    {"getHistograms", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getHistograms, 0), "getHistograms() -> retval\n."},
    {"getLabels", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getLabels, 0), "getLabels() -> retval\n."},
    {"getNeighbors", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getNeighbors, 0), "getNeighbors() -> retval\n.   @see setNeighbors"},
    {"getRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getRadius, 0), "getRadius() -> retval\n.   @see setRadius"},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_getThreshold, 0), "getThreshold() -> retval\n.   @see setThreshold"},
    {"setGridX", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setGridX, 0), "setGridX(val) -> None\n.   @copybrief getGridX @see getGridX"},
    {"setGridY", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setGridY, 0), "setGridY(val) -> None\n.   @copybrief getGridY @see getGridY"},
    {"setNeighbors", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setNeighbors, 0), "setNeighbors(val) -> None\n.   @copybrief getNeighbors @see getNeighbors"},
    {"setRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setRadius, 0), "setRadius(val) -> None\n.   @copybrief getRadius @see getRadius"},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_LBPHFaceRecognizer_setThreshold, 0), "setThreshold(val) -> None\n.   @copybrief getThreshold @see getThreshold"},

    {NULL,          NULL}
};

static void pyopencv_face_LBPHFaceRecognizer_specials(void)
{
    pyopencv_face_LBPHFaceRecognizer_Type.tp_base = &pyopencv_face_FaceRecognizer_Type;
    pyopencv_face_LBPHFaceRecognizer_Type.tp_dealloc = pyopencv_face_LBPHFaceRecognizer_dealloc;
    pyopencv_face_LBPHFaceRecognizer_Type.tp_repr = pyopencv_face_LBPHFaceRecognizer_repr;
    pyopencv_face_LBPHFaceRecognizer_Type.tp_getset = pyopencv_face_LBPHFaceRecognizer_getseters;
    pyopencv_face_LBPHFaceRecognizer_Type.tp_init = (initproc)0;
    pyopencv_face_LBPHFaceRecognizer_Type.tp_methods = pyopencv_face_LBPHFaceRecognizer_methods;
}

static PyObject* pyopencv_face_MACE_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_MACE %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_MACE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_MACE_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    int IMGSIZE=64;
    cv::Ptr<MACE> retval;

    const char* keywords[] = { "IMGSIZE", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:face_MACE.create", (char**)keywords, &IMGSIZE) )
    {
        ERRWRAP2(retval = cv::face::MACE::create(IMGSIZE));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_MACE_load_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    PyObject* pyobj_filename = NULL;
    String filename;
    PyObject* pyobj_objname = NULL;
    String objname;
    cv::Ptr<MACE> retval;

    const char* keywords[] = { "filename", "objname", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:face_MACE.load", (char**)keywords, &pyobj_filename, &pyobj_objname) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_objname, objname, ArgInfo("objname", 0)) )
    {
        ERRWRAP2(retval = cv::face::MACE::load(filename, objname));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_MACE_salt(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::MACE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_MACE_Type))
        _self_ = dynamic_cast<cv::face::MACE*>(((pyopencv_face_MACE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_MACE' or its derivative)");
    PyObject* pyobj_passphrase = NULL;
    String passphrase;

    const char* keywords[] = { "passphrase", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_MACE.salt", (char**)keywords, &pyobj_passphrase) &&
        pyopencv_to(pyobj_passphrase, passphrase, ArgInfo("passphrase", 0)) )
    {
        ERRWRAP2(_self_->salt(passphrase));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_MACE_same(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::MACE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_MACE_Type))
        _self_ = dynamic_cast<cv::face::MACE*>(((pyopencv_face_MACE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_MACE' or its derivative)");
    {
    PyObject* pyobj_query = NULL;
    Mat query;
    bool retval;

    const char* keywords[] = { "query", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_MACE.same", (char**)keywords, &pyobj_query) &&
        pyopencv_to(pyobj_query, query, ArgInfo("query", 0)) )
    {
        ERRWRAP2(retval = _self_->same(query));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_query = NULL;
    UMat query;
    bool retval;

    const char* keywords[] = { "query", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_MACE.same", (char**)keywords, &pyobj_query) &&
        pyopencv_to(pyobj_query, query, ArgInfo("query", 0)) )
    {
        ERRWRAP2(retval = _self_->same(query));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_MACE_train(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::MACE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_MACE_Type))
        _self_ = dynamic_cast<cv::face::MACE*>(((pyopencv_face_MACE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_MACE' or its derivative)");
    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;

    const char* keywords[] = { "images", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_MACE.train", (char**)keywords, &pyobj_images) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) )
    {
        ERRWRAP2(_self_->train(images));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;

    const char* keywords[] = { "images", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:face_MACE.train", (char**)keywords, &pyobj_images) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) )
    {
        ERRWRAP2(_self_->train(images));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_face_MACE_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_MACE_create_cls, METH_CLASS), "create([, IMGSIZE]) -> retval\n.   @brief constructor\n.   @param IMGSIZE  images will get resized to this (should be an even number)"},
    {"load", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_MACE_load_cls, METH_CLASS), "load(filename[, objname]) -> retval\n.   @brief constructor\n.   @param filename  build a new MACE instance from a pre-serialized FileStorage\n.   @param objname (optional) top-level node in the FileStorage"},
    {"salt", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_MACE_salt, 0), "salt(passphrase) -> None\n.   @brief optionally encrypt images with random convolution\n.   @param passphrase a crc64 random seed will get generated from this"},
    {"same", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_MACE_same, 0), "same(query) -> retval\n.   @brief correlate query img and threshold to min class value\n.   @param query  a Mat with query image"},
    {"train", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_MACE_train, 0), "train(images) -> None\n.   @brief train it on positive features\n.   compute the mace filter: `h = D(-1) * X * (X(+) * D(-1) * X)(-1) * C`\n.   also calculate a minimal threshold for this class, the smallest self-similarity from the train images\n.   @param images  a vector<Mat> with the train images"},

    {NULL,          NULL}
};

static void pyopencv_face_MACE_specials(void)
{
    pyopencv_face_MACE_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_face_MACE_Type.tp_dealloc = pyopencv_face_MACE_dealloc;
    pyopencv_face_MACE_Type.tp_repr = pyopencv_face_MACE_repr;
    pyopencv_face_MACE_Type.tp_getset = pyopencv_face_MACE_getseters;
    pyopencv_face_MACE_Type.tp_init = (initproc)0;
    pyopencv_face_MACE_Type.tp_methods = pyopencv_face_MACE_methods;
}

static PyObject* pyopencv_face_PredictCollector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_PredictCollector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_PredictCollector_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_face_PredictCollector_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_face_PredictCollector_specials(void)
{
    pyopencv_face_PredictCollector_Type.tp_base = NULL;
    pyopencv_face_PredictCollector_Type.tp_dealloc = pyopencv_face_PredictCollector_dealloc;
    pyopencv_face_PredictCollector_Type.tp_repr = pyopencv_face_PredictCollector_repr;
    pyopencv_face_PredictCollector_Type.tp_getset = pyopencv_face_PredictCollector_getseters;
    pyopencv_face_PredictCollector_Type.tp_init = (initproc)0;
    pyopencv_face_PredictCollector_Type.tp_methods = pyopencv_face_PredictCollector_methods;
}

static PyObject* pyopencv_face_StandardCollector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<face_StandardCollector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_face_StandardCollector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_face_face_StandardCollector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    double threshold=DBL_MAX;
    Ptr<StandardCollector> retval;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|d:face_StandardCollector.create", (char**)keywords, &threshold) )
    {
        ERRWRAP2(retval = cv::face::StandardCollector::create(threshold));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_StandardCollector_getMinDist(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::StandardCollector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_StandardCollector_Type))
        _self_ = ((pyopencv_face_StandardCollector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_StandardCollector' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinDist());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_StandardCollector_getMinLabel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::StandardCollector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_StandardCollector_Type))
        _self_ = ((pyopencv_face_StandardCollector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_StandardCollector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinLabel());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_face_face_StandardCollector_getResults(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::face;

    cv::face::StandardCollector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_face_StandardCollector_Type))
        _self_ = ((pyopencv_face_StandardCollector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'face_StandardCollector' or its derivative)");
    bool sorted=false;
    std::vector< std::pair<int, double> > retval;

    const char* keywords[] = { "sorted", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|b:face_StandardCollector.getResults", (char**)keywords, &sorted) )
    {
        ERRWRAP2(retval = _self_->getResults(sorted));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_face_StandardCollector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_create_cls, METH_CLASS), "create([, threshold]) -> retval\n.   @brief Static constructor\n.   @param threshold set threshold"},
    {"getMinDist", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_getMinDist, 0), "getMinDist() -> retval\n.   @brief Returns minimal distance value"},
    {"getMinLabel", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_getMinLabel, 0), "getMinLabel() -> retval\n.   @brief Returns label with minimal distance"},
    {"getResults", CV_PY_FN_WITH_KW_(pyopencv_cv_face_face_StandardCollector_getResults, 0), "getResults([, sorted]) -> retval\n.   @brief Return results as vector\n.   @param sorted If set, results will be sorted by distance\n.   Each values is a pair of label and distance."},

    {NULL,          NULL}
};

static void pyopencv_face_StandardCollector_specials(void)
{
    pyopencv_face_StandardCollector_Type.tp_base = &pyopencv_face_PredictCollector_Type;
    pyopencv_face_StandardCollector_Type.tp_dealloc = pyopencv_face_StandardCollector_dealloc;
    pyopencv_face_StandardCollector_Type.tp_repr = pyopencv_face_StandardCollector_repr;
    pyopencv_face_StandardCollector_Type.tp_getset = pyopencv_face_StandardCollector_getseters;
    pyopencv_face_StandardCollector_Type.tp_init = (initproc)0;
    pyopencv_face_StandardCollector_Type.tp_methods = pyopencv_face_StandardCollector_methods;
}

static PyObject* pyopencv_hfs_HfsSegment_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<hfs_HfsSegment %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_hfs_HfsSegment_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    int height=0;
    int width=0;
    float segEgbThresholdI=0.08f;
    int minRegionSizeI=100;
    float segEgbThresholdII=0.28f;
    int minRegionSizeII=200;
    float spatialWeight=0.6f;
    int slicSpixelSize=8;
    int numSlicIter=5;
    Ptr<HfsSegment> retval;

    const char* keywords[] = { "height", "width", "segEgbThresholdI", "minRegionSizeI", "segEgbThresholdII", "minRegionSizeII", "spatialWeight", "slicSpixelSize", "numSlicIter", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii|fififii:hfs_HfsSegment.create", (char**)keywords, &height, &width, &segEgbThresholdI, &minRegionSizeI, &segEgbThresholdII, &minRegionSizeII, &spatialWeight, &slicSpixelSize, &numSlicIter) )
    {
        ERRWRAP2(retval = cv::hfs::HfsSegment::create(height, width, segEgbThresholdI, minRegionSizeI, segEgbThresholdII, minRegionSizeII, spatialWeight, slicSpixelSize, numSlicIter));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getMinRegionSizeI(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinRegionSizeI());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getMinRegionSizeII(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinRegionSizeII());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getNumSlicIter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumSlicIter());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getSegEgbThresholdI(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSegEgbThresholdI());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getSegEgbThresholdII(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSegEgbThresholdII());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getSlicSpixelSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSlicSpixelSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_getSpatialWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSpatialWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_performSegmentCpu(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    bool ifDraw=true;
    Mat retval;

    const char* keywords[] = { "src", "ifDraw", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|b:hfs_HfsSegment.performSegmentCpu", (char**)keywords, &pyobj_src, &ifDraw) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(retval = _self_->performSegmentCpu(src, ifDraw));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    bool ifDraw=true;
    Mat retval;

    const char* keywords[] = { "src", "ifDraw", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|b:hfs_HfsSegment.performSegmentCpu", (char**)keywords, &pyobj_src, &ifDraw) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(retval = _self_->performSegmentCpu(src, ifDraw));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_performSegmentGpu(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    bool ifDraw=true;
    Mat retval;

    const char* keywords[] = { "src", "ifDraw", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|b:hfs_HfsSegment.performSegmentGpu", (char**)keywords, &pyobj_src, &ifDraw) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(retval = _self_->performSegmentGpu(src, ifDraw));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    bool ifDraw=true;
    Mat retval;

    const char* keywords[] = { "src", "ifDraw", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|b:hfs_HfsSegment.performSegmentGpu", (char**)keywords, &pyobj_src, &ifDraw) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) )
    {
        ERRWRAP2(retval = _self_->performSegmentGpu(src, ifDraw));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setMinRegionSizeI(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int n=0;

    const char* keywords[] = { "n", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:hfs_HfsSegment.setMinRegionSizeI", (char**)keywords, &n) )
    {
        ERRWRAP2(_self_->setMinRegionSizeI(n));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setMinRegionSizeII(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int n=0;

    const char* keywords[] = { "n", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:hfs_HfsSegment.setMinRegionSizeII", (char**)keywords, &n) )
    {
        ERRWRAP2(_self_->setMinRegionSizeII(n));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setNumSlicIter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int n=0;

    const char* keywords[] = { "n", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:hfs_HfsSegment.setNumSlicIter", (char**)keywords, &n) )
    {
        ERRWRAP2(_self_->setNumSlicIter(n));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setSegEgbThresholdI(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    float c=0.f;

    const char* keywords[] = { "c", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:hfs_HfsSegment.setSegEgbThresholdI", (char**)keywords, &c) )
    {
        ERRWRAP2(_self_->setSegEgbThresholdI(c));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setSegEgbThresholdII(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    float c=0.f;

    const char* keywords[] = { "c", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:hfs_HfsSegment.setSegEgbThresholdII", (char**)keywords, &c) )
    {
        ERRWRAP2(_self_->setSegEgbThresholdII(c));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setSlicSpixelSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    int n=0;

    const char* keywords[] = { "n", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:hfs_HfsSegment.setSlicSpixelSize", (char**)keywords, &n) )
    {
        ERRWRAP2(_self_->setSlicSpixelSize(n));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_hfs_hfs_HfsSegment_setSpatialWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::hfs;

    cv::hfs::HfsSegment* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_hfs_HfsSegment_Type))
        _self_ = dynamic_cast<cv::hfs::HfsSegment*>(((pyopencv_hfs_HfsSegment_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'hfs_HfsSegment' or its derivative)");
    float w=0.f;

    const char* keywords[] = { "w", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:hfs_HfsSegment.setSpatialWeight", (char**)keywords, &w) )
    {
        ERRWRAP2(_self_->setSpatialWeight(w));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_hfs_HfsSegment_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_create_cls, METH_CLASS), "create(height, width[, segEgbThresholdI[, minRegionSizeI[, segEgbThresholdII[, minRegionSizeII[, spatialWeight[, slicSpixelSize[, numSlicIter]]]]]]]) -> retval\n.   @brief: create a hfs object\n.   * @param height: the height of the input image\n.   * @param width: the width of the input image\n.   * @param segEgbThresholdI: parameter segEgbThresholdI\n.   * @param minRegionSizeI: parameter minRegionSizeI\n.   * @param segEgbThresholdII: parameter segEgbThresholdII\n.   * @param minRegionSizeII: parameter minRegionSizeII\n.   * @param spatialWeight: parameter spatialWeight\n.   * @param slicSpixelSize: parameter slicSpixelSize\n.   * @param numSlicIter: parameter numSlicIter"},
    {"getMinRegionSizeI", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getMinRegionSizeI, 0), "getMinRegionSizeI() -> retval\n."},
    {"getMinRegionSizeII", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getMinRegionSizeII, 0), "getMinRegionSizeII() -> retval\n."},
    {"getNumSlicIter", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getNumSlicIter, 0), "getNumSlicIter() -> retval\n."},
    {"getSegEgbThresholdI", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getSegEgbThresholdI, 0), "getSegEgbThresholdI() -> retval\n."},
    {"getSegEgbThresholdII", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getSegEgbThresholdII, 0), "getSegEgbThresholdII() -> retval\n."},
    {"getSlicSpixelSize", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getSlicSpixelSize, 0), "getSlicSpixelSize() -> retval\n."},
    {"getSpatialWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_getSpatialWeight, 0), "getSpatialWeight() -> retval\n."},
    {"performSegmentCpu", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_performSegmentCpu, 0), "performSegmentCpu(src[, ifDraw]) -> retval\n.   @brief do segmentation with cpu\n.   * This method is only implemented for reference.\n.   * It is highly NOT recommanded to use it."},
    {"performSegmentGpu", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_performSegmentGpu, 0), "performSegmentGpu(src[, ifDraw]) -> retval\n.   @brief do segmentation gpu\n.   * @param src: the input image\n.   * @param ifDraw: if draw the image in the returned Mat. if this parameter is false,\n.   * then the content of the returned Mat is a matrix of index, describing the region\n.   * each pixel belongs to. And it's data type is CV_16U. If this parameter is true,\n.   * then the returned Mat is a segmented picture, and color of each region is the\n.   * average color of all pixels in that region. And it's data type is the same as\n.   * the input image"},
    {"setMinRegionSizeI", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setMinRegionSizeI, 0), "setMinRegionSizeI(n) -> None\n.   @brief: set and get the parameter minRegionSizeI.\n.   * This parameter is used in the second stage\n.   * mentioned above. After the EGB segmentation, regions that have fewer\n.   * pixels then this parameter will be merged into it's adjacent region."},
    {"setMinRegionSizeII", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setMinRegionSizeII, 0), "setMinRegionSizeII(n) -> None\n.   @brief: set and get the parameter minRegionSizeII.\n.   * This parameter is used in the third stage\n.   * mentioned above. It serves the same purpose as minRegionSizeI"},
    {"setNumSlicIter", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setNumSlicIter, 0), "setNumSlicIter(n) -> None\n.   @brief: set and get the parameter numSlicIter.\n.   * This parameter is used in the first stage. It\n.   * describes how many iteration to perform when executing SLIC."},
    {"setSegEgbThresholdI", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setSegEgbThresholdI, 0), "setSegEgbThresholdI(c) -> None\n.   @brief: set and get the parameter segEgbThresholdI.\n.   * This parameter is used in the second stage mentioned above.\n.   * It is a constant used to threshold weights of the edge when merging\n.   * adjacent nodes when applying EGB algorithm. The segmentation result\n.   * tends to have more regions remained if this value is large and vice versa."},
    {"setSegEgbThresholdII", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setSegEgbThresholdII, 0), "setSegEgbThresholdII(c) -> None\n.   @brief: set and get the parameter segEgbThresholdII.\n.   * This parameter is used in the third stage\n.   * mentioned above. It serves the same purpose as segEgbThresholdI.\n.   * The segmentation result tends to have more regions remained if\n.   * this value is large and vice versa."},
    {"setSlicSpixelSize", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setSlicSpixelSize, 0), "setSlicSpixelSize(n) -> None\n.   @brief: set and get the parameter slicSpixelSize.\n.   * This parameter is used in the first stage mentioned\n.   * above(the SLIC stage). It describes the size of each\n.   * superpixel when initializing SLIC. Every superpixel\n.   * approximately has \\f$slicSpixelSize \\times slicSpixelSize\\f$\n.   * pixels in the begining."},
    {"setSpatialWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_hfs_hfs_HfsSegment_setSpatialWeight, 0), "setSpatialWeight(w) -> None\n.   @brief: set and get the parameter spatialWeight.\n.   * This parameter is used in the first stage\n.   * mentioned above(the SLIC stage). It describes how important is the role\n.   * of position when calculating the distance between each pixel and it's\n.   * center. The exact formula to calculate the distance is\n.   * \\f$colorDistance + spatialWeight \\times spatialDistance\\f$.\n.   * The segmentation result tends to have more local consistency\n.   * if this value is larger."},

    {NULL,          NULL}
};

static void pyopencv_hfs_HfsSegment_specials(void)
{
    pyopencv_hfs_HfsSegment_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_hfs_HfsSegment_Type.tp_dealloc = pyopencv_hfs_HfsSegment_dealloc;
    pyopencv_hfs_HfsSegment_Type.tp_repr = pyopencv_hfs_HfsSegment_repr;
    pyopencv_hfs_HfsSegment_Type.tp_getset = pyopencv_hfs_HfsSegment_getseters;
    pyopencv_hfs_HfsSegment_Type.tp_init = (initproc)0;
    pyopencv_hfs_HfsSegment_Type.tp_methods = pyopencv_hfs_HfsSegment_methods;
}

static PyObject* pyopencv_img_hash_AverageHash_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_AverageHash %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_AverageHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_AverageHash_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    Ptr<AverageHash> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::img_hash::AverageHash::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_AverageHash_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_AverageHash_create_cls, METH_CLASS), "create() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_img_hash_AverageHash_specials(void)
{
    pyopencv_img_hash_AverageHash_Type.tp_base = &pyopencv_img_hash_ImgHashBase_Type;
    pyopencv_img_hash_AverageHash_Type.tp_dealloc = pyopencv_img_hash_AverageHash_dealloc;
    pyopencv_img_hash_AverageHash_Type.tp_repr = pyopencv_img_hash_AverageHash_repr;
    pyopencv_img_hash_AverageHash_Type.tp_getset = pyopencv_img_hash_AverageHash_getseters;
    pyopencv_img_hash_AverageHash_Type.tp_init = (initproc)0;
    pyopencv_img_hash_AverageHash_Type.tp_methods = pyopencv_img_hash_AverageHash_methods;
}

static PyObject* pyopencv_img_hash_BlockMeanHash_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_BlockMeanHash %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_BlockMeanHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_BlockMeanHash_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    int mode=BLOCK_MEAN_HASH_MODE_0;
    Ptr<BlockMeanHash> retval;

    const char* keywords[] = { "mode", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:img_hash_BlockMeanHash.create", (char**)keywords, &mode) )
    {
        ERRWRAP2(retval = cv::img_hash::BlockMeanHash::create(mode));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_BlockMeanHash_getMean(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::BlockMeanHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_BlockMeanHash_Type))
        _self_ = dynamic_cast<cv::img_hash::BlockMeanHash*>(((pyopencv_img_hash_BlockMeanHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_BlockMeanHash' or its derivative)");
    std::vector<double> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMean());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_BlockMeanHash_setMode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::BlockMeanHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_BlockMeanHash_Type))
        _self_ = dynamic_cast<cv::img_hash::BlockMeanHash*>(((pyopencv_img_hash_BlockMeanHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_BlockMeanHash' or its derivative)");
    int mode=0;

    const char* keywords[] = { "mode", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:img_hash_BlockMeanHash.setMode", (char**)keywords, &mode) )
    {
        ERRWRAP2(_self_->setMode(mode));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_BlockMeanHash_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_BlockMeanHash_create_cls, METH_CLASS), "create([, mode]) -> retval\n."},
    {"getMean", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_BlockMeanHash_getMean, 0), "getMean() -> retval\n."},
    {"setMode", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_BlockMeanHash_setMode, 0), "setMode(mode) -> None\n.   @brief Create BlockMeanHash object\n.   @param mode"},

    {NULL,          NULL}
};

static void pyopencv_img_hash_BlockMeanHash_specials(void)
{
    pyopencv_img_hash_BlockMeanHash_Type.tp_base = &pyopencv_img_hash_ImgHashBase_Type;
    pyopencv_img_hash_BlockMeanHash_Type.tp_dealloc = pyopencv_img_hash_BlockMeanHash_dealloc;
    pyopencv_img_hash_BlockMeanHash_Type.tp_repr = pyopencv_img_hash_BlockMeanHash_repr;
    pyopencv_img_hash_BlockMeanHash_Type.tp_getset = pyopencv_img_hash_BlockMeanHash_getseters;
    pyopencv_img_hash_BlockMeanHash_Type.tp_init = (initproc)0;
    pyopencv_img_hash_BlockMeanHash_Type.tp_methods = pyopencv_img_hash_BlockMeanHash_methods;
}

static PyObject* pyopencv_img_hash_ColorMomentHash_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_ColorMomentHash %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_ColorMomentHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_ColorMomentHash_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    Ptr<ColorMomentHash> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::img_hash::ColorMomentHash::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_ColorMomentHash_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_ColorMomentHash_create_cls, METH_CLASS), "create() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_img_hash_ColorMomentHash_specials(void)
{
    pyopencv_img_hash_ColorMomentHash_Type.tp_base = &pyopencv_img_hash_ImgHashBase_Type;
    pyopencv_img_hash_ColorMomentHash_Type.tp_dealloc = pyopencv_img_hash_ColorMomentHash_dealloc;
    pyopencv_img_hash_ColorMomentHash_Type.tp_repr = pyopencv_img_hash_ColorMomentHash_repr;
    pyopencv_img_hash_ColorMomentHash_Type.tp_getset = pyopencv_img_hash_ColorMomentHash_getseters;
    pyopencv_img_hash_ColorMomentHash_Type.tp_init = (initproc)0;
    pyopencv_img_hash_ColorMomentHash_Type.tp_methods = pyopencv_img_hash_ColorMomentHash_methods;
}

static PyObject* pyopencv_img_hash_ImgHashBase_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_ImgHashBase %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_ImgHashBase_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_ImgHashBase_compare(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::ImgHashBase* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_ImgHashBase_Type))
        _self_ = dynamic_cast<cv::img_hash::ImgHashBase*>(((pyopencv_img_hash_ImgHashBase_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_ImgHashBase' or its derivative)");
    {
    PyObject* pyobj_hashOne = NULL;
    Mat hashOne;
    PyObject* pyobj_hashTwo = NULL;
    Mat hashTwo;
    double retval;

    const char* keywords[] = { "hashOne", "hashTwo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:img_hash_ImgHashBase.compare", (char**)keywords, &pyobj_hashOne, &pyobj_hashTwo) &&
        pyopencv_to(pyobj_hashOne, hashOne, ArgInfo("hashOne", 0)) &&
        pyopencv_to(pyobj_hashTwo, hashTwo, ArgInfo("hashTwo", 0)) )
    {
        ERRWRAP2(retval = _self_->compare(hashOne, hashTwo));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_hashOne = NULL;
    UMat hashOne;
    PyObject* pyobj_hashTwo = NULL;
    UMat hashTwo;
    double retval;

    const char* keywords[] = { "hashOne", "hashTwo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:img_hash_ImgHashBase.compare", (char**)keywords, &pyobj_hashOne, &pyobj_hashTwo) &&
        pyopencv_to(pyobj_hashOne, hashOne, ArgInfo("hashOne", 0)) &&
        pyopencv_to(pyobj_hashTwo, hashTwo, ArgInfo("hashTwo", 0)) )
    {
        ERRWRAP2(retval = _self_->compare(hashOne, hashTwo));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_ImgHashBase_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::ImgHashBase* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_ImgHashBase_Type))
        _self_ = dynamic_cast<cv::img_hash::ImgHashBase*>(((pyopencv_img_hash_ImgHashBase_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_ImgHashBase' or its derivative)");
    {
    PyObject* pyobj_inputArr = NULL;
    Mat inputArr;
    PyObject* pyobj_outputArr = NULL;
    Mat outputArr;

    const char* keywords[] = { "inputArr", "outputArr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:img_hash_ImgHashBase.compute", (char**)keywords, &pyobj_inputArr, &pyobj_outputArr) &&
        pyopencv_to(pyobj_inputArr, inputArr, ArgInfo("inputArr", 0)) &&
        pyopencv_to(pyobj_outputArr, outputArr, ArgInfo("outputArr", 1)) )
    {
        ERRWRAP2(_self_->compute(inputArr, outputArr));
        return pyopencv_from(outputArr);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputArr = NULL;
    UMat inputArr;
    PyObject* pyobj_outputArr = NULL;
    UMat outputArr;

    const char* keywords[] = { "inputArr", "outputArr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:img_hash_ImgHashBase.compute", (char**)keywords, &pyobj_inputArr, &pyobj_outputArr) &&
        pyopencv_to(pyobj_inputArr, inputArr, ArgInfo("inputArr", 0)) &&
        pyopencv_to(pyobj_outputArr, outputArr, ArgInfo("outputArr", 1)) )
    {
        ERRWRAP2(_self_->compute(inputArr, outputArr));
        return pyopencv_from(outputArr);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_ImgHashBase_methods[] =
{
    {"compare", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_ImgHashBase_compare, 0), "compare(hashOne, hashTwo) -> retval\n.   @brief Compare the hash value between inOne and inTwo\n.   @param hashOne Hash value one\n.   @param hashTwo Hash value two\n.   @return value indicate similarity between inOne and inTwo, the meaning\n.   of the value vary from algorithms to algorithms"},
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_ImgHashBase_compute, 0), "compute(inputArr[, outputArr]) -> outputArr\n.   @brief Computes hash of the input image\n.   @param inputArr input image want to compute hash value\n.   @param outputArr hash of the image"},

    {NULL,          NULL}
};

static void pyopencv_img_hash_ImgHashBase_specials(void)
{
    pyopencv_img_hash_ImgHashBase_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_img_hash_ImgHashBase_Type.tp_dealloc = pyopencv_img_hash_ImgHashBase_dealloc;
    pyopencv_img_hash_ImgHashBase_Type.tp_repr = pyopencv_img_hash_ImgHashBase_repr;
    pyopencv_img_hash_ImgHashBase_Type.tp_getset = pyopencv_img_hash_ImgHashBase_getseters;
    pyopencv_img_hash_ImgHashBase_Type.tp_init = (initproc)0;
    pyopencv_img_hash_ImgHashBase_Type.tp_methods = pyopencv_img_hash_ImgHashBase_methods;
}

static PyObject* pyopencv_img_hash_MarrHildrethHash_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_MarrHildrethHash %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_MarrHildrethHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_MarrHildrethHash_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    float alpha=2.0f;
    float scale=1.0f;
    Ptr<MarrHildrethHash> retval;

    const char* keywords[] = { "alpha", "scale", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ff:img_hash_MarrHildrethHash.create", (char**)keywords, &alpha, &scale) )
    {
        ERRWRAP2(retval = cv::img_hash::MarrHildrethHash::create(alpha, scale));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::MarrHildrethHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_MarrHildrethHash_Type))
        _self_ = dynamic_cast<cv::img_hash::MarrHildrethHash*>(((pyopencv_img_hash_MarrHildrethHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_MarrHildrethHash' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAlpha());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::MarrHildrethHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_MarrHildrethHash_Type))
        _self_ = dynamic_cast<cv::img_hash::MarrHildrethHash*>(((pyopencv_img_hash_MarrHildrethHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_MarrHildrethHash' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScale());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_MarrHildrethHash_setKernelParam(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::MarrHildrethHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_MarrHildrethHash_Type))
        _self_ = dynamic_cast<cv::img_hash::MarrHildrethHash*>(((pyopencv_img_hash_MarrHildrethHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_MarrHildrethHash' or its derivative)");
    float alpha=0.f;
    float scale=0.f;

    const char* keywords[] = { "alpha", "scale", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ff:img_hash_MarrHildrethHash.setKernelParam", (char**)keywords, &alpha, &scale) )
    {
        ERRWRAP2(_self_->setKernelParam(alpha, scale));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_MarrHildrethHash_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_create_cls, METH_CLASS), "create([, alpha[, scale]]) -> retval\n.   @param alpha int scale factor for marr wavelet (default=2).\n.   @param scale int level of scale factor (default = 1)"},
    {"getAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getAlpha, 0), "getAlpha() -> retval\n.   * @brief self explain"},
    {"getScale", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_getScale, 0), "getScale() -> retval\n.   * @brief self explain"},
    {"setKernelParam", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_MarrHildrethHash_setKernelParam, 0), "setKernelParam(alpha, scale) -> None\n.   @brief Set Mh kernel parameters\n.   @param alpha int scale factor for marr wavelet (default=2).\n.   @param scale int level of scale factor (default = 1)"},

    {NULL,          NULL}
};

static void pyopencv_img_hash_MarrHildrethHash_specials(void)
{
    pyopencv_img_hash_MarrHildrethHash_Type.tp_base = &pyopencv_img_hash_ImgHashBase_Type;
    pyopencv_img_hash_MarrHildrethHash_Type.tp_dealloc = pyopencv_img_hash_MarrHildrethHash_dealloc;
    pyopencv_img_hash_MarrHildrethHash_Type.tp_repr = pyopencv_img_hash_MarrHildrethHash_repr;
    pyopencv_img_hash_MarrHildrethHash_Type.tp_getset = pyopencv_img_hash_MarrHildrethHash_getseters;
    pyopencv_img_hash_MarrHildrethHash_Type.tp_init = (initproc)0;
    pyopencv_img_hash_MarrHildrethHash_Type.tp_methods = pyopencv_img_hash_MarrHildrethHash_methods;
}

static PyObject* pyopencv_img_hash_PHash_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_PHash %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_PHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_PHash_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    Ptr<PHash> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::img_hash::PHash::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_PHash_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_PHash_create_cls, METH_CLASS), "create() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_img_hash_PHash_specials(void)
{
    pyopencv_img_hash_PHash_Type.tp_base = &pyopencv_img_hash_ImgHashBase_Type;
    pyopencv_img_hash_PHash_Type.tp_dealloc = pyopencv_img_hash_PHash_dealloc;
    pyopencv_img_hash_PHash_Type.tp_repr = pyopencv_img_hash_PHash_repr;
    pyopencv_img_hash_PHash_Type.tp_getset = pyopencv_img_hash_PHash_getseters;
    pyopencv_img_hash_PHash_Type.tp_init = (initproc)0;
    pyopencv_img_hash_PHash_Type.tp_methods = pyopencv_img_hash_PHash_methods;
}

static PyObject* pyopencv_img_hash_RadialVarianceHash_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<img_hash_RadialVarianceHash %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_img_hash_RadialVarianceHash_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_img_hash_img_hash_RadialVarianceHash_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    double sigma=1;
    int numOfAngleLine=180;
    Ptr<RadialVarianceHash> retval;

    const char* keywords[] = { "sigma", "numOfAngleLine", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|di:img_hash_RadialVarianceHash.create", (char**)keywords, &sigma, &numOfAngleLine) )
    {
        ERRWRAP2(retval = cv::img_hash::RadialVarianceHash::create(sigma, numOfAngleLine));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getNumOfAngleLine(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::RadialVarianceHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_RadialVarianceHash_Type))
        _self_ = dynamic_cast<cv::img_hash::RadialVarianceHash*>(((pyopencv_img_hash_RadialVarianceHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumOfAngleLine());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::RadialVarianceHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_RadialVarianceHash_Type))
        _self_ = dynamic_cast<cv::img_hash::RadialVarianceHash*>(((pyopencv_img_hash_RadialVarianceHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setNumOfAngleLine(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::RadialVarianceHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_RadialVarianceHash_Type))
        _self_ = dynamic_cast<cv::img_hash::RadialVarianceHash*>(((pyopencv_img_hash_RadialVarianceHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    int value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:img_hash_RadialVarianceHash.setNumOfAngleLine", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setNumOfAngleLine(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::img_hash;

    cv::img_hash::RadialVarianceHash* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_img_hash_RadialVarianceHash_Type))
        _self_ = dynamic_cast<cv::img_hash::RadialVarianceHash*>(((pyopencv_img_hash_RadialVarianceHash_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'img_hash_RadialVarianceHash' or its derivative)");
    double value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:img_hash_RadialVarianceHash.setSigma", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setSigma(value));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_img_hash_RadialVarianceHash_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_create_cls, METH_CLASS), "create([, sigma[, numOfAngleLine]]) -> retval\n."},
    {"getNumOfAngleLine", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getNumOfAngleLine, 0), "getNumOfAngleLine() -> retval\n."},
    {"getSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_getSigma, 0), "getSigma() -> retval\n."},
    {"setNumOfAngleLine", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setNumOfAngleLine, 0), "setNumOfAngleLine(value) -> None\n."},
    {"setSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_img_hash_img_hash_RadialVarianceHash_setSigma, 0), "setSigma(value) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_img_hash_RadialVarianceHash_specials(void)
{
    pyopencv_img_hash_RadialVarianceHash_Type.tp_base = &pyopencv_img_hash_ImgHashBase_Type;
    pyopencv_img_hash_RadialVarianceHash_Type.tp_dealloc = pyopencv_img_hash_RadialVarianceHash_dealloc;
    pyopencv_img_hash_RadialVarianceHash_Type.tp_repr = pyopencv_img_hash_RadialVarianceHash_repr;
    pyopencv_img_hash_RadialVarianceHash_Type.tp_getset = pyopencv_img_hash_RadialVarianceHash_getseters;
    pyopencv_img_hash_RadialVarianceHash_Type.tp_init = (initproc)0;
    pyopencv_img_hash_RadialVarianceHash_Type.tp_methods = pyopencv_img_hash_RadialVarianceHash_methods;
}

static PyObject* pyopencv_HistogramCostExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<HistogramCostExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_HistogramCostExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_HistogramCostExtractor_buildCostMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::HistogramCostExtractor*>(((pyopencv_HistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HistogramCostExtractor' or its derivative)");
    {
    PyObject* pyobj_descriptors1 = NULL;
    Mat descriptors1;
    PyObject* pyobj_descriptors2 = NULL;
    Mat descriptors2;
    PyObject* pyobj_costMatrix = NULL;
    Mat costMatrix;

    const char* keywords[] = { "descriptors1", "descriptors2", "costMatrix", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:HistogramCostExtractor.buildCostMatrix", (char**)keywords, &pyobj_descriptors1, &pyobj_descriptors2, &pyobj_costMatrix) &&
        pyopencv_to(pyobj_descriptors1, descriptors1, ArgInfo("descriptors1", 0)) &&
        pyopencv_to(pyobj_descriptors2, descriptors2, ArgInfo("descriptors2", 0)) &&
        pyopencv_to(pyobj_costMatrix, costMatrix, ArgInfo("costMatrix", 1)) )
    {
        ERRWRAP2(_self_->buildCostMatrix(descriptors1, descriptors2, costMatrix));
        return pyopencv_from(costMatrix);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors1 = NULL;
    UMat descriptors1;
    PyObject* pyobj_descriptors2 = NULL;
    UMat descriptors2;
    PyObject* pyobj_costMatrix = NULL;
    UMat costMatrix;

    const char* keywords[] = { "descriptors1", "descriptors2", "costMatrix", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:HistogramCostExtractor.buildCostMatrix", (char**)keywords, &pyobj_descriptors1, &pyobj_descriptors2, &pyobj_costMatrix) &&
        pyopencv_to(pyobj_descriptors1, descriptors1, ArgInfo("descriptors1", 0)) &&
        pyopencv_to(pyobj_descriptors2, descriptors2, ArgInfo("descriptors2", 0)) &&
        pyopencv_to(pyobj_costMatrix, costMatrix, ArgInfo("costMatrix", 1)) )
    {
        ERRWRAP2(_self_->buildCostMatrix(descriptors1, descriptors2, costMatrix));
        return pyopencv_from(costMatrix);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_HistogramCostExtractor_getDefaultCost(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::HistogramCostExtractor*>(((pyopencv_HistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HistogramCostExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultCost());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HistogramCostExtractor_getNDummies(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::HistogramCostExtractor*>(((pyopencv_HistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HistogramCostExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNDummies());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HistogramCostExtractor_setDefaultCost(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::HistogramCostExtractor*>(((pyopencv_HistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HistogramCostExtractor' or its derivative)");
    float defaultCost=0.f;

    const char* keywords[] = { "defaultCost", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:HistogramCostExtractor.setDefaultCost", (char**)keywords, &defaultCost) )
    {
        ERRWRAP2(_self_->setDefaultCost(defaultCost));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_HistogramCostExtractor_setNDummies(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::HistogramCostExtractor*>(((pyopencv_HistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HistogramCostExtractor' or its derivative)");
    int nDummies=0;

    const char* keywords[] = { "nDummies", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:HistogramCostExtractor.setNDummies", (char**)keywords, &nDummies) )
    {
        ERRWRAP2(_self_->setNDummies(nDummies));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_HistogramCostExtractor_methods[] =
{
    {"buildCostMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_HistogramCostExtractor_buildCostMatrix, 0), "buildCostMatrix(descriptors1, descriptors2[, costMatrix]) -> costMatrix\n."},
    {"getDefaultCost", CV_PY_FN_WITH_KW_(pyopencv_cv_HistogramCostExtractor_getDefaultCost, 0), "getDefaultCost() -> retval\n."},
    {"getNDummies", CV_PY_FN_WITH_KW_(pyopencv_cv_HistogramCostExtractor_getNDummies, 0), "getNDummies() -> retval\n."},
    {"setDefaultCost", CV_PY_FN_WITH_KW_(pyopencv_cv_HistogramCostExtractor_setDefaultCost, 0), "setDefaultCost(defaultCost) -> None\n."},
    {"setNDummies", CV_PY_FN_WITH_KW_(pyopencv_cv_HistogramCostExtractor_setNDummies, 0), "setNDummies(nDummies) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_HistogramCostExtractor_specials(void)
{
    pyopencv_HistogramCostExtractor_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_HistogramCostExtractor_Type.tp_dealloc = pyopencv_HistogramCostExtractor_dealloc;
    pyopencv_HistogramCostExtractor_Type.tp_repr = pyopencv_HistogramCostExtractor_repr;
    pyopencv_HistogramCostExtractor_Type.tp_getset = pyopencv_HistogramCostExtractor_getseters;
    pyopencv_HistogramCostExtractor_Type.tp_init = (initproc)0;
    pyopencv_HistogramCostExtractor_Type.tp_methods = pyopencv_HistogramCostExtractor_methods;
}

static PyObject* pyopencv_NormHistogramCostExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<NormHistogramCostExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_NormHistogramCostExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_NormHistogramCostExtractor_getNormFlag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::NormHistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_NormHistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::NormHistogramCostExtractor*>(((pyopencv_NormHistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'NormHistogramCostExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNormFlag());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_NormHistogramCostExtractor_setNormFlag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::NormHistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_NormHistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::NormHistogramCostExtractor*>(((pyopencv_NormHistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'NormHistogramCostExtractor' or its derivative)");
    int flag=0;

    const char* keywords[] = { "flag", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:NormHistogramCostExtractor.setNormFlag", (char**)keywords, &flag) )
    {
        ERRWRAP2(_self_->setNormFlag(flag));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_NormHistogramCostExtractor_methods[] =
{
    {"getNormFlag", CV_PY_FN_WITH_KW_(pyopencv_cv_NormHistogramCostExtractor_getNormFlag, 0), "getNormFlag() -> retval\n."},
    {"setNormFlag", CV_PY_FN_WITH_KW_(pyopencv_cv_NormHistogramCostExtractor_setNormFlag, 0), "setNormFlag(flag) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_NormHistogramCostExtractor_specials(void)
{
    pyopencv_NormHistogramCostExtractor_Type.tp_base = &pyopencv_HistogramCostExtractor_Type;
    pyopencv_NormHistogramCostExtractor_Type.tp_dealloc = pyopencv_NormHistogramCostExtractor_dealloc;
    pyopencv_NormHistogramCostExtractor_Type.tp_repr = pyopencv_NormHistogramCostExtractor_repr;
    pyopencv_NormHistogramCostExtractor_Type.tp_getset = pyopencv_NormHistogramCostExtractor_getseters;
    pyopencv_NormHistogramCostExtractor_Type.tp_init = (initproc)0;
    pyopencv_NormHistogramCostExtractor_Type.tp_methods = pyopencv_NormHistogramCostExtractor_methods;
}

static PyObject* pyopencv_EMDHistogramCostExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<EMDHistogramCostExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_EMDHistogramCostExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_EMDHistogramCostExtractor_getNormFlag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::EMDHistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_EMDHistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::EMDHistogramCostExtractor*>(((pyopencv_EMDHistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'EMDHistogramCostExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNormFlag());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_EMDHistogramCostExtractor_setNormFlag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::EMDHistogramCostExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_EMDHistogramCostExtractor_Type))
        _self_ = dynamic_cast<cv::EMDHistogramCostExtractor*>(((pyopencv_EMDHistogramCostExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'EMDHistogramCostExtractor' or its derivative)");
    int flag=0;

    const char* keywords[] = { "flag", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:EMDHistogramCostExtractor.setNormFlag", (char**)keywords, &flag) )
    {
        ERRWRAP2(_self_->setNormFlag(flag));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_EMDHistogramCostExtractor_methods[] =
{
    {"getNormFlag", CV_PY_FN_WITH_KW_(pyopencv_cv_EMDHistogramCostExtractor_getNormFlag, 0), "getNormFlag() -> retval\n."},
    {"setNormFlag", CV_PY_FN_WITH_KW_(pyopencv_cv_EMDHistogramCostExtractor_setNormFlag, 0), "setNormFlag(flag) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_EMDHistogramCostExtractor_specials(void)
{
    pyopencv_EMDHistogramCostExtractor_Type.tp_base = &pyopencv_HistogramCostExtractor_Type;
    pyopencv_EMDHistogramCostExtractor_Type.tp_dealloc = pyopencv_EMDHistogramCostExtractor_dealloc;
    pyopencv_EMDHistogramCostExtractor_Type.tp_repr = pyopencv_EMDHistogramCostExtractor_repr;
    pyopencv_EMDHistogramCostExtractor_Type.tp_getset = pyopencv_EMDHistogramCostExtractor_getseters;
    pyopencv_EMDHistogramCostExtractor_Type.tp_init = (initproc)0;
    pyopencv_EMDHistogramCostExtractor_Type.tp_methods = pyopencv_EMDHistogramCostExtractor_methods;
}

static PyObject* pyopencv_ChiHistogramCostExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ChiHistogramCostExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ChiHistogramCostExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ChiHistogramCostExtractor_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ChiHistogramCostExtractor_specials(void)
{
    pyopencv_ChiHistogramCostExtractor_Type.tp_base = &pyopencv_HistogramCostExtractor_Type;
    pyopencv_ChiHistogramCostExtractor_Type.tp_dealloc = pyopencv_ChiHistogramCostExtractor_dealloc;
    pyopencv_ChiHistogramCostExtractor_Type.tp_repr = pyopencv_ChiHistogramCostExtractor_repr;
    pyopencv_ChiHistogramCostExtractor_Type.tp_getset = pyopencv_ChiHistogramCostExtractor_getseters;
    pyopencv_ChiHistogramCostExtractor_Type.tp_init = (initproc)0;
    pyopencv_ChiHistogramCostExtractor_Type.tp_methods = pyopencv_ChiHistogramCostExtractor_methods;
}

static PyObject* pyopencv_EMDL1HistogramCostExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<EMDL1HistogramCostExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_EMDL1HistogramCostExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_EMDL1HistogramCostExtractor_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_EMDL1HistogramCostExtractor_specials(void)
{
    pyopencv_EMDL1HistogramCostExtractor_Type.tp_base = &pyopencv_HistogramCostExtractor_Type;
    pyopencv_EMDL1HistogramCostExtractor_Type.tp_dealloc = pyopencv_EMDL1HistogramCostExtractor_dealloc;
    pyopencv_EMDL1HistogramCostExtractor_Type.tp_repr = pyopencv_EMDL1HistogramCostExtractor_repr;
    pyopencv_EMDL1HistogramCostExtractor_Type.tp_getset = pyopencv_EMDL1HistogramCostExtractor_getseters;
    pyopencv_EMDL1HistogramCostExtractor_Type.tp_init = (initproc)0;
    pyopencv_EMDL1HistogramCostExtractor_Type.tp_methods = pyopencv_EMDL1HistogramCostExtractor_methods;
}

static PyObject* pyopencv_ShapeDistanceExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ShapeDistanceExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ShapeDistanceExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ShapeDistanceExtractor_computeDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeDistanceExtractor*>(((pyopencv_ShapeDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeDistanceExtractor' or its derivative)");
    {
    PyObject* pyobj_contour1 = NULL;
    Mat contour1;
    PyObject* pyobj_contour2 = NULL;
    Mat contour2;
    float retval;

    const char* keywords[] = { "contour1", "contour2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ShapeDistanceExtractor.computeDistance", (char**)keywords, &pyobj_contour1, &pyobj_contour2) &&
        pyopencv_to(pyobj_contour1, contour1, ArgInfo("contour1", 0)) &&
        pyopencv_to(pyobj_contour2, contour2, ArgInfo("contour2", 0)) )
    {
        ERRWRAP2(retval = _self_->computeDistance(contour1, contour2));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_contour1 = NULL;
    UMat contour1;
    PyObject* pyobj_contour2 = NULL;
    UMat contour2;
    float retval;

    const char* keywords[] = { "contour1", "contour2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ShapeDistanceExtractor.computeDistance", (char**)keywords, &pyobj_contour1, &pyobj_contour2) &&
        pyopencv_to(pyobj_contour1, contour1, ArgInfo("contour1", 0)) &&
        pyopencv_to(pyobj_contour2, contour2, ArgInfo("contour2", 0)) )
    {
        ERRWRAP2(retval = _self_->computeDistance(contour1, contour2));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ShapeDistanceExtractor_methods[] =
{
    {"computeDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeDistanceExtractor_computeDistance, 0), "computeDistance(contour1, contour2) -> retval\n.   @brief Compute the shape distance between two shapes defined by its contours.\n.   \n.   @param contour1 Contour defining first shape.\n.   @param contour2 Contour defining second shape."},

    {NULL,          NULL}
};

static void pyopencv_ShapeDistanceExtractor_specials(void)
{
    pyopencv_ShapeDistanceExtractor_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ShapeDistanceExtractor_Type.tp_dealloc = pyopencv_ShapeDistanceExtractor_dealloc;
    pyopencv_ShapeDistanceExtractor_Type.tp_repr = pyopencv_ShapeDistanceExtractor_repr;
    pyopencv_ShapeDistanceExtractor_Type.tp_getset = pyopencv_ShapeDistanceExtractor_getseters;
    pyopencv_ShapeDistanceExtractor_Type.tp_init = (initproc)0;
    pyopencv_ShapeDistanceExtractor_Type.tp_methods = pyopencv_ShapeDistanceExtractor_methods;
}

static PyObject* pyopencv_ShapeContextDistanceExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ShapeContextDistanceExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ShapeContextDistanceExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getAngularBins(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAngularBins());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getBendingEnergyWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBendingEnergyWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getCostExtractor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    Ptr<HistogramCostExtractor> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCostExtractor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getImageAppearanceWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getImageAppearanceWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getImages(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    {
    PyObject* pyobj_image1 = NULL;
    Mat image1;
    PyObject* pyobj_image2 = NULL;
    Mat image2;

    const char* keywords[] = { "image1", "image2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:ShapeContextDistanceExtractor.getImages", (char**)keywords, &pyobj_image1, &pyobj_image2) &&
        pyopencv_to(pyobj_image1, image1, ArgInfo("image1", 1)) &&
        pyopencv_to(pyobj_image2, image2, ArgInfo("image2", 1)) )
    {
        ERRWRAP2(_self_->getImages(image1, image2));
        return Py_BuildValue("(NN)", pyopencv_from(image1), pyopencv_from(image2));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image1 = NULL;
    UMat image1;
    PyObject* pyobj_image2 = NULL;
    UMat image2;

    const char* keywords[] = { "image1", "image2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:ShapeContextDistanceExtractor.getImages", (char**)keywords, &pyobj_image1, &pyobj_image2) &&
        pyopencv_to(pyobj_image1, image1, ArgInfo("image1", 1)) &&
        pyopencv_to(pyobj_image2, image2, ArgInfo("image2", 1)) )
    {
        ERRWRAP2(_self_->getImages(image1, image2));
        return Py_BuildValue("(NN)", pyopencv_from(image1), pyopencv_from(image2));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getInnerRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInnerRadius());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getOuterRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOuterRadius());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getRadialBins(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRadialBins());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getRotationInvariant(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRotationInvariant());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getShapeContextWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getShapeContextWeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getStdDev(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getStdDev());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_getTransformAlgorithm(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    Ptr<ShapeTransformer> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTransformAlgorithm());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setAngularBins(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    int nAngularBins=0;

    const char* keywords[] = { "nAngularBins", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ShapeContextDistanceExtractor.setAngularBins", (char**)keywords, &nAngularBins) )
    {
        ERRWRAP2(_self_->setAngularBins(nAngularBins));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setBendingEnergyWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float bendingEnergyWeight=0.f;

    const char* keywords[] = { "bendingEnergyWeight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ShapeContextDistanceExtractor.setBendingEnergyWeight", (char**)keywords, &bendingEnergyWeight) )
    {
        ERRWRAP2(_self_->setBendingEnergyWeight(bendingEnergyWeight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setCostExtractor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    PyObject* pyobj_comparer = NULL;
    Ptr<HistogramCostExtractor> comparer;

    const char* keywords[] = { "comparer", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ShapeContextDistanceExtractor.setCostExtractor", (char**)keywords, &pyobj_comparer) &&
        pyopencv_to(pyobj_comparer, comparer, ArgInfo("comparer", 0)) )
    {
        ERRWRAP2(_self_->setCostExtractor(comparer));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setImageAppearanceWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float imageAppearanceWeight=0.f;

    const char* keywords[] = { "imageAppearanceWeight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ShapeContextDistanceExtractor.setImageAppearanceWeight", (char**)keywords, &imageAppearanceWeight) )
    {
        ERRWRAP2(_self_->setImageAppearanceWeight(imageAppearanceWeight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setImages(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    {
    PyObject* pyobj_image1 = NULL;
    Mat image1;
    PyObject* pyobj_image2 = NULL;
    Mat image2;

    const char* keywords[] = { "image1", "image2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ShapeContextDistanceExtractor.setImages", (char**)keywords, &pyobj_image1, &pyobj_image2) &&
        pyopencv_to(pyobj_image1, image1, ArgInfo("image1", 0)) &&
        pyopencv_to(pyobj_image2, image2, ArgInfo("image2", 0)) )
    {
        ERRWRAP2(_self_->setImages(image1, image2));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image1 = NULL;
    UMat image1;
    PyObject* pyobj_image2 = NULL;
    UMat image2;

    const char* keywords[] = { "image1", "image2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ShapeContextDistanceExtractor.setImages", (char**)keywords, &pyobj_image1, &pyobj_image2) &&
        pyopencv_to(pyobj_image1, image1, ArgInfo("image1", 0)) &&
        pyopencv_to(pyobj_image2, image2, ArgInfo("image2", 0)) )
    {
        ERRWRAP2(_self_->setImages(image1, image2));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setInnerRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float innerRadius=0.f;

    const char* keywords[] = { "innerRadius", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ShapeContextDistanceExtractor.setInnerRadius", (char**)keywords, &innerRadius) )
    {
        ERRWRAP2(_self_->setInnerRadius(innerRadius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    int iterations=0;

    const char* keywords[] = { "iterations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ShapeContextDistanceExtractor.setIterations", (char**)keywords, &iterations) )
    {
        ERRWRAP2(_self_->setIterations(iterations));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setOuterRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float outerRadius=0.f;

    const char* keywords[] = { "outerRadius", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ShapeContextDistanceExtractor.setOuterRadius", (char**)keywords, &outerRadius) )
    {
        ERRWRAP2(_self_->setOuterRadius(outerRadius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setRadialBins(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    int nRadialBins=0;

    const char* keywords[] = { "nRadialBins", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ShapeContextDistanceExtractor.setRadialBins", (char**)keywords, &nRadialBins) )
    {
        ERRWRAP2(_self_->setRadialBins(nRadialBins));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setRotationInvariant(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    bool rotationInvariant=0;

    const char* keywords[] = { "rotationInvariant", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ShapeContextDistanceExtractor.setRotationInvariant", (char**)keywords, &rotationInvariant) )
    {
        ERRWRAP2(_self_->setRotationInvariant(rotationInvariant));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setShapeContextWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float shapeContextWeight=0.f;

    const char* keywords[] = { "shapeContextWeight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ShapeContextDistanceExtractor.setShapeContextWeight", (char**)keywords, &shapeContextWeight) )
    {
        ERRWRAP2(_self_->setShapeContextWeight(shapeContextWeight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setStdDev(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    float sigma=0.f;

    const char* keywords[] = { "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ShapeContextDistanceExtractor.setStdDev", (char**)keywords, &sigma) )
    {
        ERRWRAP2(_self_->setStdDev(sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeContextDistanceExtractor_setTransformAlgorithm(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeContextDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeContextDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::ShapeContextDistanceExtractor*>(((pyopencv_ShapeContextDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeContextDistanceExtractor' or its derivative)");
    PyObject* pyobj_transformer = NULL;
    Ptr<ShapeTransformer> transformer;

    const char* keywords[] = { "transformer", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ShapeContextDistanceExtractor.setTransformAlgorithm", (char**)keywords, &pyobj_transformer) &&
        pyopencv_to(pyobj_transformer, transformer, ArgInfo("transformer", 0)) )
    {
        ERRWRAP2(_self_->setTransformAlgorithm(transformer));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ShapeContextDistanceExtractor_methods[] =
{
    {"getAngularBins", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getAngularBins, 0), "getAngularBins() -> retval\n."},
    {"getBendingEnergyWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getBendingEnergyWeight, 0), "getBendingEnergyWeight() -> retval\n."},
    {"getCostExtractor", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getCostExtractor, 0), "getCostExtractor() -> retval\n."},
    {"getImageAppearanceWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getImageAppearanceWeight, 0), "getImageAppearanceWeight() -> retval\n."},
    {"getImages", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getImages, 0), "getImages([, image1[, image2]]) -> image1, image2\n."},
    {"getInnerRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getInnerRadius, 0), "getInnerRadius() -> retval\n."},
    {"getIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getIterations, 0), "getIterations() -> retval\n."},
    {"getOuterRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getOuterRadius, 0), "getOuterRadius() -> retval\n."},
    {"getRadialBins", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getRadialBins, 0), "getRadialBins() -> retval\n."},
    {"getRotationInvariant", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getRotationInvariant, 0), "getRotationInvariant() -> retval\n."},
    {"getShapeContextWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getShapeContextWeight, 0), "getShapeContextWeight() -> retval\n."},
    {"getStdDev", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getStdDev, 0), "getStdDev() -> retval\n."},
    {"getTransformAlgorithm", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_getTransformAlgorithm, 0), "getTransformAlgorithm() -> retval\n."},
    {"setAngularBins", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setAngularBins, 0), "setAngularBins(nAngularBins) -> None\n.   @brief Establish the number of angular bins for the Shape Context Descriptor used in the shape matching\n.   pipeline.\n.   \n.   @param nAngularBins The number of angular bins in the shape context descriptor."},
    {"setBendingEnergyWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setBendingEnergyWeight, 0), "setBendingEnergyWeight(bendingEnergyWeight) -> None\n.   @brief Set the weight of the Bending Energy in the final value of the shape distance. The bending energy\n.   definition depends on what transformation is being used to align the shapes. The final value of the\n.   shape distance is a user-defined linear combination of the shape context distance, an image\n.   appearance distance, and a bending energy.\n.   \n.   @param bendingEnergyWeight The weight of the Bending Energy in the final distance value."},
    {"setCostExtractor", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setCostExtractor, 0), "setCostExtractor(comparer) -> None\n.   @brief Set the algorithm used for building the shape context descriptor cost matrix.\n.   \n.   @param comparer Smart pointer to a HistogramCostExtractor, an algorithm that defines the cost\n.   matrix between descriptors."},
    {"setImageAppearanceWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setImageAppearanceWeight, 0), "setImageAppearanceWeight(imageAppearanceWeight) -> None\n.   @brief Set the weight of the Image Appearance cost in the final value of the shape distance. The image\n.   appearance cost is defined as the sum of squared brightness differences in Gaussian windows around\n.   corresponding image points. The final value of the shape distance is a user-defined linear\n.   combination of the shape context distance, an image appearance distance, and a bending energy. If\n.   this value is set to a number different from 0, is mandatory to set the images that correspond to\n.   each shape.\n.   \n.   @param imageAppearanceWeight The weight of the appearance cost in the final distance value."},
    {"setImages", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setImages, 0), "setImages(image1, image2) -> None\n.   @brief Set the images that correspond to each shape. This images are used in the calculation of the Image\n.   Appearance cost.\n.   \n.   @param image1 Image corresponding to the shape defined by contours1.\n.   @param image2 Image corresponding to the shape defined by contours2."},
    {"setInnerRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setInnerRadius, 0), "setInnerRadius(innerRadius) -> None\n.   @brief Set the inner radius of the shape context descriptor.\n.   \n.   @param innerRadius The value of the inner radius."},
    {"setIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setIterations, 0), "setIterations(iterations) -> None\n."},
    {"setOuterRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setOuterRadius, 0), "setOuterRadius(outerRadius) -> None\n.   @brief Set the outer radius of the shape context descriptor.\n.   \n.   @param outerRadius The value of the outer radius."},
    {"setRadialBins", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setRadialBins, 0), "setRadialBins(nRadialBins) -> None\n.   @brief Establish the number of radial bins for the Shape Context Descriptor used in the shape matching\n.   pipeline.\n.   \n.   @param nRadialBins The number of radial bins in the shape context descriptor."},
    {"setRotationInvariant", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setRotationInvariant, 0), "setRotationInvariant(rotationInvariant) -> None\n."},
    {"setShapeContextWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setShapeContextWeight, 0), "setShapeContextWeight(shapeContextWeight) -> None\n.   @brief Set the weight of the shape context distance in the final value of the shape distance. The shape\n.   context distance between two shapes is defined as the symmetric sum of shape context matching costs\n.   over best matching points. The final value of the shape distance is a user-defined linear\n.   combination of the shape context distance, an image appearance distance, and a bending energy.\n.   \n.   @param shapeContextWeight The weight of the shape context distance in the final distance value."},
    {"setStdDev", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setStdDev, 0), "setStdDev(sigma) -> None\n.   @brief Set the value of the standard deviation for the Gaussian window for the image appearance cost.\n.   \n.   @param sigma Standard Deviation."},
    {"setTransformAlgorithm", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeContextDistanceExtractor_setTransformAlgorithm, 0), "setTransformAlgorithm(transformer) -> None\n.   @brief Set the algorithm used for aligning the shapes.\n.   \n.   @param transformer Smart pointer to a ShapeTransformer, an algorithm that defines the aligning\n.   transformation."},

    {NULL,          NULL}
};

static void pyopencv_ShapeContextDistanceExtractor_specials(void)
{
    pyopencv_ShapeContextDistanceExtractor_Type.tp_base = &pyopencv_ShapeDistanceExtractor_Type;
    pyopencv_ShapeContextDistanceExtractor_Type.tp_dealloc = pyopencv_ShapeContextDistanceExtractor_dealloc;
    pyopencv_ShapeContextDistanceExtractor_Type.tp_repr = pyopencv_ShapeContextDistanceExtractor_repr;
    pyopencv_ShapeContextDistanceExtractor_Type.tp_getset = pyopencv_ShapeContextDistanceExtractor_getseters;
    pyopencv_ShapeContextDistanceExtractor_Type.tp_init = (initproc)0;
    pyopencv_ShapeContextDistanceExtractor_Type.tp_methods = pyopencv_ShapeContextDistanceExtractor_methods;
}

static PyObject* pyopencv_HausdorffDistanceExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<HausdorffDistanceExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_HausdorffDistanceExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_HausdorffDistanceExtractor_getDistanceFlag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HausdorffDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HausdorffDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::HausdorffDistanceExtractor*>(((pyopencv_HausdorffDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HausdorffDistanceExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDistanceFlag());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HausdorffDistanceExtractor_getRankProportion(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HausdorffDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HausdorffDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::HausdorffDistanceExtractor*>(((pyopencv_HausdorffDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HausdorffDistanceExtractor' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRankProportion());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_HausdorffDistanceExtractor_setDistanceFlag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HausdorffDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HausdorffDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::HausdorffDistanceExtractor*>(((pyopencv_HausdorffDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HausdorffDistanceExtractor' or its derivative)");
    int distanceFlag=0;

    const char* keywords[] = { "distanceFlag", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:HausdorffDistanceExtractor.setDistanceFlag", (char**)keywords, &distanceFlag) )
    {
        ERRWRAP2(_self_->setDistanceFlag(distanceFlag));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_HausdorffDistanceExtractor_setRankProportion(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::HausdorffDistanceExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_HausdorffDistanceExtractor_Type))
        _self_ = dynamic_cast<cv::HausdorffDistanceExtractor*>(((pyopencv_HausdorffDistanceExtractor_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'HausdorffDistanceExtractor' or its derivative)");
    float rankProportion=0.f;

    const char* keywords[] = { "rankProportion", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:HausdorffDistanceExtractor.setRankProportion", (char**)keywords, &rankProportion) )
    {
        ERRWRAP2(_self_->setRankProportion(rankProportion));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_HausdorffDistanceExtractor_methods[] =
{
    {"getDistanceFlag", CV_PY_FN_WITH_KW_(pyopencv_cv_HausdorffDistanceExtractor_getDistanceFlag, 0), "getDistanceFlag() -> retval\n."},
    {"getRankProportion", CV_PY_FN_WITH_KW_(pyopencv_cv_HausdorffDistanceExtractor_getRankProportion, 0), "getRankProportion() -> retval\n."},
    {"setDistanceFlag", CV_PY_FN_WITH_KW_(pyopencv_cv_HausdorffDistanceExtractor_setDistanceFlag, 0), "setDistanceFlag(distanceFlag) -> None\n.   @brief Set the norm used to compute the Hausdorff value between two shapes. It can be L1 or L2 norm.\n.   \n.   @param distanceFlag Flag indicating which norm is used to compute the Hausdorff distance\n.   (NORM_L1, NORM_L2)."},
    {"setRankProportion", CV_PY_FN_WITH_KW_(pyopencv_cv_HausdorffDistanceExtractor_setRankProportion, 0), "setRankProportion(rankProportion) -> None\n.   @brief This method sets the rank proportion (or fractional value) that establish the Kth ranked value of\n.   the partial Hausdorff distance. Experimentally had been shown that 0.6 is a good value to compare\n.   shapes.\n.   \n.   @param rankProportion fractional value (between 0 and 1)."},

    {NULL,          NULL}
};

static void pyopencv_HausdorffDistanceExtractor_specials(void)
{
    pyopencv_HausdorffDistanceExtractor_Type.tp_base = &pyopencv_ShapeDistanceExtractor_Type;
    pyopencv_HausdorffDistanceExtractor_Type.tp_dealloc = pyopencv_HausdorffDistanceExtractor_dealloc;
    pyopencv_HausdorffDistanceExtractor_Type.tp_repr = pyopencv_HausdorffDistanceExtractor_repr;
    pyopencv_HausdorffDistanceExtractor_Type.tp_getset = pyopencv_HausdorffDistanceExtractor_getseters;
    pyopencv_HausdorffDistanceExtractor_Type.tp_init = (initproc)0;
    pyopencv_HausdorffDistanceExtractor_Type.tp_methods = pyopencv_HausdorffDistanceExtractor_methods;
}

static PyObject* pyopencv_ShapeTransformer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ShapeTransformer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ShapeTransformer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ShapeTransformer_applyTransformation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeTransformer_Type))
        _self_ = dynamic_cast<cv::ShapeTransformer*>(((pyopencv_ShapeTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeTransformer' or its derivative)");
    {
    PyObject* pyobj_input = NULL;
    Mat input;
    PyObject* pyobj_output = NULL;
    Mat output;
    float retval;

    const char* keywords[] = { "input", "output", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ShapeTransformer.applyTransformation", (char**)keywords, &pyobj_input, &pyobj_output) &&
        pyopencv_to(pyobj_input, input, ArgInfo("input", 0)) &&
        pyopencv_to(pyobj_output, output, ArgInfo("output", 1)) )
    {
        ERRWRAP2(retval = _self_->applyTransformation(input, output));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(output));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_input = NULL;
    UMat input;
    PyObject* pyobj_output = NULL;
    UMat output;
    float retval;

    const char* keywords[] = { "input", "output", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ShapeTransformer.applyTransformation", (char**)keywords, &pyobj_input, &pyobj_output) &&
        pyopencv_to(pyobj_input, input, ArgInfo("input", 0)) &&
        pyopencv_to(pyobj_output, output, ArgInfo("output", 1)) )
    {
        ERRWRAP2(retval = _self_->applyTransformation(input, output));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(output));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeTransformer_estimateTransformation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeTransformer_Type))
        _self_ = dynamic_cast<cv::ShapeTransformer*>(((pyopencv_ShapeTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeTransformer' or its derivative)");
    {
    PyObject* pyobj_transformingShape = NULL;
    Mat transformingShape;
    PyObject* pyobj_targetShape = NULL;
    Mat targetShape;
    PyObject* pyobj_matches = NULL;
    vector_DMatch matches;

    const char* keywords[] = { "transformingShape", "targetShape", "matches", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:ShapeTransformer.estimateTransformation", (char**)keywords, &pyobj_transformingShape, &pyobj_targetShape, &pyobj_matches) &&
        pyopencv_to(pyobj_transformingShape, transformingShape, ArgInfo("transformingShape", 0)) &&
        pyopencv_to(pyobj_targetShape, targetShape, ArgInfo("targetShape", 0)) &&
        pyopencv_to(pyobj_matches, matches, ArgInfo("matches", 0)) )
    {
        ERRWRAP2(_self_->estimateTransformation(transformingShape, targetShape, matches));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_transformingShape = NULL;
    UMat transformingShape;
    PyObject* pyobj_targetShape = NULL;
    UMat targetShape;
    PyObject* pyobj_matches = NULL;
    vector_DMatch matches;

    const char* keywords[] = { "transformingShape", "targetShape", "matches", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:ShapeTransformer.estimateTransformation", (char**)keywords, &pyobj_transformingShape, &pyobj_targetShape, &pyobj_matches) &&
        pyopencv_to(pyobj_transformingShape, transformingShape, ArgInfo("transformingShape", 0)) &&
        pyopencv_to(pyobj_targetShape, targetShape, ArgInfo("targetShape", 0)) &&
        pyopencv_to(pyobj_matches, matches, ArgInfo("matches", 0)) )
    {
        ERRWRAP2(_self_->estimateTransformation(transformingShape, targetShape, matches));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ShapeTransformer_warpImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ShapeTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ShapeTransformer_Type))
        _self_ = dynamic_cast<cv::ShapeTransformer*>(((pyopencv_ShapeTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ShapeTransformer' or its derivative)");
    {
    PyObject* pyobj_transformingImage = NULL;
    Mat transformingImage;
    PyObject* pyobj_output = NULL;
    Mat output;
    int flags=INTER_LINEAR;
    int borderMode=BORDER_CONSTANT;
    PyObject* pyobj_borderValue = NULL;
    Scalar borderValue;

    const char* keywords[] = { "transformingImage", "output", "flags", "borderMode", "borderValue", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OiiO:ShapeTransformer.warpImage", (char**)keywords, &pyobj_transformingImage, &pyobj_output, &flags, &borderMode, &pyobj_borderValue) &&
        pyopencv_to(pyobj_transformingImage, transformingImage, ArgInfo("transformingImage", 0)) &&
        pyopencv_to(pyobj_output, output, ArgInfo("output", 1)) &&
        pyopencv_to(pyobj_borderValue, borderValue, ArgInfo("borderValue", 0)) )
    {
        ERRWRAP2(_self_->warpImage(transformingImage, output, flags, borderMode, borderValue));
        return pyopencv_from(output);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_transformingImage = NULL;
    UMat transformingImage;
    PyObject* pyobj_output = NULL;
    UMat output;
    int flags=INTER_LINEAR;
    int borderMode=BORDER_CONSTANT;
    PyObject* pyobj_borderValue = NULL;
    Scalar borderValue;

    const char* keywords[] = { "transformingImage", "output", "flags", "borderMode", "borderValue", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OiiO:ShapeTransformer.warpImage", (char**)keywords, &pyobj_transformingImage, &pyobj_output, &flags, &borderMode, &pyobj_borderValue) &&
        pyopencv_to(pyobj_transformingImage, transformingImage, ArgInfo("transformingImage", 0)) &&
        pyopencv_to(pyobj_output, output, ArgInfo("output", 1)) &&
        pyopencv_to(pyobj_borderValue, borderValue, ArgInfo("borderValue", 0)) )
    {
        ERRWRAP2(_self_->warpImage(transformingImage, output, flags, borderMode, borderValue));
        return pyopencv_from(output);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ShapeTransformer_methods[] =
{
    {"applyTransformation", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeTransformer_applyTransformation, 0), "applyTransformation(input[, output]) -> retval, output\n.   @brief Apply a transformation, given a pre-estimated transformation parameters.\n.   \n.   @param input Contour (set of points) to apply the transformation.\n.   @param output Output contour."},
    {"estimateTransformation", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeTransformer_estimateTransformation, 0), "estimateTransformation(transformingShape, targetShape, matches) -> None\n.   @brief Estimate the transformation parameters of the current transformer algorithm, based on point matches.\n.   \n.   @param transformingShape Contour defining first shape.\n.   @param targetShape Contour defining second shape (Target).\n.   @param matches Standard vector of Matches between points."},
    {"warpImage", CV_PY_FN_WITH_KW_(pyopencv_cv_ShapeTransformer_warpImage, 0), "warpImage(transformingImage[, output[, flags[, borderMode[, borderValue]]]]) -> output\n.   @brief Apply a transformation, given a pre-estimated transformation parameters, to an Image.\n.   \n.   @param transformingImage Input image.\n.   @param output Output image.\n.   @param flags Image interpolation method.\n.   @param borderMode border style.\n.   @param borderValue border value."},

    {NULL,          NULL}
};

static void pyopencv_ShapeTransformer_specials(void)
{
    pyopencv_ShapeTransformer_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ShapeTransformer_Type.tp_dealloc = pyopencv_ShapeTransformer_dealloc;
    pyopencv_ShapeTransformer_Type.tp_repr = pyopencv_ShapeTransformer_repr;
    pyopencv_ShapeTransformer_Type.tp_getset = pyopencv_ShapeTransformer_getseters;
    pyopencv_ShapeTransformer_Type.tp_init = (initproc)0;
    pyopencv_ShapeTransformer_Type.tp_methods = pyopencv_ShapeTransformer_methods;
}

static PyObject* pyopencv_ThinPlateSplineShapeTransformer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ThinPlateSplineShapeTransformer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ThinPlateSplineShapeTransformer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ThinPlateSplineShapeTransformer_getRegularizationParameter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ThinPlateSplineShapeTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ThinPlateSplineShapeTransformer_Type))
        _self_ = dynamic_cast<cv::ThinPlateSplineShapeTransformer*>(((pyopencv_ThinPlateSplineShapeTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ThinPlateSplineShapeTransformer' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRegularizationParameter());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ThinPlateSplineShapeTransformer_setRegularizationParameter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ThinPlateSplineShapeTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ThinPlateSplineShapeTransformer_Type))
        _self_ = dynamic_cast<cv::ThinPlateSplineShapeTransformer*>(((pyopencv_ThinPlateSplineShapeTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ThinPlateSplineShapeTransformer' or its derivative)");
    double beta=0;

    const char* keywords[] = { "beta", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ThinPlateSplineShapeTransformer.setRegularizationParameter", (char**)keywords, &beta) )
    {
        ERRWRAP2(_self_->setRegularizationParameter(beta));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ThinPlateSplineShapeTransformer_methods[] =
{
    {"getRegularizationParameter", CV_PY_FN_WITH_KW_(pyopencv_cv_ThinPlateSplineShapeTransformer_getRegularizationParameter, 0), "getRegularizationParameter() -> retval\n."},
    {"setRegularizationParameter", CV_PY_FN_WITH_KW_(pyopencv_cv_ThinPlateSplineShapeTransformer_setRegularizationParameter, 0), "setRegularizationParameter(beta) -> None\n.   @brief Set the regularization parameter for relaxing the exact interpolation requirements of the TPS\n.   algorithm.\n.   \n.   @param beta value of the regularization parameter."},

    {NULL,          NULL}
};

static void pyopencv_ThinPlateSplineShapeTransformer_specials(void)
{
    pyopencv_ThinPlateSplineShapeTransformer_Type.tp_base = &pyopencv_ShapeTransformer_Type;
    pyopencv_ThinPlateSplineShapeTransformer_Type.tp_dealloc = pyopencv_ThinPlateSplineShapeTransformer_dealloc;
    pyopencv_ThinPlateSplineShapeTransformer_Type.tp_repr = pyopencv_ThinPlateSplineShapeTransformer_repr;
    pyopencv_ThinPlateSplineShapeTransformer_Type.tp_getset = pyopencv_ThinPlateSplineShapeTransformer_getseters;
    pyopencv_ThinPlateSplineShapeTransformer_Type.tp_init = (initproc)0;
    pyopencv_ThinPlateSplineShapeTransformer_Type.tp_methods = pyopencv_ThinPlateSplineShapeTransformer_methods;
}

static PyObject* pyopencv_AffineTransformer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<AffineTransformer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_AffineTransformer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_AffineTransformer_getFullAffine(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AffineTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AffineTransformer_Type))
        _self_ = dynamic_cast<cv::AffineTransformer*>(((pyopencv_AffineTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AffineTransformer' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFullAffine());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AffineTransformer_setFullAffine(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AffineTransformer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AffineTransformer_Type))
        _self_ = dynamic_cast<cv::AffineTransformer*>(((pyopencv_AffineTransformer_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AffineTransformer' or its derivative)");
    bool fullAffine=0;

    const char* keywords[] = { "fullAffine", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:AffineTransformer.setFullAffine", (char**)keywords, &fullAffine) )
    {
        ERRWRAP2(_self_->setFullAffine(fullAffine));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_AffineTransformer_methods[] =
{
    {"getFullAffine", CV_PY_FN_WITH_KW_(pyopencv_cv_AffineTransformer_getFullAffine, 0), "getFullAffine() -> retval\n."},
    {"setFullAffine", CV_PY_FN_WITH_KW_(pyopencv_cv_AffineTransformer_setFullAffine, 0), "setFullAffine(fullAffine) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_AffineTransformer_specials(void)
{
    pyopencv_AffineTransformer_Type.tp_base = &pyopencv_ShapeTransformer_Type;
    pyopencv_AffineTransformer_Type.tp_dealloc = pyopencv_AffineTransformer_dealloc;
    pyopencv_AffineTransformer_Type.tp_repr = pyopencv_AffineTransformer_repr;
    pyopencv_AffineTransformer_Type.tp_getset = pyopencv_AffineTransformer_getseters;
    pyopencv_AffineTransformer_Type.tp_init = (initproc)0;
    pyopencv_AffineTransformer_Type.tp_methods = pyopencv_AffineTransformer_methods;
}

static PyObject* pyopencv_VideoCapture_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<VideoCapture %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_VideoCapture_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_VideoCapture_VideoCapture(pyopencv_VideoCapture_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoCapture()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:VideoCapture", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoCapture(filename)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int apiPreference=0;

    const char* keywords[] = { "filename", "apiPreference", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:VideoCapture", (char**)keywords, &pyobj_filename, &apiPreference) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoCapture(filename, apiPreference)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    int index=0;

    const char* keywords[] = { "index", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:VideoCapture", (char**)keywords, &index) )
    {
        new (&(self->v)) Ptr<cv::VideoCapture>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoCapture(index)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_VideoCapture_get(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    int propId=0;
    double retval;

    const char* keywords[] = { "propId", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:VideoCapture.get", (char**)keywords, &propId) )
    {
        ERRWRAP2(retval = _self_->get(propId));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_grab(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->grab());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_isOpened(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isOpened());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_open(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    {
    PyObject* pyobj_filename = NULL;
    String filename;
    bool retval;

    const char* keywords[] = { "filename", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:VideoCapture.open", (char**)keywords, &pyobj_filename) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(retval = _self_->open(filename));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int index=0;
    bool retval;

    const char* keywords[] = { "index", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:VideoCapture.open", (char**)keywords, &index) )
    {
        ERRWRAP2(retval = _self_->open(index));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int cameraNum=0;
    int apiPreference=0;
    bool retval;

    const char* keywords[] = { "cameraNum", "apiPreference", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:VideoCapture.open", (char**)keywords, &cameraNum, &apiPreference) )
    {
        ERRWRAP2(retval = _self_->open(cameraNum, apiPreference));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int apiPreference=0;
    bool retval;

    const char* keywords[] = { "filename", "apiPreference", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:VideoCapture.open", (char**)keywords, &pyobj_filename, &apiPreference) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) )
    {
        ERRWRAP2(retval = _self_->open(filename, apiPreference));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:VideoCapture.read", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(retval = _self_->read(image));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(image));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:VideoCapture.read", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(retval = _self_->read(image));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(image));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_release(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_retrieve(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    int flag=0;
    bool retval;

    const char* keywords[] = { "image", "flag", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Oi:VideoCapture.retrieve", (char**)keywords, &pyobj_image, &flag) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(retval = _self_->retrieve(image, flag));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(image));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    int flag=0;
    bool retval;

    const char* keywords[] = { "image", "flag", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Oi:VideoCapture.retrieve", (char**)keywords, &pyobj_image, &flag) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(retval = _self_->retrieve(image, flag));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(image));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoCapture_set(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoCapture* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoCapture_Type))
        _self_ = ((pyopencv_VideoCapture_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoCapture' or its derivative)");
    int propId=0;
    double value=0;
    bool retval;

    const char* keywords[] = { "propId", "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "id:VideoCapture.set", (char**)keywords, &propId, &value) )
    {
        ERRWRAP2(retval = _self_->set(propId, value));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_VideoCapture_methods[] =
{
    {"get", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_get, 0), "get(propId) -> retval\n.   @brief Returns the specified VideoCapture property\n.   \n.   @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n.   or one from @ref videoio_flags_others\n.   @return Value for the specified property. Value 0 is returned when querying a property that is\n.   not supported by the backend used by the VideoCapture instance.\n.   \n.   @note Reading / writing properties involves many layers. Some unexpected result might happens\n.   along this chain.\n.   @code {.txt}\n.   `VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware`\n.   @endcode\n.   The returned value might be different from what really used by the device or it could be encoded\n.   using device dependent rules (eg. steps or percentage). Effective behaviour depends from device\n.   driver and API Backend"},
    {"grab", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_grab, 0), "grab() -> retval\n.   @brief Grabs the next frame from video file or capturing device.\n.   \n.   @return `true` (non-zero) in the case of success.\n.   \n.   The method/function grabs the next frame from video file or camera and returns true (non-zero) in\n.   the case of success.\n.   \n.   The primary use of the function is in multi-camera environments, especially when the cameras do not\n.   have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that\n.   call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way\n.   the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames\n.   from different cameras will be closer in time.\n.   \n.   Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the\n.   correct way of retrieving data from it is to call VideoCapture::grab() first and then call\n.   VideoCapture::retrieve() one or more times with different values of the channel parameter.\n.   \n.   @ref tutorial_kinect_openni"},
    {"isOpened", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_isOpened, 0), "isOpened() -> retval\n.   @brief Returns true if video capturing has been initialized already.\n.   \n.   If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns\n.   true."},
    {"open", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_open, 0), "open(filename) -> retval\n.   @brief  Open video file or a capturing device or a IP video stream for video capturing\n.   \n.   @overload\n.   \n.   Parameters are same as the constructor VideoCapture(const String& filename)\n.   @return `true` if the file has been successfully opened\n.   \n.   The method first calls VideoCapture::release to close the already opened file or camera.\n\n\n\nopen(index) -> retval\n.   @brief  Open a camera for video capturing\n.   \n.   @overload\n.   \n.   Parameters are same as the constructor VideoCapture(int index)\n.   @return `true` if the camera has been successfully opened.\n.   \n.   The method first calls VideoCapture::release to close the already opened file or camera.\n\n\n\nopen(cameraNum, apiPreference) -> retval\n.   @brief  Open a camera for video capturing\n.   \n.   @overload\n.   \n.   Parameters are similar as the constructor VideoCapture(int index),except it takes an additional argument apiPreference.\n.   Definitely, is same as open(int index) where `index=cameraNum + apiPreference`\n.   @return `true` if the camera has been successfully opened.\n\n\n\nopen(filename, apiPreference) -> retval\n.   @brief Open video file or a capturing device or a IP video stream for video capturing with API Preference\n.   \n.   @overload\n.   \n.   Parameters are same as the constructor VideoCapture(const String& filename, int apiPreference)\n.   @return `true` if the file has been successfully opened\n.   \n.   The method first calls VideoCapture::release to close the already opened file or camera."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_read, 0), "read([, image]) -> retval, image\n.   @brief Grabs, decodes and returns the next video frame.\n.   \n.   @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n.   @return `false` if no frames has been grabbed\n.   \n.   The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the\n.   most convenient method for reading video files or capturing data from decode and returns the just\n.   grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more\n.   frames in video file), the method returns false and the function returns empty image (with %cv::Mat, test it with Mat::empty()).\n.   \n.   @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n.   capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n.   :ocvcvCloneImage and then do whatever you want with the copy."},
    {"release", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_release, 0), "release() -> None\n.   @brief Closes video file or capturing device.\n.   \n.   The method is automatically called by subsequent VideoCapture::open and by VideoCapture\n.   destructor.\n.   \n.   The C function also deallocates memory and clears \\*capture pointer."},
    {"retrieve", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_retrieve, 0), "retrieve([, image[, flag]]) -> retval, image\n.   @brief Decodes and returns the grabbed video frame.\n.   \n.   @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n.   @param flag it could be a frame index or a driver specific flag\n.   @return `false` if no frames has been grabbed\n.   \n.   The method decodes and returns the just grabbed frame. If no frames has been grabbed\n.   (camera has been disconnected, or there are no more frames in video file), the method returns false\n.   and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).\n.   \n.   @sa read()\n.   \n.   @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n.   capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n.   :ocvcvCloneImage and then do whatever you want with the copy."},
    {"set", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoCapture_set, 0), "set(propId, value) -> retval\n.   @brief Sets a property in the VideoCapture.\n.   \n.   @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n.   or one from @ref videoio_flags_others\n.   @param value Value of the property.\n.   @return `true` if the property is supported by backend used by the VideoCapture instance.\n.   @note Even if it returns `true` this doesn't ensure that the property\n.   value has been accepted by the capture device. See note in VideoCapture::get()"},

    {NULL,          NULL}
};

static void pyopencv_VideoCapture_specials(void)
{
    pyopencv_VideoCapture_Type.tp_base = NULL;
    pyopencv_VideoCapture_Type.tp_dealloc = pyopencv_VideoCapture_dealloc;
    pyopencv_VideoCapture_Type.tp_repr = pyopencv_VideoCapture_repr;
    pyopencv_VideoCapture_Type.tp_getset = pyopencv_VideoCapture_getseters;
    pyopencv_VideoCapture_Type.tp_init = (initproc)pyopencv_cv_VideoCapture_VideoCapture;
    pyopencv_VideoCapture_Type.tp_methods = pyopencv_VideoCapture_methods;
}

static PyObject* pyopencv_VideoWriter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<VideoWriter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_VideoWriter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_VideoWriter_VideoWriter(pyopencv_VideoWriter_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoWriter()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int fourcc=0;
    double fps=0;
    PyObject* pyobj_frameSize = NULL;
    Size frameSize;
    bool isColor=true;

    const char* keywords[] = { "filename", "fourcc", "fps", "frameSize", "isColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OidO|b:VideoWriter", (char**)keywords, &pyobj_filename, &fourcc, &fps, &pyobj_frameSize, &isColor) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) )
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoWriter(filename, fourcc, fps, frameSize, isColor)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int apiPreference=0;
    int fourcc=0;
    double fps=0;
    PyObject* pyobj_frameSize = NULL;
    Size frameSize;
    bool isColor=true;

    const char* keywords[] = { "filename", "apiPreference", "fourcc", "fps", "frameSize", "isColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiidO|b:VideoWriter", (char**)keywords, &pyobj_filename, &apiPreference, &fourcc, &fps, &pyobj_frameSize, &isColor) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) )
    {
        new (&(self->v)) Ptr<cv::VideoWriter>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::VideoWriter(filename, apiPreference, fourcc, fps, frameSize, isColor)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_VideoWriter_fourcc_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_c1 = NULL;
    char c1;
    PyObject* pyobj_c2 = NULL;
    char c2;
    PyObject* pyobj_c3 = NULL;
    char c3;
    PyObject* pyobj_c4 = NULL;
    char c4;
    int retval;

    const char* keywords[] = { "c1", "c2", "c3", "c4", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:VideoWriter.fourcc", (char**)keywords, &pyobj_c1, &pyobj_c2, &pyobj_c3, &pyobj_c4) &&
        convert_to_char(pyobj_c1, &c1, ArgInfo("c1", 0)) &&
        convert_to_char(pyobj_c2, &c2, ArgInfo("c2", 0)) &&
        convert_to_char(pyobj_c3, &c3, ArgInfo("c3", 0)) &&
        convert_to_char(pyobj_c4, &c4, ArgInfo("c4", 0)) )
    {
        ERRWRAP2(retval = cv::VideoWriter::fourcc(c1, c2, c3, c4));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoWriter_get(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoWriter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoWriter_Type))
        _self_ = ((pyopencv_VideoWriter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    int propId=0;
    double retval;

    const char* keywords[] = { "propId", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:VideoWriter.get", (char**)keywords, &propId) )
    {
        ERRWRAP2(retval = _self_->get(propId));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoWriter_isOpened(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoWriter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoWriter_Type))
        _self_ = ((pyopencv_VideoWriter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isOpened());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoWriter_open(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoWriter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoWriter_Type))
        _self_ = ((pyopencv_VideoWriter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int fourcc=0;
    double fps=0;
    PyObject* pyobj_frameSize = NULL;
    Size frameSize;
    bool isColor=true;
    bool retval;

    const char* keywords[] = { "filename", "fourcc", "fps", "frameSize", "isColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OidO|b:VideoWriter.open", (char**)keywords, &pyobj_filename, &fourcc, &fps, &pyobj_frameSize, &isColor) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) )
    {
        ERRWRAP2(retval = _self_->open(filename, fourcc, fps, frameSize, isColor));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_filename = NULL;
    String filename;
    int apiPreference=0;
    int fourcc=0;
    double fps=0;
    PyObject* pyobj_frameSize = NULL;
    Size frameSize;
    bool isColor=true;
    bool retval;

    const char* keywords[] = { "filename", "apiPreference", "fourcc", "fps", "frameSize", "isColor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OiidO|b:VideoWriter.open", (char**)keywords, &pyobj_filename, &apiPreference, &fourcc, &fps, &pyobj_frameSize, &isColor) &&
        pyopencv_to(pyobj_filename, filename, ArgInfo("filename", 0)) &&
        pyopencv_to(pyobj_frameSize, frameSize, ArgInfo("frameSize", 0)) )
    {
        ERRWRAP2(retval = _self_->open(filename, apiPreference, fourcc, fps, frameSize, isColor));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoWriter_release(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoWriter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoWriter_Type))
        _self_ = ((pyopencv_VideoWriter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoWriter_set(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoWriter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoWriter_Type))
        _self_ = ((pyopencv_VideoWriter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    int propId=0;
    double value=0;
    bool retval;

    const char* keywords[] = { "propId", "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "id:VideoWriter.set", (char**)keywords, &propId, &value) )
    {
        ERRWRAP2(retval = _self_->set(propId, value));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_VideoWriter_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::VideoWriter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_VideoWriter_Type))
        _self_ = ((pyopencv_VideoWriter_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'VideoWriter' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:VideoWriter.write", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(_self_->write(image));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    Mat image;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:VideoWriter.write", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(_self_->write(image));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_VideoWriter_methods[] =
{
    {"fourcc", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_fourcc_cls, METH_CLASS), "fourcc(c1, c2, c3, c4) -> retval\n.   @brief Concatenates 4 chars to a fourcc code\n.   \n.   @return a fourcc code\n.   \n.   This static method constructs the fourcc code of the codec to be used in the constructor\n.   VideoWriter::VideoWriter or VideoWriter::open."},
    {"get", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_get, 0), "get(propId) -> retval\n.   @brief Returns the specified VideoWriter property\n.   \n.   @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)\n.   or one of @ref videoio_flags_others\n.   \n.   @return Value for the specified property. Value 0 is returned when querying a property that is\n.   not supported by the backend used by the VideoWriter instance."},
    {"isOpened", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_isOpened, 0), "isOpened() -> retval\n.   @brief Returns true if video writer has been successfully initialized."},
    {"open", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_open, 0), "open(filename, fourcc, fps, frameSize[, isColor]) -> retval\n.   @brief Initializes or reinitializes video writer.\n.   \n.   The method opens video writer. Parameters are the same as in the constructor\n.   VideoWriter::VideoWriter.\n.   @return `true` if video writer has been successfully initialized\n.   \n.   The method first calls VideoWriter::release to close the already opened file.\n\n\n\nopen(filename, apiPreference, fourcc, fps, frameSize[, isColor]) -> retval\n.   @overload"},
    {"release", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_release, 0), "release() -> None\n.   @brief Closes the video writer.\n.   \n.   The method is automatically called by subsequent VideoWriter::open and by the VideoWriter\n.   destructor."},
    {"set", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_set, 0), "set(propId, value) -> retval\n.   @brief Sets a property in the VideoWriter.\n.   \n.   @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)\n.   or one of @ref videoio_flags_others\n.   \n.   @param value Value of the property.\n.   @return  `true` if the property is supported by the backend used by the VideoWriter instance."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_VideoWriter_write, 0), "write(image) -> None\n.   @brief Writes the next video frame\n.   \n.   @param image The written frame. In general, color images are expected in BGR format.\n.   \n.   The function/method writes the specified image to video file. It must have the same size as has\n.   been specified when opening the video writer."},

    {NULL,          NULL}
};

static void pyopencv_VideoWriter_specials(void)
{
    pyopencv_VideoWriter_Type.tp_base = NULL;
    pyopencv_VideoWriter_Type.tp_dealloc = pyopencv_VideoWriter_dealloc;
    pyopencv_VideoWriter_Type.tp_repr = pyopencv_VideoWriter_repr;
    pyopencv_VideoWriter_Type.tp_getset = pyopencv_VideoWriter_getseters;
    pyopencv_VideoWriter_Type.tp_init = (initproc)pyopencv_cv_VideoWriter_VideoWriter;
    pyopencv_VideoWriter_Type.tp_methods = pyopencv_VideoWriter_methods;
}

static PyObject* pyopencv_Tracker_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<Tracker %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_Tracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_Tracker_init(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Tracker* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Tracker_Type))
        _self_ = dynamic_cast<cv::Tracker*>(((pyopencv_Tracker_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Tracker' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", "boundingBox", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:Tracker.init", (char**)keywords, &pyobj_image, &pyobj_boundingBox) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)) )
    {
        ERRWRAP2(retval = _self_->init(image, boundingBox));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", "boundingBox", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:Tracker.init", (char**)keywords, &pyobj_image, &pyobj_boundingBox) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)) )
    {
        ERRWRAP2(retval = _self_->init(image, boundingBox));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Tracker_update(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Tracker* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Tracker_Type))
        _self_ = dynamic_cast<cv::Tracker*>(((pyopencv_Tracker_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Tracker' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Tracker.update", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(boundingBox));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Tracker.update", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(boundingBox));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_Tracker_methods[] =
{
    {"init", CV_PY_FN_WITH_KW_(pyopencv_cv_Tracker_init, 0), "init(image, boundingBox) -> retval\n.   @brief Initialize the tracker with a known bounding box that surrounded the target\n.   @param image The initial frame\n.   @param boundingBox The initial bounding box\n.   \n.   @return True if initialization went succesfully, false otherwise"},
    {"update", CV_PY_FN_WITH_KW_(pyopencv_cv_Tracker_update, 0), "update(image) -> retval, boundingBox\n.   @brief Update the tracker, find the new most likely bounding box for the target\n.   @param image The current frame\n.   @param boundingBox The bounding box that represent the new target location, if true was returned, not\n.   modified otherwise\n.   \n.   @return True means that target was located and false means that tracker cannot locate target in\n.   current frame. Note, that latter *does not* imply that tracker has failed, maybe target is indeed\n.   missing from the frame (say, out of sight)"},

    {NULL,          NULL}
};

static void pyopencv_Tracker_specials(void)
{
    pyopencv_Tracker_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_Tracker_Type.tp_dealloc = pyopencv_Tracker_dealloc;
    pyopencv_Tracker_Type.tp_repr = pyopencv_Tracker_repr;
    pyopencv_Tracker_Type.tp_getset = pyopencv_Tracker_getseters;
    pyopencv_Tracker_Type.tp_init = (initproc)0;
    pyopencv_Tracker_Type.tp_methods = pyopencv_Tracker_methods;
}

static PyObject* pyopencv_TrackerMIL_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerMIL %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerMIL_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerMIL_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerMIL> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerMIL::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerMIL_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerMIL_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters MIL parameters TrackerMIL::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerMIL_specials(void)
{
    pyopencv_TrackerMIL_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerMIL_Type.tp_dealloc = pyopencv_TrackerMIL_dealloc;
    pyopencv_TrackerMIL_Type.tp_repr = pyopencv_TrackerMIL_repr;
    pyopencv_TrackerMIL_Type.tp_getset = pyopencv_TrackerMIL_getseters;
    pyopencv_TrackerMIL_Type.tp_init = (initproc)0;
    pyopencv_TrackerMIL_Type.tp_methods = pyopencv_TrackerMIL_methods;
}

static PyObject* pyopencv_TrackerBoosting_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerBoosting %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerBoosting_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerBoosting_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerBoosting> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerBoosting::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerBoosting_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerBoosting_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters BOOSTING parameters TrackerBoosting::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerBoosting_specials(void)
{
    pyopencv_TrackerBoosting_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerBoosting_Type.tp_dealloc = pyopencv_TrackerBoosting_dealloc;
    pyopencv_TrackerBoosting_Type.tp_repr = pyopencv_TrackerBoosting_repr;
    pyopencv_TrackerBoosting_Type.tp_getset = pyopencv_TrackerBoosting_getseters;
    pyopencv_TrackerBoosting_Type.tp_init = (initproc)0;
    pyopencv_TrackerBoosting_Type.tp_methods = pyopencv_TrackerBoosting_methods;
}

static PyObject* pyopencv_TrackerMedianFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerMedianFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerMedianFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerMedianFlow_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerMedianFlow> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerMedianFlow::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerMedianFlow_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerMedianFlow_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters Median Flow parameters TrackerMedianFlow::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerMedianFlow_specials(void)
{
    pyopencv_TrackerMedianFlow_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerMedianFlow_Type.tp_dealloc = pyopencv_TrackerMedianFlow_dealloc;
    pyopencv_TrackerMedianFlow_Type.tp_repr = pyopencv_TrackerMedianFlow_repr;
    pyopencv_TrackerMedianFlow_Type.tp_getset = pyopencv_TrackerMedianFlow_getseters;
    pyopencv_TrackerMedianFlow_Type.tp_init = (initproc)0;
    pyopencv_TrackerMedianFlow_Type.tp_methods = pyopencv_TrackerMedianFlow_methods;
}

static PyObject* pyopencv_TrackerTLD_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerTLD %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerTLD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerTLD_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerTLD> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerTLD::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerTLD_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerTLD_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters TLD parameters TrackerTLD::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerTLD_specials(void)
{
    pyopencv_TrackerTLD_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerTLD_Type.tp_dealloc = pyopencv_TrackerTLD_dealloc;
    pyopencv_TrackerTLD_Type.tp_repr = pyopencv_TrackerTLD_repr;
    pyopencv_TrackerTLD_Type.tp_getset = pyopencv_TrackerTLD_getseters;
    pyopencv_TrackerTLD_Type.tp_init = (initproc)0;
    pyopencv_TrackerTLD_Type.tp_methods = pyopencv_TrackerTLD_methods;
}

static PyObject* pyopencv_TrackerKCF_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerKCF %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerKCF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerKCF_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerKCF> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerKCF::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerKCF_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerKCF_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters KCF parameters TrackerKCF::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerKCF_specials(void)
{
    pyopencv_TrackerKCF_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerKCF_Type.tp_dealloc = pyopencv_TrackerKCF_dealloc;
    pyopencv_TrackerKCF_Type.tp_repr = pyopencv_TrackerKCF_repr;
    pyopencv_TrackerKCF_Type.tp_getset = pyopencv_TrackerKCF_getseters;
    pyopencv_TrackerKCF_Type.tp_init = (initproc)0;
    pyopencv_TrackerKCF_Type.tp_methods = pyopencv_TrackerKCF_methods;
}

static PyObject* pyopencv_TrackerGOTURN_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerGOTURN %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerGOTURN_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerGOTURN_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerGOTURN> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerGOTURN::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerGOTURN_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerGOTURN_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters GOTURN parameters TrackerGOTURN::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerGOTURN_specials(void)
{
    pyopencv_TrackerGOTURN_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerGOTURN_Type.tp_dealloc = pyopencv_TrackerGOTURN_dealloc;
    pyopencv_TrackerGOTURN_Type.tp_repr = pyopencv_TrackerGOTURN_repr;
    pyopencv_TrackerGOTURN_Type.tp_getset = pyopencv_TrackerGOTURN_getseters;
    pyopencv_TrackerGOTURN_Type.tp_init = (initproc)0;
    pyopencv_TrackerGOTURN_Type.tp_methods = pyopencv_TrackerGOTURN_methods;
}

static PyObject* pyopencv_TrackerMOSSE_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerMOSSE %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerMOSSE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerMOSSE_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerMOSSE> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerMOSSE::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerMOSSE_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerMOSSE_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor"},

    {NULL,          NULL}
};

static void pyopencv_TrackerMOSSE_specials(void)
{
    pyopencv_TrackerMOSSE_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerMOSSE_Type.tp_dealloc = pyopencv_TrackerMOSSE_dealloc;
    pyopencv_TrackerMOSSE_Type.tp_repr = pyopencv_TrackerMOSSE_repr;
    pyopencv_TrackerMOSSE_Type.tp_getset = pyopencv_TrackerMOSSE_getseters;
    pyopencv_TrackerMOSSE_Type.tp_init = (initproc)0;
    pyopencv_TrackerMOSSE_Type.tp_methods = pyopencv_TrackerMOSSE_methods;
}

static PyObject* pyopencv_MultiTracker_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<MultiTracker %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_MultiTracker_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_MultiTracker_MultiTracker(pyopencv_MultiTracker_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::MultiTracker>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::MultiTracker()));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_MultiTracker_add(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MultiTracker* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MultiTracker_Type))
        _self_ = dynamic_cast<cv::MultiTracker*>(((pyopencv_MultiTracker_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MultiTracker' or its derivative)");
    {
    PyObject* pyobj_newTracker = NULL;
    Ptr<Tracker> newTracker;
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "newTracker", "image", "boundingBox", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:MultiTracker.add", (char**)keywords, &pyobj_newTracker, &pyobj_image, &pyobj_boundingBox) &&
        pyopencv_to(pyobj_newTracker, newTracker, ArgInfo("newTracker", 0)) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)) )
    {
        ERRWRAP2(retval = _self_->add(newTracker, image, boundingBox));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_newTracker = NULL;
    Ptr<Tracker> newTracker;
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_boundingBox = NULL;
    Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "newTracker", "image", "boundingBox", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:MultiTracker.add", (char**)keywords, &pyobj_newTracker, &pyobj_image, &pyobj_boundingBox) &&
        pyopencv_to(pyobj_newTracker, newTracker, ArgInfo("newTracker", 0)) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_boundingBox, boundingBox, ArgInfo("boundingBox", 0)) )
    {
        ERRWRAP2(retval = _self_->add(newTracker, image, boundingBox));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_MultiTracker_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<MultiTracker> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::MultiTracker::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MultiTracker_getObjects(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MultiTracker* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MultiTracker_Type))
        _self_ = dynamic_cast<cv::MultiTracker*>(((pyopencv_MultiTracker_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MultiTracker' or its derivative)");
    std::vector<Rect2d> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getObjects());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MultiTracker_update(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MultiTracker* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MultiTracker_Type))
        _self_ = dynamic_cast<cv::MultiTracker*>(((pyopencv_MultiTracker_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MultiTracker' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MultiTracker.update", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(boundingBox));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    vector_Rect2d boundingBox;
    bool retval;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MultiTracker.update", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(retval = _self_->update(image, boundingBox));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(boundingBox));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_MultiTracker_methods[] =
{
    {"add", CV_PY_FN_WITH_KW_(pyopencv_cv_MultiTracker_add, 0), "add(newTracker, image, boundingBox) -> retval\n.   * \\brief Add a new object to be tracked.\n.   *\n.   * @param newTracker tracking algorithm to be used\n.   * @param image input image\n.   * @param boundingBox a rectangle represents ROI of the tracked object"},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_MultiTracker_create_cls, METH_CLASS), "create() -> retval\n.   * \\brief Returns a pointer to a new instance of MultiTracker"},
    {"getObjects", CV_PY_FN_WITH_KW_(pyopencv_cv_MultiTracker_getObjects, 0), "getObjects() -> retval\n.   * \\brief Returns a reference to a storage for the tracked objects, each object corresponds to one tracker algorithm"},
    {"update", CV_PY_FN_WITH_KW_(pyopencv_cv_MultiTracker_update, 0), "update(image) -> retval, boundingBox\n.   * \\brief Update the current tracking status.\n.   * @param image input image\n.   * @param boundingBox the tracking result, represent a list of ROIs of the tracked objects."},

    {NULL,          NULL}
};

static void pyopencv_MultiTracker_specials(void)
{
    pyopencv_MultiTracker_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_MultiTracker_Type.tp_dealloc = pyopencv_MultiTracker_dealloc;
    pyopencv_MultiTracker_Type.tp_repr = pyopencv_MultiTracker_repr;
    pyopencv_MultiTracker_Type.tp_getset = pyopencv_MultiTracker_getseters;
    pyopencv_MultiTracker_Type.tp_init = (initproc)pyopencv_cv_MultiTracker_MultiTracker;
    pyopencv_MultiTracker_Type.tp_methods = pyopencv_MultiTracker_methods;
}

static PyObject* pyopencv_TrackerCSRT_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<TrackerCSRT %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_TrackerCSRT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_TrackerCSRT_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<TrackerCSRT> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::TrackerCSRT::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_TrackerCSRT_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_TrackerCSRT_create_cls, METH_CLASS), "create() -> retval\n.   @brief Constructor\n.   @param parameters CSRT parameters TrackerCSRT::Params"},

    {NULL,          NULL}
};

static void pyopencv_TrackerCSRT_specials(void)
{
    pyopencv_TrackerCSRT_Type.tp_base = &pyopencv_Tracker_Type;
    pyopencv_TrackerCSRT_Type.tp_dealloc = pyopencv_TrackerCSRT_dealloc;
    pyopencv_TrackerCSRT_Type.tp_repr = pyopencv_TrackerCSRT_repr;
    pyopencv_TrackerCSRT_Type.tp_getset = pyopencv_TrackerCSRT_getseters;
    pyopencv_TrackerCSRT_Type.tp_init = (initproc)0;
    pyopencv_TrackerCSRT_Type.tp_methods = pyopencv_TrackerCSRT_methods;
}

static PyObject* pyopencv_bioinspired_Retina_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bioinspired_Retina %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bioinspired_Retina_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_activateContoursProcessing(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    bool activate=0;

    const char* keywords[] = { "activate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:bioinspired_Retina.activateContoursProcessing", (char**)keywords, &activate) )
    {
        ERRWRAP2(_self_->activateContoursProcessing(activate));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_activateMovingContoursProcessing(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    bool activate=0;

    const char* keywords[] = { "activate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:bioinspired_Retina.activateMovingContoursProcessing", (char**)keywords, &activate) )
    {
        ERRWRAP2(_self_->activateMovingContoursProcessing(activate));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_applyFastToneMapping(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    {
    PyObject* pyobj_inputImage = NULL;
    Mat inputImage;
    PyObject* pyobj_outputToneMappedImage = NULL;
    Mat outputToneMappedImage;

    const char* keywords[] = { "inputImage", "outputToneMappedImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:bioinspired_Retina.applyFastToneMapping", (char**)keywords, &pyobj_inputImage, &pyobj_outputToneMappedImage) &&
        pyopencv_to(pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)) &&
        pyopencv_to(pyobj_outputToneMappedImage, outputToneMappedImage, ArgInfo("outputToneMappedImage", 1)) )
    {
        ERRWRAP2(_self_->applyFastToneMapping(inputImage, outputToneMappedImage));
        return pyopencv_from(outputToneMappedImage);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputImage = NULL;
    UMat inputImage;
    PyObject* pyobj_outputToneMappedImage = NULL;
    UMat outputToneMappedImage;

    const char* keywords[] = { "inputImage", "outputToneMappedImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:bioinspired_Retina.applyFastToneMapping", (char**)keywords, &pyobj_inputImage, &pyobj_outputToneMappedImage) &&
        pyopencv_to(pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)) &&
        pyopencv_to(pyobj_outputToneMappedImage, outputToneMappedImage, ArgInfo("outputToneMappedImage", 1)) )
    {
        ERRWRAP2(_self_->applyFastToneMapping(inputImage, outputToneMappedImage));
        return pyopencv_from(outputToneMappedImage);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_clearBuffers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clearBuffers());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    {
    PyObject* pyobj_inputSize = NULL;
    Size inputSize;
    Ptr<Retina> retval;

    const char* keywords[] = { "inputSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_Retina.create", (char**)keywords, &pyobj_inputSize) &&
        pyopencv_to(pyobj_inputSize, inputSize, ArgInfo("inputSize", 0)) )
    {
        ERRWRAP2(retval = cv::bioinspired::Retina::create(inputSize));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputSize = NULL;
    Size inputSize;
    bool colorMode=0;
    int colorSamplingMethod=RETINA_COLOR_BAYER;
    bool useRetinaLogSampling=false;
    float reductionFactor=1.0f;
    float samplingStrenght=10.0f;
    Ptr<Retina> retval;

    const char* keywords[] = { "inputSize", "colorMode", "colorSamplingMethod", "useRetinaLogSampling", "reductionFactor", "samplingStrenght", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Ob|ibff:bioinspired_Retina.create", (char**)keywords, &pyobj_inputSize, &colorMode, &colorSamplingMethod, &useRetinaLogSampling, &reductionFactor, &samplingStrenght) &&
        pyopencv_to(pyobj_inputSize, inputSize, ArgInfo("inputSize", 0)) )
    {
        ERRWRAP2(retval = cv::bioinspired::Retina::create(inputSize, colorMode, colorSamplingMethod, useRetinaLogSampling, reductionFactor, samplingStrenght));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_getInputSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInputSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_getMagno(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    {
    PyObject* pyobj_retinaOutput_magno = NULL;
    Mat retinaOutput_magno;

    const char* keywords[] = { "retinaOutput_magno", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getMagno", (char**)keywords, &pyobj_retinaOutput_magno) &&
        pyopencv_to(pyobj_retinaOutput_magno, retinaOutput_magno, ArgInfo("retinaOutput_magno", 1)) )
    {
        ERRWRAP2(_self_->getMagno(retinaOutput_magno));
        return pyopencv_from(retinaOutput_magno);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_retinaOutput_magno = NULL;
    UMat retinaOutput_magno;

    const char* keywords[] = { "retinaOutput_magno", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getMagno", (char**)keywords, &pyobj_retinaOutput_magno) &&
        pyopencv_to(pyobj_retinaOutput_magno, retinaOutput_magno, ArgInfo("retinaOutput_magno", 1)) )
    {
        ERRWRAP2(_self_->getMagno(retinaOutput_magno));
        return pyopencv_from(retinaOutput_magno);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_getMagnoRAW(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    {
    PyObject* pyobj_retinaOutput_magno = NULL;
    Mat retinaOutput_magno;

    const char* keywords[] = { "retinaOutput_magno", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getMagnoRAW", (char**)keywords, &pyobj_retinaOutput_magno) &&
        pyopencv_to(pyobj_retinaOutput_magno, retinaOutput_magno, ArgInfo("retinaOutput_magno", 1)) )
    {
        ERRWRAP2(_self_->getMagnoRAW(retinaOutput_magno));
        return pyopencv_from(retinaOutput_magno);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_retinaOutput_magno = NULL;
    UMat retinaOutput_magno;

    const char* keywords[] = { "retinaOutput_magno", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getMagnoRAW", (char**)keywords, &pyobj_retinaOutput_magno) &&
        pyopencv_to(pyobj_retinaOutput_magno, retinaOutput_magno, ArgInfo("retinaOutput_magno", 1)) )
    {
        ERRWRAP2(_self_->getMagnoRAW(retinaOutput_magno));
        return pyopencv_from(retinaOutput_magno);
    }
    }
    PyErr_Clear();

    {
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMagnoRAW());
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_getOutputSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOutputSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_getParvo(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    {
    PyObject* pyobj_retinaOutput_parvo = NULL;
    Mat retinaOutput_parvo;

    const char* keywords[] = { "retinaOutput_parvo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getParvo", (char**)keywords, &pyobj_retinaOutput_parvo) &&
        pyopencv_to(pyobj_retinaOutput_parvo, retinaOutput_parvo, ArgInfo("retinaOutput_parvo", 1)) )
    {
        ERRWRAP2(_self_->getParvo(retinaOutput_parvo));
        return pyopencv_from(retinaOutput_parvo);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_retinaOutput_parvo = NULL;
    UMat retinaOutput_parvo;

    const char* keywords[] = { "retinaOutput_parvo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getParvo", (char**)keywords, &pyobj_retinaOutput_parvo) &&
        pyopencv_to(pyobj_retinaOutput_parvo, retinaOutput_parvo, ArgInfo("retinaOutput_parvo", 1)) )
    {
        ERRWRAP2(_self_->getParvo(retinaOutput_parvo));
        return pyopencv_from(retinaOutput_parvo);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_getParvoRAW(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    {
    PyObject* pyobj_retinaOutput_parvo = NULL;
    Mat retinaOutput_parvo;

    const char* keywords[] = { "retinaOutput_parvo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getParvoRAW", (char**)keywords, &pyobj_retinaOutput_parvo) &&
        pyopencv_to(pyobj_retinaOutput_parvo, retinaOutput_parvo, ArgInfo("retinaOutput_parvo", 1)) )
    {
        ERRWRAP2(_self_->getParvoRAW(retinaOutput_parvo));
        return pyopencv_from(retinaOutput_parvo);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_retinaOutput_parvo = NULL;
    UMat retinaOutput_parvo;

    const char* keywords[] = { "retinaOutput_parvo", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_Retina.getParvoRAW", (char**)keywords, &pyobj_retinaOutput_parvo) &&
        pyopencv_to(pyobj_retinaOutput_parvo, retinaOutput_parvo, ArgInfo("retinaOutput_parvo", 1)) )
    {
        ERRWRAP2(_self_->getParvoRAW(retinaOutput_parvo));
        return pyopencv_from(retinaOutput_parvo);
    }
    }
    PyErr_Clear();

    {
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getParvoRAW());
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_printSetup(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->printSetup());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_run(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    {
    PyObject* pyobj_inputImage = NULL;
    Mat inputImage;

    const char* keywords[] = { "inputImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_Retina.run", (char**)keywords, &pyobj_inputImage) &&
        pyopencv_to(pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)) )
    {
        ERRWRAP2(_self_->run(inputImage));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputImage = NULL;
    UMat inputImage;

    const char* keywords[] = { "inputImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_Retina.run", (char**)keywords, &pyobj_inputImage) &&
        pyopencv_to(pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)) )
    {
        ERRWRAP2(_self_->run(inputImage));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_setColorSaturation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    bool saturateColors=true;
    float colorSaturationValue=4.0f;

    const char* keywords[] = { "saturateColors", "colorSaturationValue", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|bf:bioinspired_Retina.setColorSaturation", (char**)keywords, &saturateColors, &colorSaturationValue) )
    {
        ERRWRAP2(_self_->setColorSaturation(saturateColors, colorSaturationValue));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_setup(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    PyObject* pyobj_retinaParameterFile = NULL;
    String retinaParameterFile="";
    bool applyDefaultSetupOnFailure=true;

    const char* keywords[] = { "retinaParameterFile", "applyDefaultSetupOnFailure", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:bioinspired_Retina.setup", (char**)keywords, &pyobj_retinaParameterFile, &applyDefaultSetupOnFailure) &&
        pyopencv_to(pyobj_retinaParameterFile, retinaParameterFile, ArgInfo("retinaParameterFile", 0)) )
    {
        ERRWRAP2(_self_->setup(retinaParameterFile, applyDefaultSetupOnFailure));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_setupIPLMagnoChannel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    bool normaliseOutput=true;
    float parasolCells_beta=0.f;
    float parasolCells_tau=0.f;
    float parasolCells_k=7.f;
    float amacrinCellsTemporalCutFrequency=1.2f;
    float V0CompressionParameter=0.95f;
    float localAdaptintegration_tau=0.f;
    float localAdaptintegration_k=7.f;

    const char* keywords[] = { "normaliseOutput", "parasolCells_beta", "parasolCells_tau", "parasolCells_k", "amacrinCellsTemporalCutFrequency", "V0CompressionParameter", "localAdaptintegration_tau", "localAdaptintegration_k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|bfffffff:bioinspired_Retina.setupIPLMagnoChannel", (char**)keywords, &normaliseOutput, &parasolCells_beta, &parasolCells_tau, &parasolCells_k, &amacrinCellsTemporalCutFrequency, &V0CompressionParameter, &localAdaptintegration_tau, &localAdaptintegration_k) )
    {
        ERRWRAP2(_self_->setupIPLMagnoChannel(normaliseOutput, parasolCells_beta, parasolCells_tau, parasolCells_k, amacrinCellsTemporalCutFrequency, V0CompressionParameter, localAdaptintegration_tau, localAdaptintegration_k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_setupOPLandIPLParvoChannel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    bool colorMode=true;
    bool normaliseOutput=true;
    float photoreceptorsLocalAdaptationSensitivity=0.7f;
    float photoreceptorsTemporalConstant=0.5f;
    float photoreceptorsSpatialConstant=0.53f;
    float horizontalCellsGain=0.f;
    float HcellsTemporalConstant=1.f;
    float HcellsSpatialConstant=7.f;
    float ganglionCellsSensitivity=0.7f;

    const char* keywords[] = { "colorMode", "normaliseOutput", "photoreceptorsLocalAdaptationSensitivity", "photoreceptorsTemporalConstant", "photoreceptorsSpatialConstant", "horizontalCellsGain", "HcellsTemporalConstant", "HcellsSpatialConstant", "ganglionCellsSensitivity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|bbfffffff:bioinspired_Retina.setupOPLandIPLParvoChannel", (char**)keywords, &colorMode, &normaliseOutput, &photoreceptorsLocalAdaptationSensitivity, &photoreceptorsTemporalConstant, &photoreceptorsSpatialConstant, &horizontalCellsGain, &HcellsTemporalConstant, &HcellsSpatialConstant, &ganglionCellsSensitivity) )
    {
        ERRWRAP2(_self_->setupOPLandIPLParvoChannel(colorMode, normaliseOutput, photoreceptorsLocalAdaptationSensitivity, photoreceptorsTemporalConstant, photoreceptorsSpatialConstant, horizontalCellsGain, HcellsTemporalConstant, HcellsSpatialConstant, ganglionCellsSensitivity));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_Retina_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::Retina* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_Retina_Type))
        _self_ = dynamic_cast<cv::bioinspired::Retina*>(((pyopencv_bioinspired_Retina_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_Retina' or its derivative)");
    PyObject* pyobj_fs = NULL;
    String fs;

    const char* keywords[] = { "fs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_Retina.write", (char**)keywords, &pyobj_fs) &&
        pyopencv_to(pyobj_fs, fs, ArgInfo("fs", 0)) )
    {
        ERRWRAP2(_self_->write(fs));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_bioinspired_Retina_methods[] =
{
    {"activateContoursProcessing", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_activateContoursProcessing, 0), "activateContoursProcessing(activate) -> None\n.   @brief Activate/desactivate the Parvocellular pathway processing (contours information extraction), by\n.   default, it is activated\n.   @param activate true if Parvocellular (contours information extraction) output should be\n.   activated, false if not... if activated, the Parvocellular output can be retrieved using the\n.   Retina::getParvo methods"},
    {"activateMovingContoursProcessing", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_activateMovingContoursProcessing, 0), "activateMovingContoursProcessing(activate) -> None\n.   @brief Activate/desactivate the Magnocellular pathway processing (motion information extraction), by\n.   default, it is activated\n.   @param activate true if Magnocellular output should be activated, false if not... if activated,\n.   the Magnocellular output can be retrieved using the **getMagno** methods"},
    {"applyFastToneMapping", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_applyFastToneMapping, 0), "applyFastToneMapping(inputImage[, outputToneMappedImage]) -> outputToneMappedImage\n.   @brief Method which processes an image in the aim to correct its luminance correct\n.   backlight problems, enhance details in shadows.\n.   \n.   This method is designed to perform High Dynamic Range image tone mapping (compress \\>8bit/pixel\n.   images to 8bit/pixel). This is a simplified version of the Retina Parvocellular model\n.   (simplified version of the run/getParvo methods call) since it does not include the\n.   spatio-temporal filter modelling the Outer Plexiform Layer of the retina that performs spectral\n.   whitening and many other stuff. However, it works great for tone mapping and in a faster way.\n.   \n.   Check the demos and experiments section to see examples and the way to perform tone mapping\n.   using the original retina model and the method.\n.   \n.   @param inputImage the input image to process (should be coded in float format : CV_32F,\n.   CV_32FC1, CV_32F_C3, CV_32F_C4, the 4th channel won't be considered).\n.   @param outputToneMappedImage the output 8bit/channel tone mapped image (CV_8U or CV_8UC3 format)."},
    {"clearBuffers", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_clearBuffers, 0), "clearBuffers() -> None\n.   @brief Clears all retina buffers\n.   \n.   (equivalent to opening the eyes after a long period of eye close ;o) whatchout the temporal\n.   transition occuring just after this method call."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_create_cls, METH_CLASS), "create(inputSize) -> retval\n.   @overload\n\n\n\ncreate(inputSize, colorMode[, colorSamplingMethod[, useRetinaLogSampling[, reductionFactor[, samplingStrenght]]]]) -> retval\n.   @brief Constructors from standardized interfaces : retreive a smart pointer to a Retina instance\n.   \n.   @param inputSize the input frame size\n.   @param colorMode the chosen processing mode : with or without color processing\n.   @param colorSamplingMethod specifies which kind of color sampling will be used :\n.   -   cv::bioinspired::RETINA_COLOR_RANDOM: each pixel position is either R, G or B in a random choice\n.   -   cv::bioinspired::RETINA_COLOR_DIAGONAL: color sampling is RGBRGBRGB..., line 2 BRGBRGBRG..., line 3, GBRGBRGBR...\n.   -   cv::bioinspired::RETINA_COLOR_BAYER: standard bayer sampling\n.   @param useRetinaLogSampling activate retina log sampling, if true, the 2 following parameters can\n.   be used\n.   @param reductionFactor only usefull if param useRetinaLogSampling=true, specifies the reduction\n.   factor of the output frame (as the center (fovea) is high resolution and corners can be\n.   underscaled, then a reduction of the output is allowed without precision leak\n.   @param samplingStrenght only usefull if param useRetinaLogSampling=true, specifies the strenght of\n.   the log scale that is applied"},
    {"getInputSize", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_getInputSize, 0), "getInputSize() -> retval\n.   @brief Retreive retina input buffer size\n.   @return the retina input buffer size"},
    {"getMagno", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_getMagno, 0), "getMagno([, retinaOutput_magno]) -> retinaOutput_magno\n.   @brief Accessor of the motion channel of the retina (models peripheral vision).\n.   \n.   Warning, getMagnoRAW methods return buffers that are not rescaled within range [0;255] while\n.   the non RAW method allows a normalized matrix to be retrieved.\n.   @param retinaOutput_magno the output buffer (reallocated if necessary), format can be :\n.   -   a Mat, this output is rescaled for standard 8bits image processing use in OpenCV\n.   -   RAW methods actually return a 1D matrix (encoding is M1, M2,... Mn), this output is the\n.   original retina filter model output, without any quantification or rescaling.\n.   @see getMagnoRAW"},
    {"getMagnoRAW", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_getMagnoRAW, 0), "getMagnoRAW([, retinaOutput_magno]) -> retinaOutput_magno\n.   @brief Accessor of the motion channel of the retina (models peripheral vision).\n.   @see getMagno\n\n\n\ngetMagnoRAW() -> retval\n.   @overload"},
    {"getOutputSize", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_getOutputSize, 0), "getOutputSize() -> retval\n.   @brief Retreive retina output buffer size that can be different from the input if a spatial log\n.   transformation is applied\n.   @return the retina output buffer size"},
    {"getParvo", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_getParvo, 0), "getParvo([, retinaOutput_parvo]) -> retinaOutput_parvo\n.   @brief Accessor of the details channel of the retina (models foveal vision).\n.   \n.   Warning, getParvoRAW methods return buffers that are not rescaled within range [0;255] while\n.   the non RAW method allows a normalized matrix to be retrieved.\n.   \n.   @param retinaOutput_parvo the output buffer (reallocated if necessary), format can be :\n.   -   a Mat, this output is rescaled for standard 8bits image processing use in OpenCV\n.   -   RAW methods actually return a 1D matrix (encoding is R1, R2, ... Rn, G1, G2, ..., Gn, B1,\n.   B2, ...Bn), this output is the original retina filter model output, without any\n.   quantification or rescaling.\n.   @see getParvoRAW"},
    {"getParvoRAW", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_getParvoRAW, 0), "getParvoRAW([, retinaOutput_parvo]) -> retinaOutput_parvo\n.   @brief Accessor of the details channel of the retina (models foveal vision).\n.   @see getParvo\n\n\n\ngetParvoRAW() -> retval\n.   @overload"},
    {"printSetup", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_printSetup, 0), "printSetup() -> retval\n.   @brief Outputs a string showing the used parameters setup\n.   @return a string which contains formated parameters information"},
    {"run", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_run, 0), "run(inputImage) -> None\n.   @brief Method which allows retina to be applied on an input image,\n.   \n.   after run, encapsulated retina module is ready to deliver its outputs using dedicated\n.   acccessors, see getParvo and getMagno methods\n.   @param inputImage the input Mat image to be processed, can be gray level or BGR coded in any\n.   format (from 8bit to 16bits)"},
    {"setColorSaturation", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_setColorSaturation, 0), "setColorSaturation([, saturateColors[, colorSaturationValue]]) -> None\n.   @brief Activate color saturation as the final step of the color demultiplexing process -\\> this\n.   saturation is a sigmoide function applied to each channel of the demultiplexed image.\n.   @param saturateColors boolean that activates color saturation (if true) or desactivate (if false)\n.   @param colorSaturationValue the saturation factor : a simple factor applied on the chrominance\n.   buffers"},
    {"setup", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_setup, 0), "setup([, retinaParameterFile[, applyDefaultSetupOnFailure]]) -> None\n.   @brief Try to open an XML retina parameters file to adjust current retina instance setup\n.   \n.   - if the xml file does not exist, then default setup is applied\n.   - warning, Exceptions are thrown if read XML file is not valid\n.   @param retinaParameterFile the parameters filename\n.   @param applyDefaultSetupOnFailure set to true if an error must be thrown on error\n.   \n.   You can retrieve the current parameters structure using the method Retina::getParameters and update\n.   it before running method Retina::setup."},
    {"setupIPLMagnoChannel", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_setupIPLMagnoChannel, 0), "setupIPLMagnoChannel([, normaliseOutput[, parasolCells_beta[, parasolCells_tau[, parasolCells_k[, amacrinCellsTemporalCutFrequency[, V0CompressionParameter[, localAdaptintegration_tau[, localAdaptintegration_k]]]]]]]]) -> None\n.   @brief Set parameters values for the Inner Plexiform Layer (IPL) magnocellular channel\n.   \n.   this channel processes signals output from OPL processing stage in peripheral vision, it allows\n.   motion information enhancement. It is decorrelated from the details channel. See reference\n.   papers for more details.\n.   \n.   @param normaliseOutput specifies if (true) output is rescaled between 0 and 255 of not (false)\n.   @param parasolCells_beta the low pass filter gain used for local contrast adaptation at the\n.   IPL level of the retina (for ganglion cells local adaptation), typical value is 0\n.   @param parasolCells_tau the low pass filter time constant used for local contrast adaptation\n.   at the IPL level of the retina (for ganglion cells local adaptation), unit is frame, typical\n.   value is 0 (immediate response)\n.   @param parasolCells_k the low pass filter spatial constant used for local contrast adaptation\n.   at the IPL level of the retina (for ganglion cells local adaptation), unit is pixels, typical\n.   value is 5\n.   @param amacrinCellsTemporalCutFrequency the time constant of the first order high pass fiter of\n.   the magnocellular way (motion information channel), unit is frames, typical value is 1.2\n.   @param V0CompressionParameter the compression strengh of the ganglion cells local adaptation\n.   output, set a value between 0.6 and 1 for best results, a high value increases more the low\n.   value sensitivity... and the output saturates faster, recommended value: 0.95\n.   @param localAdaptintegration_tau specifies the temporal constant of the low pas filter\n.   involved in the computation of the local \"motion mean\" for the local adaptation computation\n.   @param localAdaptintegration_k specifies the spatial constant of the low pas filter involved\n.   in the computation of the local \"motion mean\" for the local adaptation computation"},
    {"setupOPLandIPLParvoChannel", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_setupOPLandIPLParvoChannel, 0), "setupOPLandIPLParvoChannel([, colorMode[, normaliseOutput[, photoreceptorsLocalAdaptationSensitivity[, photoreceptorsTemporalConstant[, photoreceptorsSpatialConstant[, horizontalCellsGain[, HcellsTemporalConstant[, HcellsSpatialConstant[, ganglionCellsSensitivity]]]]]]]]]) -> None\n.   @brief Setup the OPL and IPL parvo channels (see biologocal model)\n.   \n.   OPL is referred as Outer Plexiform Layer of the retina, it allows the spatio-temporal filtering\n.   which withens the spectrum and reduces spatio-temporal noise while attenuating global luminance\n.   (low frequency energy) IPL parvo is the OPL next processing stage, it refers to a part of the\n.   Inner Plexiform layer of the retina, it allows high contours sensitivity in foveal vision. See\n.   reference papers for more informations.\n.   for more informations, please have a look at the paper Benoit A., Caplier A., Durette B., Herault, J., \"USING HUMAN VISUAL SYSTEM MODELING FOR BIO-INSPIRED LOW LEVEL IMAGE PROCESSING\", Elsevier, Computer Vision and Image Understanding 114 (2010), pp. 758-773, DOI: http://dx.doi.org/10.1016/j.cviu.2010.01.011\n.   @param colorMode specifies if (true) color is processed of not (false) to then processing gray\n.   level image\n.   @param normaliseOutput specifies if (true) output is rescaled between 0 and 255 of not (false)\n.   @param photoreceptorsLocalAdaptationSensitivity the photoreceptors sensitivity renage is 0-1\n.   (more log compression effect when value increases)\n.   @param photoreceptorsTemporalConstant the time constant of the first order low pass filter of\n.   the photoreceptors, use it to cut high temporal frequencies (noise or fast motion), unit is\n.   frames, typical value is 1 frame\n.   @param photoreceptorsSpatialConstant the spatial constant of the first order low pass filter of\n.   the photoreceptors, use it to cut high spatial frequencies (noise or thick contours), unit is\n.   pixels, typical value is 1 pixel\n.   @param horizontalCellsGain gain of the horizontal cells network, if 0, then the mean value of\n.   the output is zero, if the parameter is near 1, then, the luminance is not filtered and is\n.   still reachable at the output, typicall value is 0\n.   @param HcellsTemporalConstant the time constant of the first order low pass filter of the\n.   horizontal cells, use it to cut low temporal frequencies (local luminance variations), unit is\n.   frames, typical value is 1 frame, as the photoreceptors\n.   @param HcellsSpatialConstant the spatial constant of the first order low pass filter of the\n.   horizontal cells, use it to cut low spatial frequencies (local luminance), unit is pixels,\n.   typical value is 5 pixel, this value is also used for local contrast computing when computing\n.   the local contrast adaptation at the ganglion cells level (Inner Plexiform Layer parvocellular\n.   channel model)\n.   @param ganglionCellsSensitivity the compression strengh of the ganglion cells local adaptation\n.   output, set a value between 0.6 and 1 for best results, a high value increases more the low\n.   value sensitivity... and the output saturates faster, recommended value: 0.7"},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_Retina_write, 0), "write(fs) -> None\n.   @brief Write xml/yml formated parameters information\n.   @param fs the filename of the xml file that will be open and writen with formatted parameters\n.   information"},

    {NULL,          NULL}
};

static void pyopencv_bioinspired_Retina_specials(void)
{
    pyopencv_bioinspired_Retina_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_bioinspired_Retina_Type.tp_dealloc = pyopencv_bioinspired_Retina_dealloc;
    pyopencv_bioinspired_Retina_Type.tp_repr = pyopencv_bioinspired_Retina_repr;
    pyopencv_bioinspired_Retina_Type.tp_getset = pyopencv_bioinspired_Retina_getseters;
    pyopencv_bioinspired_Retina_Type.tp_init = (initproc)0;
    pyopencv_bioinspired_Retina_Type.tp_methods = pyopencv_bioinspired_Retina_methods;
}

static PyObject* pyopencv_bioinspired_RetinaFastToneMapping_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bioinspired_RetinaFastToneMapping %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bioinspired_RetinaFastToneMapping_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bioinspired_bioinspired_RetinaFastToneMapping_applyFastToneMapping(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::RetinaFastToneMapping* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_RetinaFastToneMapping_Type))
        _self_ = dynamic_cast<cv::bioinspired::RetinaFastToneMapping*>(((pyopencv_bioinspired_RetinaFastToneMapping_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_RetinaFastToneMapping' or its derivative)");
    {
    PyObject* pyobj_inputImage = NULL;
    Mat inputImage;
    PyObject* pyobj_outputToneMappedImage = NULL;
    Mat outputToneMappedImage;

    const char* keywords[] = { "inputImage", "outputToneMappedImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:bioinspired_RetinaFastToneMapping.applyFastToneMapping", (char**)keywords, &pyobj_inputImage, &pyobj_outputToneMappedImage) &&
        pyopencv_to(pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)) &&
        pyopencv_to(pyobj_outputToneMappedImage, outputToneMappedImage, ArgInfo("outputToneMappedImage", 1)) )
    {
        ERRWRAP2(_self_->applyFastToneMapping(inputImage, outputToneMappedImage));
        return pyopencv_from(outputToneMappedImage);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputImage = NULL;
    UMat inputImage;
    PyObject* pyobj_outputToneMappedImage = NULL;
    UMat outputToneMappedImage;

    const char* keywords[] = { "inputImage", "outputToneMappedImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:bioinspired_RetinaFastToneMapping.applyFastToneMapping", (char**)keywords, &pyobj_inputImage, &pyobj_outputToneMappedImage) &&
        pyopencv_to(pyobj_inputImage, inputImage, ArgInfo("inputImage", 0)) &&
        pyopencv_to(pyobj_outputToneMappedImage, outputToneMappedImage, ArgInfo("outputToneMappedImage", 1)) )
    {
        ERRWRAP2(_self_->applyFastToneMapping(inputImage, outputToneMappedImage));
        return pyopencv_from(outputToneMappedImage);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_RetinaFastToneMapping_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    PyObject* pyobj_inputSize = NULL;
    Size inputSize;
    Ptr<RetinaFastToneMapping> retval;

    const char* keywords[] = { "inputSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_RetinaFastToneMapping.create", (char**)keywords, &pyobj_inputSize) &&
        pyopencv_to(pyobj_inputSize, inputSize, ArgInfo("inputSize", 0)) )
    {
        ERRWRAP2(retval = cv::bioinspired::RetinaFastToneMapping::create(inputSize));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_RetinaFastToneMapping_setup(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::RetinaFastToneMapping* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_RetinaFastToneMapping_Type))
        _self_ = dynamic_cast<cv::bioinspired::RetinaFastToneMapping*>(((pyopencv_bioinspired_RetinaFastToneMapping_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_RetinaFastToneMapping' or its derivative)");
    float photoreceptorsNeighborhoodRadius=3.f;
    float ganglioncellsNeighborhoodRadius=1.f;
    float meanLuminanceModulatorK=1.f;

    const char* keywords[] = { "photoreceptorsNeighborhoodRadius", "ganglioncellsNeighborhoodRadius", "meanLuminanceModulatorK", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|fff:bioinspired_RetinaFastToneMapping.setup", (char**)keywords, &photoreceptorsNeighborhoodRadius, &ganglioncellsNeighborhoodRadius, &meanLuminanceModulatorK) )
    {
        ERRWRAP2(_self_->setup(photoreceptorsNeighborhoodRadius, ganglioncellsNeighborhoodRadius, meanLuminanceModulatorK));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_bioinspired_RetinaFastToneMapping_methods[] =
{
    {"applyFastToneMapping", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_RetinaFastToneMapping_applyFastToneMapping, 0), "applyFastToneMapping(inputImage[, outputToneMappedImage]) -> outputToneMappedImage\n.   @brief applies a luminance correction (initially High Dynamic Range (HDR) tone mapping)\n.   \n.   using only the 2 local adaptation stages of the retina parvocellular channel : photoreceptors\n.   level and ganlion cells level. Spatio temporal filtering is applied but limited to temporal\n.   smoothing and eventually high frequencies attenuation. This is a lighter method than the one\n.   available using the regular retina::run method. It is then faster but it does not include\n.   complete temporal filtering nor retina spectral whitening. Then, it can have a more limited\n.   effect on images with a very high dynamic range. This is an adptation of the original still\n.   image HDR tone mapping algorithm of David Alleyson, Sabine Susstruck and Laurence Meylan's\n.   work, please cite: -> Meylan L., Alleysson D., and Susstrunk S., A Model of Retinal Local\n.   Adaptation for the Tone Mapping of Color Filter Array Images, Journal of Optical Society of\n.   America, A, Vol. 24, N 9, September, 1st, 2007, pp. 2807-2816\n.   \n.   @param inputImage the input image to process RGB or gray levels\n.   @param outputToneMappedImage the output tone mapped image"},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_RetinaFastToneMapping_create_cls, METH_CLASS), "create(inputSize) -> retval\n."},
    {"setup", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_RetinaFastToneMapping_setup, 0), "setup([, photoreceptorsNeighborhoodRadius[, ganglioncellsNeighborhoodRadius[, meanLuminanceModulatorK]]]) -> None\n.   @brief updates tone mapping behaviors by adjusing the local luminance computation area\n.   \n.   @param photoreceptorsNeighborhoodRadius the first stage local adaptation area\n.   @param ganglioncellsNeighborhoodRadius the second stage local adaptation area\n.   @param meanLuminanceModulatorK the factor applied to modulate the meanLuminance information\n.   (default is 1, see reference paper)"},

    {NULL,          NULL}
};

static void pyopencv_bioinspired_RetinaFastToneMapping_specials(void)
{
    pyopencv_bioinspired_RetinaFastToneMapping_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_bioinspired_RetinaFastToneMapping_Type.tp_dealloc = pyopencv_bioinspired_RetinaFastToneMapping_dealloc;
    pyopencv_bioinspired_RetinaFastToneMapping_Type.tp_repr = pyopencv_bioinspired_RetinaFastToneMapping_repr;
    pyopencv_bioinspired_RetinaFastToneMapping_Type.tp_getset = pyopencv_bioinspired_RetinaFastToneMapping_getseters;
    pyopencv_bioinspired_RetinaFastToneMapping_Type.tp_init = (initproc)0;
    pyopencv_bioinspired_RetinaFastToneMapping_Type.tp_methods = pyopencv_bioinspired_RetinaFastToneMapping_methods;
}

static PyObject* pyopencv_bioinspired_TransientAreasSegmentationModule_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bioinspired_TransientAreasSegmentationModule %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bioinspired_TransientAreasSegmentationModule_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_clearAllBuffers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clearAllBuffers());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    PyObject* pyobj_inputSize = NULL;
    Size inputSize;
    Ptr<TransientAreasSegmentationModule> retval;

    const char* keywords[] = { "inputSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_TransientAreasSegmentationModule.create", (char**)keywords, &pyobj_inputSize) &&
        pyopencv_to(pyobj_inputSize, inputSize, ArgInfo("inputSize", 0)) )
    {
        ERRWRAP2(retval = cv::bioinspired::TransientAreasSegmentationModule::create(inputSize));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_getSegmentationPicture(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");
    {
    PyObject* pyobj_transientAreas = NULL;
    Mat transientAreas;

    const char* keywords[] = { "transientAreas", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_TransientAreasSegmentationModule.getSegmentationPicture", (char**)keywords, &pyobj_transientAreas) &&
        pyopencv_to(pyobj_transientAreas, transientAreas, ArgInfo("transientAreas", 1)) )
    {
        ERRWRAP2(_self_->getSegmentationPicture(transientAreas));
        return pyopencv_from(transientAreas);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_transientAreas = NULL;
    UMat transientAreas;

    const char* keywords[] = { "transientAreas", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bioinspired_TransientAreasSegmentationModule.getSegmentationPicture", (char**)keywords, &pyobj_transientAreas) &&
        pyopencv_to(pyobj_transientAreas, transientAreas, ArgInfo("transientAreas", 1)) )
    {
        ERRWRAP2(_self_->getSegmentationPicture(transientAreas));
        return pyopencv_from(transientAreas);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_getSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_printSetup(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->printSetup());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_run(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");
    {
    PyObject* pyobj_inputToSegment = NULL;
    Mat inputToSegment;
    int channelIndex=0;

    const char* keywords[] = { "inputToSegment", "channelIndex", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|i:bioinspired_TransientAreasSegmentationModule.run", (char**)keywords, &pyobj_inputToSegment, &channelIndex) &&
        pyopencv_to(pyobj_inputToSegment, inputToSegment, ArgInfo("inputToSegment", 0)) )
    {
        ERRWRAP2(_self_->run(inputToSegment, channelIndex));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_inputToSegment = NULL;
    UMat inputToSegment;
    int channelIndex=0;

    const char* keywords[] = { "inputToSegment", "channelIndex", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|i:bioinspired_TransientAreasSegmentationModule.run", (char**)keywords, &pyobj_inputToSegment, &channelIndex) &&
        pyopencv_to(pyobj_inputToSegment, inputToSegment, ArgInfo("inputToSegment", 0)) )
    {
        ERRWRAP2(_self_->run(inputToSegment, channelIndex));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_setup(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");
    PyObject* pyobj_segmentationParameterFile = NULL;
    String segmentationParameterFile="";
    bool applyDefaultSetupOnFailure=true;

    const char* keywords[] = { "segmentationParameterFile", "applyDefaultSetupOnFailure", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:bioinspired_TransientAreasSegmentationModule.setup", (char**)keywords, &pyobj_segmentationParameterFile, &applyDefaultSetupOnFailure) &&
        pyopencv_to(pyobj_segmentationParameterFile, segmentationParameterFile, ArgInfo("segmentationParameterFile", 0)) )
    {
        ERRWRAP2(_self_->setup(segmentationParameterFile, applyDefaultSetupOnFailure));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bioinspired;

    cv::bioinspired::TransientAreasSegmentationModule* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bioinspired_TransientAreasSegmentationModule_Type))
        _self_ = dynamic_cast<cv::bioinspired::TransientAreasSegmentationModule*>(((pyopencv_bioinspired_TransientAreasSegmentationModule_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bioinspired_TransientAreasSegmentationModule' or its derivative)");
    PyObject* pyobj_fs = NULL;
    String fs;

    const char* keywords[] = { "fs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:bioinspired_TransientAreasSegmentationModule.write", (char**)keywords, &pyobj_fs) &&
        pyopencv_to(pyobj_fs, fs, ArgInfo("fs", 0)) )
    {
        ERRWRAP2(_self_->write(fs));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_bioinspired_TransientAreasSegmentationModule_methods[] =
{
    {"clearAllBuffers", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_clearAllBuffers, 0), "clearAllBuffers() -> None\n.   @brief cleans all the buffers of the instance"},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_create_cls, METH_CLASS), "create(inputSize) -> retval\n.   @brief allocator\n.   @param inputSize : size of the images input to segment (output will be the same size)"},
    {"getSegmentationPicture", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_getSegmentationPicture, 0), "getSegmentationPicture([, transientAreas]) -> transientAreas\n.   @brief access function\n.   @return the last segmentation result: a boolean picture which is resampled between 0 and 255 for a display purpose"},
    {"getSize", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_getSize, 0), "getSize() -> retval\n.   @brief return the sze of the manage input and output images"},
    {"printSetup", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_printSetup, 0), "printSetup() -> retval\n.   @brief parameters setup display method\n.   @return a string which contains formatted parameters information"},
    {"run", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_run, 0), "run(inputToSegment[, channelIndex]) -> None\n.   @brief main processing method, get result using methods getSegmentationPicture()\n.   @param inputToSegment : the image to process, it must match the instance buffer size !\n.   @param channelIndex : the channel to process in case of multichannel images"},
    {"setup", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_setup, 0), "setup([, segmentationParameterFile[, applyDefaultSetupOnFailure]]) -> None\n.   @brief try to open an XML segmentation parameters file to adjust current segmentation instance setup\n.   \n.   - if the xml file does not exist, then default setup is applied\n.   - warning, Exceptions are thrown if read XML file is not valid\n.   @param segmentationParameterFile : the parameters filename\n.   @param applyDefaultSetupOnFailure : set to true if an error must be thrown on error"},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_bioinspired_bioinspired_TransientAreasSegmentationModule_write, 0), "write(fs) -> None\n.   @brief write xml/yml formated parameters information\n.   @param fs : the filename of the xml file that will be open and writen with formatted parameters information"},

    {NULL,          NULL}
};

static void pyopencv_bioinspired_TransientAreasSegmentationModule_specials(void)
{
    pyopencv_bioinspired_TransientAreasSegmentationModule_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_bioinspired_TransientAreasSegmentationModule_Type.tp_dealloc = pyopencv_bioinspired_TransientAreasSegmentationModule_dealloc;
    pyopencv_bioinspired_TransientAreasSegmentationModule_Type.tp_repr = pyopencv_bioinspired_TransientAreasSegmentationModule_repr;
    pyopencv_bioinspired_TransientAreasSegmentationModule_Type.tp_getset = pyopencv_bioinspired_TransientAreasSegmentationModule_getseters;
    pyopencv_bioinspired_TransientAreasSegmentationModule_Type.tp_init = (initproc)0;
    pyopencv_bioinspired_TransientAreasSegmentationModule_Type.tp_methods = pyopencv_bioinspired_TransientAreasSegmentationModule_methods;
}

static PyObject* pyopencv_dpm_DPMDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<dpm_DPMDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_dpm_DPMDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_dpm_DPMDetector_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_dpm_DPMDetector_specials(void)
{
    pyopencv_dpm_DPMDetector_Type.tp_base = NULL;
    pyopencv_dpm_DPMDetector_Type.tp_dealloc = pyopencv_dpm_DPMDetector_dealloc;
    pyopencv_dpm_DPMDetector_Type.tp_repr = pyopencv_dpm_DPMDetector_repr;
    pyopencv_dpm_DPMDetector_Type.tp_getset = pyopencv_dpm_DPMDetector_getseters;
    pyopencv_dpm_DPMDetector_Type.tp_init = (initproc)0;
    pyopencv_dpm_DPMDetector_Type.tp_methods = pyopencv_dpm_DPMDetector_methods;
}

static PyObject* pyopencv_dpm_DPMDetector_ObjectDetection_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<dpm_DPMDetector_ObjectDetection %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_dpm_DPMDetector_ObjectDetection_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_dpm_DPMDetector_ObjectDetection_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_dpm_DPMDetector_ObjectDetection_specials(void)
{
    pyopencv_dpm_DPMDetector_ObjectDetection_Type.tp_base = NULL;
    pyopencv_dpm_DPMDetector_ObjectDetection_Type.tp_dealloc = pyopencv_dpm_DPMDetector_ObjectDetection_dealloc;
    pyopencv_dpm_DPMDetector_ObjectDetection_Type.tp_repr = pyopencv_dpm_DPMDetector_ObjectDetection_repr;
    pyopencv_dpm_DPMDetector_ObjectDetection_Type.tp_getset = pyopencv_dpm_DPMDetector_ObjectDetection_getseters;
    pyopencv_dpm_DPMDetector_ObjectDetection_Type.tp_init = (initproc)0;
    pyopencv_dpm_DPMDetector_ObjectDetection_Type.tp_methods = pyopencv_dpm_DPMDetector_ObjectDetection_methods;
}

static PyObject* pyopencv_Feature2D_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<Feature2D %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_Feature2D_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_Feature2D_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;

    const char* keywords[] = { "image", "keypoints", "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_descriptors) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) )
    {
        ERRWRAP2(_self_->compute(image, keypoints, descriptors));
        return Py_BuildValue("(NN)", pyopencv_from(keypoints), pyopencv_from(descriptors));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    PyObject* pyobj_descriptors = NULL;
    UMat descriptors;

    const char* keywords[] = { "image", "keypoints", "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_descriptors) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) )
    {
        ERRWRAP2(_self_->compute(image, keypoints, descriptors));
        return Py_BuildValue("(NN)", pyopencv_from(keypoints), pyopencv_from(descriptors));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_keypoints = NULL;
    vector_vector_KeyPoint keypoints;
    PyObject* pyobj_descriptors = NULL;
    vector_Mat descriptors;

    const char* keywords[] = { "images", "keypoints", "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_images, &pyobj_keypoints, &pyobj_descriptors) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) )
    {
        ERRWRAP2(_self_->compute(images, keypoints, descriptors));
        return Py_BuildValue("(NN)", pyopencv_from(keypoints), pyopencv_from(descriptors));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_keypoints = NULL;
    vector_vector_KeyPoint keypoints;
    PyObject* pyobj_descriptors = NULL;
    vector_Mat descriptors;

    const char* keywords[] = { "images", "keypoints", "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:Feature2D.compute", (char**)keywords, &pyobj_images, &pyobj_keypoints, &pyobj_descriptors) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 1)) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) )
    {
        ERRWRAP2(_self_->compute(images, keypoints, descriptors));
        return Py_BuildValue("(NN)", pyopencv_from(keypoints), pyopencv_from(descriptors));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_defaultNorm(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->defaultNorm());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_descriptorSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->descriptorSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_descriptorType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->descriptorType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_detect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_KeyPoint keypoints;
    PyObject* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "image", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Feature2D.detect", (char**)keywords, &pyobj_image, &pyobj_mask) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->detect(image, keypoints, mask));
        return pyopencv_from(keypoints);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    vector_KeyPoint keypoints;
    PyObject* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "image", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Feature2D.detect", (char**)keywords, &pyobj_image, &pyobj_mask) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->detect(image, keypoints, mask));
        return pyopencv_from(keypoints);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    vector_vector_KeyPoint keypoints;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;

    const char* keywords[] = { "images", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Feature2D.detect", (char**)keywords, &pyobj_images, &pyobj_masks) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->detect(images, keypoints, masks));
        return pyopencv_from(keypoints);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    vector_vector_KeyPoint keypoints;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;

    const char* keywords[] = { "images", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Feature2D.detect", (char**)keywords, &pyobj_images, &pyobj_masks) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->detect(images, keypoints, masks));
        return pyopencv_from(keypoints);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_detectAndCompute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    vector_KeyPoint keypoints;
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;
    bool useProvidedKeypoints=false;

    const char* keywords[] = { "image", "mask", "descriptors", "useProvidedKeypoints", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Ob:Feature2D.detectAndCompute", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_descriptors, &useProvidedKeypoints) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) )
    {
        ERRWRAP2(_self_->detectAndCompute(image, mask, keypoints, descriptors, useProvidedKeypoints));
        return Py_BuildValue("(NN)", pyopencv_from(keypoints), pyopencv_from(descriptors));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_mask = NULL;
    UMat mask;
    vector_KeyPoint keypoints;
    PyObject* pyobj_descriptors = NULL;
    UMat descriptors;
    bool useProvidedKeypoints=false;

    const char* keywords[] = { "image", "mask", "descriptors", "useProvidedKeypoints", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Ob:Feature2D.detectAndCompute", (char**)keywords, &pyobj_image, &pyobj_mask, &pyobj_descriptors, &useProvidedKeypoints) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 1)) )
    {
        ERRWRAP2(_self_->detectAndCompute(image, mask, keypoints, descriptors, useProvidedKeypoints));
        return Py_BuildValue("(NN)", pyopencv_from(keypoints), pyopencv_from(descriptors));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    {
    PyObject* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Feature2D.read", (char**)keywords, &pyobj_fileName) &&
        pyopencv_to(pyobj_fileName, fileName, ArgInfo("fileName", 0)) )
    {
        ERRWRAP2(_self_->read(fileName));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_arg1 = NULL;
    FileNode arg1;

    const char* keywords[] = { "arg1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Feature2D.read", (char**)keywords, &pyobj_arg1) &&
        pyopencv_to(pyobj_arg1, arg1, ArgInfo("arg1", 0)) )
    {
        ERRWRAP2(_self_->read(arg1));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Feature2D_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Feature2D* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Feature2D_Type))
        _self_ = dynamic_cast<cv::Feature2D*>(((pyopencv_Feature2D_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Feature2D' or its derivative)");
    {
    PyObject* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Feature2D.write", (char**)keywords, &pyobj_fileName) &&
        pyopencv_to(pyobj_fileName, fileName, ArgInfo("fileName", 0)) )
    {
        ERRWRAP2(_self_->write(fileName));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_fs = NULL;
    Ptr<FileStorage> fs;
    PyObject* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Feature2D.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        pyopencv_to(pyobj_fs, fs, ArgInfo("fs", 0)) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) )
    {
        ERRWRAP2(_self_->write(fs, name));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_Feature2D_methods[] =
{
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_compute, 0), "compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n.   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n.   (second variant).\n.   \n.   @param image Image.\n.   @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n.   computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n.   with several dominant orientations (for each orientation).\n.   @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n.   descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n.   descriptor for keypoint j-th keypoint.\n\n\n\ncompute(images, keypoints[, descriptors]) -> keypoints, descriptors\n.   @overload\n.   \n.   @param images Image set.\n.   @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n.   computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n.   with several dominant orientations (for each orientation).\n.   @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n.   descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n.   descriptor for keypoint j-th keypoint."},
    {"defaultNorm", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_defaultNorm, 0), "defaultNorm() -> retval\n."},
    {"descriptorSize", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_descriptorSize, 0), "descriptorSize() -> retval\n."},
    {"descriptorType", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_descriptorType, 0), "descriptorType() -> retval\n."},
    {"detect", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_detect, 0), "detect(image[, mask]) -> keypoints\n.   @brief Detects keypoints in an image (first variant) or image set (second variant).\n.   \n.   @param image Image.\n.   @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n.   of keypoints detected in images[i] .\n.   @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n.   matrix with non-zero values in the region of interest.\n\n\n\ndetect(images[, masks]) -> keypoints\n.   @overload\n.   @param images Image set.\n.   @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n.   of keypoints detected in images[i] .\n.   @param masks Masks for each input image specifying where to look for keypoints (optional).\n.   masks[i] is a mask for images[i]."},
    {"detectAndCompute", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_detectAndCompute, 0), "detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n.   Detects keypoints and computes the descriptors"},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_empty, 0), "empty() -> retval\n."},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_read, 0), "read(fileName) -> None\n.   \n\n\n\nread(arg1) -> None\n."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_Feature2D_write, 0), "write(fileName) -> None\n.   \n\n\n\nwrite(fs[, name]) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_Feature2D_specials(void)
{
    pyopencv_Feature2D_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_Feature2D_Type.tp_dealloc = pyopencv_Feature2D_dealloc;
    pyopencv_Feature2D_Type.tp_repr = pyopencv_Feature2D_repr;
    pyopencv_Feature2D_Type.tp_getset = pyopencv_Feature2D_getseters;
    pyopencv_Feature2D_Type.tp_init = (initproc)0;
    pyopencv_Feature2D_Type.tp_methods = pyopencv_Feature2D_methods;
}

static PyObject* pyopencv_BRISK_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BRISK %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BRISK_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_BRISK_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {
    int thresh=30;
    int octaves=3;
    float patternScale=1.0f;
    Ptr<BRISK> retval;

    const char* keywords[] = { "thresh", "octaves", "patternScale", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iif:BRISK.create", (char**)keywords, &thresh, &octaves, &patternScale) )
    {
        ERRWRAP2(retval = cv::BRISK::create(thresh, octaves, patternScale));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_radiusList = NULL;
    vector_float radiusList;
    PyObject* pyobj_numberList = NULL;
    vector_int numberList;
    float dMax=5.85f;
    float dMin=8.2f;
    PyObject* pyobj_indexChange = NULL;
    vector_int indexChange=std::vector<int>();
    Ptr<BRISK> retval;

    const char* keywords[] = { "radiusList", "numberList", "dMax", "dMin", "indexChange", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|ffO:BRISK.create", (char**)keywords, &pyobj_radiusList, &pyobj_numberList, &dMax, &dMin, &pyobj_indexChange) &&
        pyopencv_to(pyobj_radiusList, radiusList, ArgInfo("radiusList", 0)) &&
        pyopencv_to(pyobj_numberList, numberList, ArgInfo("numberList", 0)) &&
        pyopencv_to(pyobj_indexChange, indexChange, ArgInfo("indexChange", 0)) )
    {
        ERRWRAP2(retval = cv::BRISK::create(radiusList, numberList, dMax, dMin, indexChange));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int thresh=0;
    int octaves=0;
    PyObject* pyobj_radiusList = NULL;
    vector_float radiusList;
    PyObject* pyobj_numberList = NULL;
    vector_int numberList;
    float dMax=5.85f;
    float dMin=8.2f;
    PyObject* pyobj_indexChange = NULL;
    vector_int indexChange=std::vector<int>();
    Ptr<BRISK> retval;

    const char* keywords[] = { "thresh", "octaves", "radiusList", "numberList", "dMax", "dMin", "indexChange", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiOO|ffO:BRISK.create", (char**)keywords, &thresh, &octaves, &pyobj_radiusList, &pyobj_numberList, &dMax, &dMin, &pyobj_indexChange) &&
        pyopencv_to(pyobj_radiusList, radiusList, ArgInfo("radiusList", 0)) &&
        pyopencv_to(pyobj_numberList, numberList, ArgInfo("numberList", 0)) &&
        pyopencv_to(pyobj_indexChange, indexChange, ArgInfo("indexChange", 0)) )
    {
        ERRWRAP2(retval = cv::BRISK::create(thresh, octaves, radiusList, numberList, dMax, dMin, indexChange));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_BRISK_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BRISK* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BRISK_Type))
        _self_ = dynamic_cast<cv::BRISK*>(((pyopencv_BRISK_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BRISK' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_BRISK_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_BRISK_create_cls, METH_CLASS), "create([, thresh[, octaves[, patternScale]]]) -> retval\n.   @brief The BRISK constructor\n.   \n.   @param thresh AGAST detection threshold score.\n.   @param octaves detection octaves. Use 0 to do single scale.\n.   @param patternScale apply this scale to the pattern used for sampling the neighbourhood of a\n.   keypoint.\n\n\n\ncreate(radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n.   @brief The BRISK constructor for a custom pattern\n.   \n.   @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n.   keypoint scale 1).\n.   @param numberList defines the number of sampling points on the sampling circle. Must be the same\n.   size as radiusList..\n.   @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n.   scale 1).\n.   @param dMin threshold for the long pairings used for orientation determination (in pixels for\n.   keypoint scale 1).\n.   @param indexChange index remapping of the bits.\n\n\n\ncreate(thresh, octaves, radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n.   @brief The BRISK constructor for a custom pattern, detection threshold and octaves\n.   \n.   @param thresh AGAST detection threshold score.\n.   @param octaves detection octaves. Use 0 to do single scale.\n.   @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n.   keypoint scale 1).\n.   @param numberList defines the number of sampling points on the sampling circle. Must be the same\n.   size as radiusList..\n.   @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n.   scale 1).\n.   @param dMin threshold for the long pairings used for orientation determination (in pixels for\n.   keypoint scale 1).\n.   @param indexChange index remapping of the bits."},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_BRISK_getDefaultName, 0), "getDefaultName() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_BRISK_specials(void)
{
    pyopencv_BRISK_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_BRISK_Type.tp_dealloc = pyopencv_BRISK_dealloc;
    pyopencv_BRISK_Type.tp_repr = pyopencv_BRISK_repr;
    pyopencv_BRISK_Type.tp_getset = pyopencv_BRISK_getseters;
    pyopencv_BRISK_Type.tp_init = (initproc)0;
    pyopencv_BRISK_Type.tp_methods = pyopencv_BRISK_methods;
}

static PyObject* pyopencv_ORB_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ORB %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ORB_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ORB_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int nfeatures=500;
    float scaleFactor=1.2f;
    int nlevels=8;
    int edgeThreshold=31;
    int firstLevel=0;
    int WTA_K=2;
    int scoreType=ORB::HARRIS_SCORE;
    int patchSize=31;
    int fastThreshold=20;
    Ptr<ORB> retval;

    const char* keywords[] = { "nfeatures", "scaleFactor", "nlevels", "edgeThreshold", "firstLevel", "WTA_K", "scoreType", "patchSize", "fastThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ifiiiiiii:ORB.create", (char**)keywords, &nfeatures, &scaleFactor, &nlevels, &edgeThreshold, &firstLevel, &WTA_K, &scoreType, &patchSize, &fastThreshold) )
    {
        ERRWRAP2(retval = cv::ORB::create(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel, WTA_K, scoreType, patchSize, fastThreshold));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getEdgeThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEdgeThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getFastThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFastThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getFirstLevel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFirstLevel());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getMaxFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxFeatures());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getNLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNLevels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getPatchSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPatchSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getScaleFactor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScaleFactor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getScoreType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScoreType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_getWTA_K(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWTA_K());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setEdgeThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int edgeThreshold=0;

    const char* keywords[] = { "edgeThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setEdgeThreshold", (char**)keywords, &edgeThreshold) )
    {
        ERRWRAP2(_self_->setEdgeThreshold(edgeThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setFastThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int fastThreshold=0;

    const char* keywords[] = { "fastThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setFastThreshold", (char**)keywords, &fastThreshold) )
    {
        ERRWRAP2(_self_->setFastThreshold(fastThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setFirstLevel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int firstLevel=0;

    const char* keywords[] = { "firstLevel", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setFirstLevel", (char**)keywords, &firstLevel) )
    {
        ERRWRAP2(_self_->setFirstLevel(firstLevel));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setMaxFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int maxFeatures=0;

    const char* keywords[] = { "maxFeatures", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setMaxFeatures", (char**)keywords, &maxFeatures) )
    {
        ERRWRAP2(_self_->setMaxFeatures(maxFeatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setNLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int nlevels=0;

    const char* keywords[] = { "nlevels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setNLevels", (char**)keywords, &nlevels) )
    {
        ERRWRAP2(_self_->setNLevels(nlevels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setPatchSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int patchSize=0;

    const char* keywords[] = { "patchSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setPatchSize", (char**)keywords, &patchSize) )
    {
        ERRWRAP2(_self_->setPatchSize(patchSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setScaleFactor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    double scaleFactor=0;

    const char* keywords[] = { "scaleFactor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ORB.setScaleFactor", (char**)keywords, &scaleFactor) )
    {
        ERRWRAP2(_self_->setScaleFactor(scaleFactor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setScoreType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int scoreType=0;

    const char* keywords[] = { "scoreType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setScoreType", (char**)keywords, &scoreType) )
    {
        ERRWRAP2(_self_->setScoreType(scoreType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ORB_setWTA_K(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::ORB* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ORB_Type))
        _self_ = dynamic_cast<cv::ORB*>(((pyopencv_ORB_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ORB' or its derivative)");
    int wta_k=0;

    const char* keywords[] = { "wta_k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ORB.setWTA_K", (char**)keywords, &wta_k) )
    {
        ERRWRAP2(_self_->setWTA_K(wta_k));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ORB_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_create_cls, METH_CLASS), "create([, nfeatures[, scaleFactor[, nlevels[, edgeThreshold[, firstLevel[, WTA_K[, scoreType[, patchSize[, fastThreshold]]]]]]]]]) -> retval\n.   @brief The ORB constructor\n.   \n.   @param nfeatures The maximum number of features to retain.\n.   @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical\n.   pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor\n.   will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor\n.   will mean that to cover certain scale range you will need more pyramid levels and so the speed\n.   will suffer.\n.   @param nlevels The number of pyramid levels. The smallest level will have linear size equal to\n.   input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).\n.   @param edgeThreshold This is size of the border where the features are not detected. It should\n.   roughly match the patchSize parameter.\n.   @param firstLevel The level of pyramid to put source image to. Previous layers are filled\n.   with upscaled source image.\n.   @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The\n.   default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,\n.   so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3\n.   random points (of course, those point coordinates are random, but they are generated from the\n.   pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel\n.   rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such\n.   output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,\n.   denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each\n.   bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).\n.   @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features\n.   (the score is written to KeyPoint::score and is used to retain best nfeatures features);\n.   FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,\n.   but it is a little faster to compute.\n.   @param patchSize size of the patch used by the oriented BRIEF descriptor. Of course, on smaller\n.   pyramid layers the perceived image area covered by a feature will be larger.\n.   @param fastThreshold"},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getEdgeThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getEdgeThreshold, 0), "getEdgeThreshold() -> retval\n."},
    {"getFastThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getFastThreshold, 0), "getFastThreshold() -> retval\n."},
    {"getFirstLevel", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getFirstLevel, 0), "getFirstLevel() -> retval\n."},
    {"getMaxFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getMaxFeatures, 0), "getMaxFeatures() -> retval\n."},
    {"getNLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getNLevels, 0), "getNLevels() -> retval\n."},
    {"getPatchSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getPatchSize, 0), "getPatchSize() -> retval\n."},
    {"getScaleFactor", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getScoreType", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getScoreType, 0), "getScoreType() -> retval\n."},
    {"getWTA_K", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_getWTA_K, 0), "getWTA_K() -> retval\n."},
    {"setEdgeThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setEdgeThreshold, 0), "setEdgeThreshold(edgeThreshold) -> None\n."},
    {"setFastThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setFastThreshold, 0), "setFastThreshold(fastThreshold) -> None\n."},
    {"setFirstLevel", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setFirstLevel, 0), "setFirstLevel(firstLevel) -> None\n."},
    {"setMaxFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setMaxFeatures, 0), "setMaxFeatures(maxFeatures) -> None\n."},
    {"setNLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setNLevels, 0), "setNLevels(nlevels) -> None\n."},
    {"setPatchSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setPatchSize, 0), "setPatchSize(patchSize) -> None\n."},
    {"setScaleFactor", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setScaleFactor, 0), "setScaleFactor(scaleFactor) -> None\n."},
    {"setScoreType", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setScoreType, 0), "setScoreType(scoreType) -> None\n."},
    {"setWTA_K", CV_PY_FN_WITH_KW_(pyopencv_cv_ORB_setWTA_K, 0), "setWTA_K(wta_k) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_ORB_specials(void)
{
    pyopencv_ORB_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_ORB_Type.tp_dealloc = pyopencv_ORB_dealloc;
    pyopencv_ORB_Type.tp_repr = pyopencv_ORB_repr;
    pyopencv_ORB_Type.tp_getset = pyopencv_ORB_getseters;
    pyopencv_ORB_Type.tp_init = (initproc)0;
    pyopencv_ORB_Type.tp_methods = pyopencv_ORB_methods;
}

static PyObject* pyopencv_MSER_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<MSER %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_MSER_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_MSER_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int _delta=5;
    int _min_area=60;
    int _max_area=14400;
    double _max_variation=0.25;
    double _min_diversity=.2;
    int _max_evolution=200;
    double _area_threshold=1.01;
    double _min_margin=0.003;
    int _edge_blur_size=5;
    Ptr<MSER> retval;

    const char* keywords[] = { "_delta", "_min_area", "_max_area", "_max_variation", "_min_diversity", "_max_evolution", "_area_threshold", "_min_margin", "_edge_blur_size", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiiddiddi:MSER.create", (char**)keywords, &_delta, &_min_area, &_max_area, &_max_variation, &_min_diversity, &_max_evolution, &_area_threshold, &_min_margin, &_edge_blur_size) )
    {
        ERRWRAP2(retval = cv::MSER::create(_delta, _min_area, _max_area, _max_variation, _min_diversity, _max_evolution, _area_threshold, _min_margin, _edge_blur_size));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_detectRegions(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_vector_Point msers;
    vector_Rect bboxes;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MSER.detectRegions", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(_self_->detectRegions(image, msers, bboxes));
        return Py_BuildValue("(NN)", pyopencv_from(msers), pyopencv_from(bboxes));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    vector_vector_Point msers;
    vector_Rect bboxes;

    const char* keywords[] = { "image", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:MSER.detectRegions", (char**)keywords, &pyobj_image) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) )
    {
        ERRWRAP2(_self_->detectRegions(image, msers, bboxes));
        return Py_BuildValue("(NN)", pyopencv_from(msers), pyopencv_from(bboxes));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_getDelta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDelta());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_getMaxArea(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxArea());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_getMinArea(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinArea());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_getPass2Only(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPass2Only());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_setDelta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    int delta=0;

    const char* keywords[] = { "delta", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:MSER.setDelta", (char**)keywords, &delta) )
    {
        ERRWRAP2(_self_->setDelta(delta));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_setMaxArea(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    int maxArea=0;

    const char* keywords[] = { "maxArea", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:MSER.setMaxArea", (char**)keywords, &maxArea) )
    {
        ERRWRAP2(_self_->setMaxArea(maxArea));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_setMinArea(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    int minArea=0;

    const char* keywords[] = { "minArea", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:MSER.setMinArea", (char**)keywords, &minArea) )
    {
        ERRWRAP2(_self_->setMinArea(minArea));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_MSER_setPass2Only(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::MSER* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_MSER_Type))
        _self_ = dynamic_cast<cv::MSER*>(((pyopencv_MSER_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'MSER' or its derivative)");
    bool f=0;

    const char* keywords[] = { "f", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:MSER.setPass2Only", (char**)keywords, &f) )
    {
        ERRWRAP2(_self_->setPass2Only(f));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_MSER_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_create_cls, METH_CLASS), "create([, _delta[, _min_area[, _max_area[, _max_variation[, _min_diversity[, _max_evolution[, _area_threshold[, _min_margin[, _edge_blur_size]]]]]]]]]) -> retval\n.   @brief Full consturctor for %MSER detector\n.   \n.   @param _delta it compares \\f$(size_{i}-size_{i-delta})/size_{i-delta}\\f$\n.   @param _min_area prune the area which smaller than minArea\n.   @param _max_area prune the area which bigger than maxArea\n.   @param _max_variation prune the area have similar size to its children\n.   @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity\n.   @param _max_evolution  for color image, the evolution steps\n.   @param _area_threshold for color image, the area threshold to cause re-initialize\n.   @param _min_margin for color image, ignore too small margin\n.   @param _edge_blur_size for color image, the aperture size for edge blur"},
    {"detectRegions", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_detectRegions, 0), "detectRegions(image) -> msers, bboxes\n.   @brief Detect %MSER regions\n.   \n.   @param image input image (8UC1, 8UC3 or 8UC4, must be greater or equal than 3x3)\n.   @param msers resulting list of point sets\n.   @param bboxes resulting bounding boxes"},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDelta", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_getDelta, 0), "getDelta() -> retval\n."},
    {"getMaxArea", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_getMaxArea, 0), "getMaxArea() -> retval\n."},
    {"getMinArea", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_getMinArea, 0), "getMinArea() -> retval\n."},
    {"getPass2Only", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_getPass2Only, 0), "getPass2Only() -> retval\n."},
    {"setDelta", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_setDelta, 0), "setDelta(delta) -> None\n."},
    {"setMaxArea", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_setMaxArea, 0), "setMaxArea(maxArea) -> None\n."},
    {"setMinArea", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_setMinArea, 0), "setMinArea(minArea) -> None\n."},
    {"setPass2Only", CV_PY_FN_WITH_KW_(pyopencv_cv_MSER_setPass2Only, 0), "setPass2Only(f) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_MSER_specials(void)
{
    pyopencv_MSER_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_MSER_Type.tp_dealloc = pyopencv_MSER_dealloc;
    pyopencv_MSER_Type.tp_repr = pyopencv_MSER_repr;
    pyopencv_MSER_Type.tp_getset = pyopencv_MSER_getseters;
    pyopencv_MSER_Type.tp_init = (initproc)0;
    pyopencv_MSER_Type.tp_methods = pyopencv_MSER_methods;
}

static PyObject* pyopencv_FastFeatureDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<FastFeatureDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_FastFeatureDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_FastFeatureDetector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int threshold=10;
    bool nonmaxSuppression=true;
    int type=FastFeatureDetector::TYPE_9_16;
    Ptr<FastFeatureDetector> retval;

    const char* keywords[] = { "threshold", "nonmaxSuppression", "type", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ibi:FastFeatureDetector.create", (char**)keywords, &threshold, &nonmaxSuppression, &type) )
    {
        ERRWRAP2(retval = cv::FastFeatureDetector::create(threshold, nonmaxSuppression, type));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_getNonmaxSuppression(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNonmaxSuppression());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_getType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_setNonmaxSuppression(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    bool f=0;

    const char* keywords[] = { "f", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:FastFeatureDetector.setNonmaxSuppression", (char**)keywords, &f) )
    {
        ERRWRAP2(_self_->setNonmaxSuppression(f));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    int threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FastFeatureDetector.setThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_FastFeatureDetector_setType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::FastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_FastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::FastFeatureDetector*>(((pyopencv_FastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'FastFeatureDetector' or its derivative)");
    int type=0;

    const char* keywords[] = { "type", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:FastFeatureDetector.setType", (char**)keywords, &type) )
    {
        ERRWRAP2(_self_->setType(type));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_FastFeatureDetector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_create_cls, METH_CLASS), "create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n."},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getNonmaxSuppression", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getNonmaxSuppression, 0), "getNonmaxSuppression() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getThreshold, 0), "getThreshold() -> retval\n."},
    {"getType", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_getType, 0), "getType() -> retval\n."},
    {"setNonmaxSuppression", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_setNonmaxSuppression, 0), "setNonmaxSuppression(f) -> None\n."},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_setThreshold, 0), "setThreshold(threshold) -> None\n."},
    {"setType", CV_PY_FN_WITH_KW_(pyopencv_cv_FastFeatureDetector_setType, 0), "setType(type) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_FastFeatureDetector_specials(void)
{
    pyopencv_FastFeatureDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_FastFeatureDetector_Type.tp_dealloc = pyopencv_FastFeatureDetector_dealloc;
    pyopencv_FastFeatureDetector_Type.tp_repr = pyopencv_FastFeatureDetector_repr;
    pyopencv_FastFeatureDetector_Type.tp_getset = pyopencv_FastFeatureDetector_getseters;
    pyopencv_FastFeatureDetector_Type.tp_init = (initproc)0;
    pyopencv_FastFeatureDetector_Type.tp_methods = pyopencv_FastFeatureDetector_methods;
}

static PyObject* pyopencv_AgastFeatureDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<AgastFeatureDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_AgastFeatureDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_AgastFeatureDetector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int threshold=10;
    bool nonmaxSuppression=true;
    int type=AgastFeatureDetector::OAST_9_16;
    Ptr<AgastFeatureDetector> retval;

    const char* keywords[] = { "threshold", "nonmaxSuppression", "type", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ibi:AgastFeatureDetector.create", (char**)keywords, &threshold, &nonmaxSuppression, &type) )
    {
        ERRWRAP2(retval = cv::AgastFeatureDetector::create(threshold, nonmaxSuppression, type));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_getNonmaxSuppression(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNonmaxSuppression());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_getType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_setNonmaxSuppression(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    bool f=0;

    const char* keywords[] = { "f", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:AgastFeatureDetector.setNonmaxSuppression", (char**)keywords, &f) )
    {
        ERRWRAP2(_self_->setNonmaxSuppression(f));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    int threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AgastFeatureDetector.setThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AgastFeatureDetector_setType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AgastFeatureDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AgastFeatureDetector_Type))
        _self_ = dynamic_cast<cv::AgastFeatureDetector*>(((pyopencv_AgastFeatureDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AgastFeatureDetector' or its derivative)");
    int type=0;

    const char* keywords[] = { "type", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AgastFeatureDetector.setType", (char**)keywords, &type) )
    {
        ERRWRAP2(_self_->setType(type));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_AgastFeatureDetector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_create_cls, METH_CLASS), "create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n."},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getNonmaxSuppression", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getNonmaxSuppression, 0), "getNonmaxSuppression() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getThreshold, 0), "getThreshold() -> retval\n."},
    {"getType", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_getType, 0), "getType() -> retval\n."},
    {"setNonmaxSuppression", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_setNonmaxSuppression, 0), "setNonmaxSuppression(f) -> None\n."},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_setThreshold, 0), "setThreshold(threshold) -> None\n."},
    {"setType", CV_PY_FN_WITH_KW_(pyopencv_cv_AgastFeatureDetector_setType, 0), "setType(type) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_AgastFeatureDetector_specials(void)
{
    pyopencv_AgastFeatureDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_AgastFeatureDetector_Type.tp_dealloc = pyopencv_AgastFeatureDetector_dealloc;
    pyopencv_AgastFeatureDetector_Type.tp_repr = pyopencv_AgastFeatureDetector_repr;
    pyopencv_AgastFeatureDetector_Type.tp_getset = pyopencv_AgastFeatureDetector_getseters;
    pyopencv_AgastFeatureDetector_Type.tp_init = (initproc)0;
    pyopencv_AgastFeatureDetector_Type.tp_methods = pyopencv_AgastFeatureDetector_methods;
}

static PyObject* pyopencv_GFTTDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<GFTTDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_GFTTDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_GFTTDetector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {
    int maxCorners=1000;
    double qualityLevel=0.01;
    double minDistance=1;
    int blockSize=3;
    bool useHarrisDetector=false;
    double k=0.04;
    Ptr<GFTTDetector> retval;

    const char* keywords[] = { "maxCorners", "qualityLevel", "minDistance", "blockSize", "useHarrisDetector", "k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iddibd:GFTTDetector.create", (char**)keywords, &maxCorners, &qualityLevel, &minDistance, &blockSize, &useHarrisDetector, &k) )
    {
        ERRWRAP2(retval = cv::GFTTDetector::create(maxCorners, qualityLevel, minDistance, blockSize, useHarrisDetector, k));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int maxCorners=0;
    double qualityLevel=0;
    double minDistance=0;
    int blockSize=0;
    int gradiantSize=0;
    bool useHarrisDetector=false;
    double k=0.04;
    Ptr<GFTTDetector> retval;

    const char* keywords[] = { "maxCorners", "qualityLevel", "minDistance", "blockSize", "gradiantSize", "useHarrisDetector", "k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iddii|bd:GFTTDetector.create", (char**)keywords, &maxCorners, &qualityLevel, &minDistance, &blockSize, &gradiantSize, &useHarrisDetector, &k) )
    {
        ERRWRAP2(retval = cv::GFTTDetector::create(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize, useHarrisDetector, k));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBlockSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getHarrisDetector(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHarrisDetector());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getK());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getMaxFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxFeatures());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getMinDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinDistance());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_getQualityLevel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getQualityLevel());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_setBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    int blockSize=0;

    const char* keywords[] = { "blockSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:GFTTDetector.setBlockSize", (char**)keywords, &blockSize) )
    {
        ERRWRAP2(_self_->setBlockSize(blockSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_setHarrisDetector(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:GFTTDetector.setHarrisDetector", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setHarrisDetector(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_setK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    double k=0;

    const char* keywords[] = { "k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:GFTTDetector.setK", (char**)keywords, &k) )
    {
        ERRWRAP2(_self_->setK(k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_setMaxFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    int maxFeatures=0;

    const char* keywords[] = { "maxFeatures", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:GFTTDetector.setMaxFeatures", (char**)keywords, &maxFeatures) )
    {
        ERRWRAP2(_self_->setMaxFeatures(maxFeatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_setMinDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    double minDistance=0;

    const char* keywords[] = { "minDistance", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:GFTTDetector.setMinDistance", (char**)keywords, &minDistance) )
    {
        ERRWRAP2(_self_->setMinDistance(minDistance));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_GFTTDetector_setQualityLevel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::GFTTDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_GFTTDetector_Type))
        _self_ = dynamic_cast<cv::GFTTDetector*>(((pyopencv_GFTTDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'GFTTDetector' or its derivative)");
    double qlevel=0;

    const char* keywords[] = { "qlevel", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:GFTTDetector.setQualityLevel", (char**)keywords, &qlevel) )
    {
        ERRWRAP2(_self_->setQualityLevel(qlevel));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_GFTTDetector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_create_cls, METH_CLASS), "create([, maxCorners[, qualityLevel[, minDistance[, blockSize[, useHarrisDetector[, k]]]]]]) -> retval\n.   \n\n\n\ncreate(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize[, useHarrisDetector[, k]]) -> retval\n."},
    {"getBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getBlockSize, 0), "getBlockSize() -> retval\n."},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getHarrisDetector", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getHarrisDetector, 0), "getHarrisDetector() -> retval\n."},
    {"getK", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getK, 0), "getK() -> retval\n."},
    {"getMaxFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getMaxFeatures, 0), "getMaxFeatures() -> retval\n."},
    {"getMinDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getMinDistance, 0), "getMinDistance() -> retval\n."},
    {"getQualityLevel", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_getQualityLevel, 0), "getQualityLevel() -> retval\n."},
    {"setBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setBlockSize, 0), "setBlockSize(blockSize) -> None\n."},
    {"setHarrisDetector", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setHarrisDetector, 0), "setHarrisDetector(val) -> None\n."},
    {"setK", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setK, 0), "setK(k) -> None\n."},
    {"setMaxFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setMaxFeatures, 0), "setMaxFeatures(maxFeatures) -> None\n."},
    {"setMinDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setMinDistance, 0), "setMinDistance(minDistance) -> None\n."},
    {"setQualityLevel", CV_PY_FN_WITH_KW_(pyopencv_cv_GFTTDetector_setQualityLevel, 0), "setQualityLevel(qlevel) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_GFTTDetector_specials(void)
{
    pyopencv_GFTTDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_GFTTDetector_Type.tp_dealloc = pyopencv_GFTTDetector_dealloc;
    pyopencv_GFTTDetector_Type.tp_repr = pyopencv_GFTTDetector_repr;
    pyopencv_GFTTDetector_Type.tp_getset = pyopencv_GFTTDetector_getseters;
    pyopencv_GFTTDetector_Type.tp_init = (initproc)0;
    pyopencv_GFTTDetector_Type.tp_methods = pyopencv_GFTTDetector_methods;
}

static PyObject* pyopencv_SimpleBlobDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<SimpleBlobDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_SimpleBlobDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_SimpleBlobDetector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_parameters = NULL;
    SimpleBlobDetector_Params parameters=SimpleBlobDetector::Params();
    Ptr<SimpleBlobDetector> retval;

    const char* keywords[] = { "parameters", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:SimpleBlobDetector.create", (char**)keywords, &pyobj_parameters) &&
        pyopencv_to(pyobj_parameters, parameters, ArgInfo("parameters", 0)) )
    {
        ERRWRAP2(retval = cv::SimpleBlobDetector::create(parameters));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_SimpleBlobDetector_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::SimpleBlobDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_SimpleBlobDetector_Type))
        _self_ = dynamic_cast<cv::SimpleBlobDetector*>(((pyopencv_SimpleBlobDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'SimpleBlobDetector' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_SimpleBlobDetector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_create_cls, METH_CLASS), "create([, parameters]) -> retval\n."},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_SimpleBlobDetector_getDefaultName, 0), "getDefaultName() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_SimpleBlobDetector_specials(void)
{
    pyopencv_SimpleBlobDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_SimpleBlobDetector_Type.tp_dealloc = pyopencv_SimpleBlobDetector_dealloc;
    pyopencv_SimpleBlobDetector_Type.tp_repr = pyopencv_SimpleBlobDetector_repr;
    pyopencv_SimpleBlobDetector_Type.tp_getset = pyopencv_SimpleBlobDetector_getseters;
    pyopencv_SimpleBlobDetector_Type.tp_init = (initproc)0;
    pyopencv_SimpleBlobDetector_Type.tp_methods = pyopencv_SimpleBlobDetector_methods;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<SimpleBlobDetector_Params %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_SimpleBlobDetector_Params_get_blobColor(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.blobColor);
}

static int pyopencv_SimpleBlobDetector_Params_set_blobColor(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the blobColor attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.blobColor) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByArea(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.filterByArea);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByArea(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByArea attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.filterByArea) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByCircularity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.filterByCircularity);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByCircularity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByCircularity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.filterByCircularity) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByColor(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.filterByColor);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByColor(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByColor attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.filterByColor) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByConvexity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.filterByConvexity);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByConvexity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByConvexity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.filterByConvexity) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_filterByInertia(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.filterByInertia);
}

static int pyopencv_SimpleBlobDetector_Params_set_filterByInertia(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the filterByInertia attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.filterByInertia) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxArea(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.maxArea);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxArea(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxArea attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.maxArea) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxCircularity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.maxCircularity);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxCircularity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxCircularity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.maxCircularity) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxConvexity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.maxConvexity);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxConvexity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxConvexity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.maxConvexity) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.maxInertiaRatio);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxInertiaRatio attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.maxInertiaRatio) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_maxThreshold(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.maxThreshold);
}

static int pyopencv_SimpleBlobDetector_Params_set_maxThreshold(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxThreshold attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.maxThreshold) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minArea(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minArea);
}

static int pyopencv_SimpleBlobDetector_Params_set_minArea(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minArea attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minArea) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minCircularity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minCircularity);
}

static int pyopencv_SimpleBlobDetector_Params_set_minCircularity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minCircularity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minCircularity) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minConvexity(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minConvexity);
}

static int pyopencv_SimpleBlobDetector_Params_set_minConvexity(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minConvexity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minConvexity) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minDistBetweenBlobs(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minDistBetweenBlobs);
}

static int pyopencv_SimpleBlobDetector_Params_set_minDistBetweenBlobs(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDistBetweenBlobs attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minDistBetweenBlobs) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minInertiaRatio);
}

static int pyopencv_SimpleBlobDetector_Params_set_minInertiaRatio(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minInertiaRatio attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minInertiaRatio) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minRepeatability(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minRepeatability);
}

static int pyopencv_SimpleBlobDetector_Params_set_minRepeatability(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minRepeatability attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minRepeatability) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_minThreshold(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.minThreshold);
}

static int pyopencv_SimpleBlobDetector_Params_set_minThreshold(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minThreshold attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minThreshold) ? 0 : -1;
}

static PyObject* pyopencv_SimpleBlobDetector_Params_get_thresholdStep(pyopencv_SimpleBlobDetector_Params_t* p, void *closure)
{
    return pyopencv_from(p->v.thresholdStep);
}

static int pyopencv_SimpleBlobDetector_Params_set_thresholdStep(pyopencv_SimpleBlobDetector_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the thresholdStep attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.thresholdStep) ? 0 : -1;
}


static PyGetSetDef pyopencv_SimpleBlobDetector_Params_getseters[] =
{
    {(char*)"blobColor", (getter)pyopencv_SimpleBlobDetector_Params_get_blobColor, (setter)pyopencv_SimpleBlobDetector_Params_set_blobColor, (char*)"blobColor", NULL},
    {(char*)"filterByArea", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByArea, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByArea, (char*)"filterByArea", NULL},
    {(char*)"filterByCircularity", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByCircularity, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByCircularity, (char*)"filterByCircularity", NULL},
    {(char*)"filterByColor", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByColor, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByColor, (char*)"filterByColor", NULL},
    {(char*)"filterByConvexity", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByConvexity, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByConvexity, (char*)"filterByConvexity", NULL},
    {(char*)"filterByInertia", (getter)pyopencv_SimpleBlobDetector_Params_get_filterByInertia, (setter)pyopencv_SimpleBlobDetector_Params_set_filterByInertia, (char*)"filterByInertia", NULL},
    {(char*)"maxArea", (getter)pyopencv_SimpleBlobDetector_Params_get_maxArea, (setter)pyopencv_SimpleBlobDetector_Params_set_maxArea, (char*)"maxArea", NULL},
    {(char*)"maxCircularity", (getter)pyopencv_SimpleBlobDetector_Params_get_maxCircularity, (setter)pyopencv_SimpleBlobDetector_Params_set_maxCircularity, (char*)"maxCircularity", NULL},
    {(char*)"maxConvexity", (getter)pyopencv_SimpleBlobDetector_Params_get_maxConvexity, (setter)pyopencv_SimpleBlobDetector_Params_set_maxConvexity, (char*)"maxConvexity", NULL},
    {(char*)"maxInertiaRatio", (getter)pyopencv_SimpleBlobDetector_Params_get_maxInertiaRatio, (setter)pyopencv_SimpleBlobDetector_Params_set_maxInertiaRatio, (char*)"maxInertiaRatio", NULL},
    {(char*)"maxThreshold", (getter)pyopencv_SimpleBlobDetector_Params_get_maxThreshold, (setter)pyopencv_SimpleBlobDetector_Params_set_maxThreshold, (char*)"maxThreshold", NULL},
    {(char*)"minArea", (getter)pyopencv_SimpleBlobDetector_Params_get_minArea, (setter)pyopencv_SimpleBlobDetector_Params_set_minArea, (char*)"minArea", NULL},
    {(char*)"minCircularity", (getter)pyopencv_SimpleBlobDetector_Params_get_minCircularity, (setter)pyopencv_SimpleBlobDetector_Params_set_minCircularity, (char*)"minCircularity", NULL},
    {(char*)"minConvexity", (getter)pyopencv_SimpleBlobDetector_Params_get_minConvexity, (setter)pyopencv_SimpleBlobDetector_Params_set_minConvexity, (char*)"minConvexity", NULL},
    {(char*)"minDistBetweenBlobs", (getter)pyopencv_SimpleBlobDetector_Params_get_minDistBetweenBlobs, (setter)pyopencv_SimpleBlobDetector_Params_set_minDistBetweenBlobs, (char*)"minDistBetweenBlobs", NULL},
    {(char*)"minInertiaRatio", (getter)pyopencv_SimpleBlobDetector_Params_get_minInertiaRatio, (setter)pyopencv_SimpleBlobDetector_Params_set_minInertiaRatio, (char*)"minInertiaRatio", NULL},
    {(char*)"minRepeatability", (getter)pyopencv_SimpleBlobDetector_Params_get_minRepeatability, (setter)pyopencv_SimpleBlobDetector_Params_set_minRepeatability, (char*)"minRepeatability", NULL},
    {(char*)"minThreshold", (getter)pyopencv_SimpleBlobDetector_Params_get_minThreshold, (setter)pyopencv_SimpleBlobDetector_Params_set_minThreshold, (char*)"minThreshold", NULL},
    {(char*)"thresholdStep", (getter)pyopencv_SimpleBlobDetector_Params_get_thresholdStep, (setter)pyopencv_SimpleBlobDetector_Params_set_thresholdStep, (char*)"thresholdStep", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_SimpleBlobDetector_Params_SimpleBlobDetector_Params(pyopencv_SimpleBlobDetector_Params_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::SimpleBlobDetector::Params());
        return 0;
    }

    return -1;
}



static PyMethodDef pyopencv_SimpleBlobDetector_Params_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_SimpleBlobDetector_Params_specials(void)
{
    pyopencv_SimpleBlobDetector_Params_Type.tp_base = NULL;
    pyopencv_SimpleBlobDetector_Params_Type.tp_dealloc = pyopencv_SimpleBlobDetector_Params_dealloc;
    pyopencv_SimpleBlobDetector_Params_Type.tp_repr = pyopencv_SimpleBlobDetector_Params_repr;
    pyopencv_SimpleBlobDetector_Params_Type.tp_getset = pyopencv_SimpleBlobDetector_Params_getseters;
    pyopencv_SimpleBlobDetector_Params_Type.tp_init = (initproc)pyopencv_cv_SimpleBlobDetector_Params_SimpleBlobDetector_Params;
    pyopencv_SimpleBlobDetector_Params_Type.tp_methods = pyopencv_SimpleBlobDetector_Params_methods;
}

static PyObject* pyopencv_KAZE_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<KAZE %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_KAZE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_KAZE_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    bool extended=false;
    bool upright=false;
    float threshold=0.001f;
    int nOctaves=4;
    int nOctaveLayers=4;
    int diffusivity=KAZE::DIFF_PM_G2;
    Ptr<KAZE> retval;

    const char* keywords[] = { "extended", "upright", "threshold", "nOctaves", "nOctaveLayers", "diffusivity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|bbfiii:KAZE.create", (char**)keywords, &extended, &upright, &threshold, &nOctaves, &nOctaveLayers, &diffusivity) )
    {
        ERRWRAP2(retval = cv::KAZE::create(extended, upright, threshold, nOctaves, nOctaveLayers, diffusivity));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getDiffusivity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDiffusivity());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getExtended(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getExtended());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getNOctaveLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNOctaveLayers());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getNOctaves(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNOctaves());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_getUpright(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUpright());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_setDiffusivity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    int diff=0;

    const char* keywords[] = { "diff", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:KAZE.setDiffusivity", (char**)keywords, &diff) )
    {
        ERRWRAP2(_self_->setDiffusivity(diff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_setExtended(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    bool extended=0;

    const char* keywords[] = { "extended", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:KAZE.setExtended", (char**)keywords, &extended) )
    {
        ERRWRAP2(_self_->setExtended(extended));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_setNOctaveLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    int octaveLayers=0;

    const char* keywords[] = { "octaveLayers", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:KAZE.setNOctaveLayers", (char**)keywords, &octaveLayers) )
    {
        ERRWRAP2(_self_->setNOctaveLayers(octaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_setNOctaves(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    int octaves=0;

    const char* keywords[] = { "octaves", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:KAZE.setNOctaves", (char**)keywords, &octaves) )
    {
        ERRWRAP2(_self_->setNOctaves(octaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:KAZE.setThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_KAZE_setUpright(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::KAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_KAZE_Type))
        _self_ = dynamic_cast<cv::KAZE*>(((pyopencv_KAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'KAZE' or its derivative)");
    bool upright=0;

    const char* keywords[] = { "upright", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:KAZE.setUpright", (char**)keywords, &upright) )
    {
        ERRWRAP2(_self_->setUpright(upright));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_KAZE_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_create_cls, METH_CLASS), "create([, extended[, upright[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]) -> retval\n.   @brief The KAZE constructor\n.   \n.   @param extended Set to enable extraction of extended (128-byte) descriptor.\n.   @param upright Set to enable use of upright descriptors (non rotation-invariant).\n.   @param threshold Detector response threshold to accept point\n.   @param nOctaves Maximum octave evolution of the image\n.   @param nOctaveLayers Default number of sublevels per scale level\n.   @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n.   DIFF_CHARBONNIER"},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDiffusivity", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getDiffusivity, 0), "getDiffusivity() -> retval\n."},
    {"getExtended", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getExtended, 0), "getExtended() -> retval\n."},
    {"getNOctaveLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getNOctaves", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getThreshold, 0), "getThreshold() -> retval\n."},
    {"getUpright", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_getUpright, 0), "getUpright() -> retval\n."},
    {"setDiffusivity", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_setDiffusivity, 0), "setDiffusivity(diff) -> None\n."},
    {"setExtended", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_setExtended, 0), "setExtended(extended) -> None\n."},
    {"setNOctaveLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_setNOctaveLayers, 0), "setNOctaveLayers(octaveLayers) -> None\n."},
    {"setNOctaves", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_setNOctaves, 0), "setNOctaves(octaves) -> None\n."},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_setThreshold, 0), "setThreshold(threshold) -> None\n."},
    {"setUpright", CV_PY_FN_WITH_KW_(pyopencv_cv_KAZE_setUpright, 0), "setUpright(upright) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_KAZE_specials(void)
{
    pyopencv_KAZE_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_KAZE_Type.tp_dealloc = pyopencv_KAZE_dealloc;
    pyopencv_KAZE_Type.tp_repr = pyopencv_KAZE_repr;
    pyopencv_KAZE_Type.tp_getset = pyopencv_KAZE_getseters;
    pyopencv_KAZE_Type.tp_init = (initproc)0;
    pyopencv_KAZE_Type.tp_methods = pyopencv_KAZE_methods;
}

static PyObject* pyopencv_AKAZE_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<AKAZE %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_AKAZE_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_AKAZE_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int descriptor_type=AKAZE::DESCRIPTOR_MLDB;
    int descriptor_size=0;
    int descriptor_channels=3;
    float threshold=0.001f;
    int nOctaves=4;
    int nOctaveLayers=4;
    int diffusivity=KAZE::DIFF_PM_G2;
    Ptr<AKAZE> retval;

    const char* keywords[] = { "descriptor_type", "descriptor_size", "descriptor_channels", "threshold", "nOctaves", "nOctaveLayers", "diffusivity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiifiii:AKAZE.create", (char**)keywords, &descriptor_type, &descriptor_size, &descriptor_channels, &threshold, &nOctaves, &nOctaveLayers, &diffusivity) )
    {
        ERRWRAP2(retval = cv::AKAZE::create(descriptor_type, descriptor_size, descriptor_channels, threshold, nOctaves, nOctaveLayers, diffusivity));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getDefaultName(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultName());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getDescriptorChannels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDescriptorChannels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getDescriptorSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDescriptorSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getDescriptorType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDescriptorType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getDiffusivity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDiffusivity());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getNOctaveLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNOctaveLayers());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getNOctaves(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNOctaves());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setDescriptorChannels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int dch=0;

    const char* keywords[] = { "dch", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AKAZE.setDescriptorChannels", (char**)keywords, &dch) )
    {
        ERRWRAP2(_self_->setDescriptorChannels(dch));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setDescriptorSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int dsize=0;

    const char* keywords[] = { "dsize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AKAZE.setDescriptorSize", (char**)keywords, &dsize) )
    {
        ERRWRAP2(_self_->setDescriptorSize(dsize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setDescriptorType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int dtype=0;

    const char* keywords[] = { "dtype", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AKAZE.setDescriptorType", (char**)keywords, &dtype) )
    {
        ERRWRAP2(_self_->setDescriptorType(dtype));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setDiffusivity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int diff=0;

    const char* keywords[] = { "diff", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AKAZE.setDiffusivity", (char**)keywords, &diff) )
    {
        ERRWRAP2(_self_->setDiffusivity(diff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setNOctaveLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int octaveLayers=0;

    const char* keywords[] = { "octaveLayers", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AKAZE.setNOctaveLayers", (char**)keywords, &octaveLayers) )
    {
        ERRWRAP2(_self_->setNOctaveLayers(octaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setNOctaves(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    int octaves=0;

    const char* keywords[] = { "octaves", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:AKAZE.setNOctaves", (char**)keywords, &octaves) )
    {
        ERRWRAP2(_self_->setNOctaves(octaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_AKAZE_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::AKAZE* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_AKAZE_Type))
        _self_ = dynamic_cast<cv::AKAZE*>(((pyopencv_AKAZE_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'AKAZE' or its derivative)");
    double threshold=0;

    const char* keywords[] = { "threshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:AKAZE.setThreshold", (char**)keywords, &threshold) )
    {
        ERRWRAP2(_self_->setThreshold(threshold));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_AKAZE_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_create_cls, METH_CLASS), "create([, descriptor_type[, descriptor_size[, descriptor_channels[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]]) -> retval\n.   @brief The AKAZE constructor\n.   \n.   @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,\n.   DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.\n.   @param descriptor_size Size of the descriptor in bits. 0 -\\> Full size\n.   @param descriptor_channels Number of channels in the descriptor (1, 2, 3)\n.   @param threshold Detector response threshold to accept point\n.   @param nOctaves Maximum octave evolution of the image\n.   @param nOctaveLayers Default number of sublevels per scale level\n.   @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n.   DIFF_CHARBONNIER"},
    {"getDefaultName", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getDefaultName, 0), "getDefaultName() -> retval\n."},
    {"getDescriptorChannels", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getDescriptorChannels, 0), "getDescriptorChannels() -> retval\n."},
    {"getDescriptorSize", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getDescriptorSize, 0), "getDescriptorSize() -> retval\n."},
    {"getDescriptorType", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getDescriptorType, 0), "getDescriptorType() -> retval\n."},
    {"getDiffusivity", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getDiffusivity, 0), "getDiffusivity() -> retval\n."},
    {"getNOctaveLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getNOctaves", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_getThreshold, 0), "getThreshold() -> retval\n."},
    {"setDescriptorChannels", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setDescriptorChannels, 0), "setDescriptorChannels(dch) -> None\n."},
    {"setDescriptorSize", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setDescriptorSize, 0), "setDescriptorSize(dsize) -> None\n."},
    {"setDescriptorType", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setDescriptorType, 0), "setDescriptorType(dtype) -> None\n."},
    {"setDiffusivity", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setDiffusivity, 0), "setDiffusivity(diff) -> None\n."},
    {"setNOctaveLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setNOctaveLayers, 0), "setNOctaveLayers(octaveLayers) -> None\n."},
    {"setNOctaves", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setNOctaves, 0), "setNOctaves(octaves) -> None\n."},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_AKAZE_setThreshold, 0), "setThreshold(threshold) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_AKAZE_specials(void)
{
    pyopencv_AKAZE_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_AKAZE_Type.tp_dealloc = pyopencv_AKAZE_dealloc;
    pyopencv_AKAZE_Type.tp_repr = pyopencv_AKAZE_repr;
    pyopencv_AKAZE_Type.tp_getset = pyopencv_AKAZE_getseters;
    pyopencv_AKAZE_Type.tp_init = (initproc)0;
    pyopencv_AKAZE_Type.tp_methods = pyopencv_AKAZE_methods;
}

static PyObject* pyopencv_DescriptorMatcher_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<DescriptorMatcher %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_DescriptorMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_DescriptorMatcher_add(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    {
    PyObject* pyobj_descriptors = NULL;
    vector_Mat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DescriptorMatcher.add", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(_self_->add(descriptors));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors = NULL;
    vector_Mat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DescriptorMatcher.add", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(_self_->add(descriptors));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_clear(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_clone(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    bool emptyTrainData=false;
    Ptr<DescriptorMatcher> retval;

    const char* keywords[] = { "emptyTrainData", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|b:DescriptorMatcher.clone", (char**)keywords, &emptyTrainData) )
    {
        ERRWRAP2(retval = _self_->clone(emptyTrainData));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    {
    PyObject* pyobj_descriptorMatcherType = NULL;
    String descriptorMatcherType;
    Ptr<DescriptorMatcher> retval;

    const char* keywords[] = { "descriptorMatcherType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DescriptorMatcher.create", (char**)keywords, &pyobj_descriptorMatcherType) &&
        pyopencv_to(pyobj_descriptorMatcherType, descriptorMatcherType, ArgInfo("descriptorMatcherType", 0)) )
    {
        ERRWRAP2(retval = cv::DescriptorMatcher::create(descriptorMatcherType));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int matcherType=0;
    Ptr<DescriptorMatcher> retval;

    const char* keywords[] = { "matcherType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:DescriptorMatcher.create", (char**)keywords, &matcherType) )
    {
        ERRWRAP2(retval = cv::DescriptorMatcher::create(matcherType));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_empty(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->empty());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_getTrainDescriptors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    std::vector<Mat> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTrainDescriptors());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_isMaskSupported(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->isMaskSupported());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_knnMatch(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    {
    PyObject* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    PyObject* pyobj_trainDescriptors = NULL;
    Mat trainDescriptors;
    vector_vector_DMatch matches;
    int k=0;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "k", "mask", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOi|Ob:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &k, &pyobj_mask, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->knnMatch(queryDescriptors, trainDescriptors, matches, k, mask, compactResult));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    PyObject* pyobj_trainDescriptors = NULL;
    UMat trainDescriptors;
    vector_vector_DMatch matches;
    int k=0;
    PyObject* pyobj_mask = NULL;
    UMat mask;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "k", "mask", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOi|Ob:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &k, &pyobj_mask, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->knnMatch(queryDescriptors, trainDescriptors, matches, k, mask, compactResult));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    vector_vector_DMatch matches;
    int k=0;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "k", "masks", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|Ob:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &k, &pyobj_masks, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->knnMatch(queryDescriptors, matches, k, masks, compactResult));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    vector_vector_DMatch matches;
    int k=0;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "k", "masks", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi|Ob:DescriptorMatcher.knnMatch", (char**)keywords, &pyobj_queryDescriptors, &k, &pyobj_masks, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->knnMatch(queryDescriptors, matches, k, masks, compactResult));
        return pyopencv_from(matches);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_match(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    {
    PyObject* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    PyObject* pyobj_trainDescriptors = NULL;
    Mat trainDescriptors;
    vector_DMatch matches;
    PyObject* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_mask) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->match(queryDescriptors, trainDescriptors, matches, mask));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    PyObject* pyobj_trainDescriptors = NULL;
    UMat trainDescriptors;
    vector_DMatch matches;
    PyObject* pyobj_mask = NULL;
    UMat mask;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &pyobj_mask) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->match(queryDescriptors, trainDescriptors, matches, mask));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    vector_DMatch matches;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;

    const char* keywords[] = { "queryDescriptors", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_masks) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->match(queryDescriptors, matches, masks));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    vector_DMatch matches;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;

    const char* keywords[] = { "queryDescriptors", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:DescriptorMatcher.match", (char**)keywords, &pyobj_queryDescriptors, &pyobj_masks) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->match(queryDescriptors, matches, masks));
        return pyopencv_from(matches);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_radiusMatch(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    {
    PyObject* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    PyObject* pyobj_trainDescriptors = NULL;
    Mat trainDescriptors;
    vector_vector_DMatch matches;
    float maxDistance=0.f;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "maxDistance", "mask", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOf|Ob:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &maxDistance, &pyobj_mask, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->radiusMatch(queryDescriptors, trainDescriptors, matches, maxDistance, mask, compactResult));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    PyObject* pyobj_trainDescriptors = NULL;
    UMat trainDescriptors;
    vector_vector_DMatch matches;
    float maxDistance=0.f;
    PyObject* pyobj_mask = NULL;
    UMat mask;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "trainDescriptors", "maxDistance", "mask", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOf|Ob:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &pyobj_trainDescriptors, &maxDistance, &pyobj_mask, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_trainDescriptors, trainDescriptors, ArgInfo("trainDescriptors", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->radiusMatch(queryDescriptors, trainDescriptors, matches, maxDistance, mask, compactResult));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    Mat queryDescriptors;
    vector_vector_DMatch matches;
    float maxDistance=0.f;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "maxDistance", "masks", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Of|Ob:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &maxDistance, &pyobj_masks, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->radiusMatch(queryDescriptors, matches, maxDistance, masks, compactResult));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_queryDescriptors = NULL;
    UMat queryDescriptors;
    vector_vector_DMatch matches;
    float maxDistance=0.f;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks;
    bool compactResult=false;

    const char* keywords[] = { "queryDescriptors", "maxDistance", "masks", "compactResult", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Of|Ob:DescriptorMatcher.radiusMatch", (char**)keywords, &pyobj_queryDescriptors, &maxDistance, &pyobj_masks, &compactResult) &&
        pyopencv_to(pyobj_queryDescriptors, queryDescriptors, ArgInfo("queryDescriptors", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->radiusMatch(queryDescriptors, matches, maxDistance, masks, compactResult));
        return pyopencv_from(matches);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    {
    PyObject* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DescriptorMatcher.read", (char**)keywords, &pyobj_fileName) &&
        pyopencv_to(pyobj_fileName, fileName, ArgInfo("fileName", 0)) )
    {
        ERRWRAP2(_self_->read(fileName));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_arg1 = NULL;
    FileNode arg1;

    const char* keywords[] = { "arg1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DescriptorMatcher.read", (char**)keywords, &pyobj_arg1) &&
        pyopencv_to(pyobj_arg1, arg1, ArgInfo("arg1", 0)) )
    {
        ERRWRAP2(_self_->read(arg1));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_train(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->train());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_DescriptorMatcher_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::DescriptorMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_DescriptorMatcher_Type))
        _self_ = dynamic_cast<cv::DescriptorMatcher*>(((pyopencv_DescriptorMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'DescriptorMatcher' or its derivative)");
    {
    PyObject* pyobj_fileName = NULL;
    String fileName;

    const char* keywords[] = { "fileName", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:DescriptorMatcher.write", (char**)keywords, &pyobj_fileName) &&
        pyopencv_to(pyobj_fileName, fileName, ArgInfo("fileName", 0)) )
    {
        ERRWRAP2(_self_->write(fileName));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_fs = NULL;
    Ptr<FileStorage> fs;
    PyObject* pyobj_name = NULL;
    String name;

    const char* keywords[] = { "fs", "name", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:DescriptorMatcher.write", (char**)keywords, &pyobj_fs, &pyobj_name) &&
        pyopencv_to(pyobj_fs, fs, ArgInfo("fs", 0)) &&
        pyopencv_to(pyobj_name, name, ArgInfo("name", 0)) )
    {
        ERRWRAP2(_self_->write(fs, name));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_DescriptorMatcher_methods[] =
{
    {"add", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_add, 0), "add(descriptors) -> None\n.   @brief Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor\n.   collection.\n.   \n.   If the collection is not empty, the new descriptors are added to existing train descriptors.\n.   \n.   @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n.   train image."},
    {"clear", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_clear, 0), "clear() -> None\n.   @brief Clears the train descriptor collections."},
    {"clone", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_clone, 0), "clone([, emptyTrainData]) -> retval\n.   @brief Clones the matcher.\n.   \n.   @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n.   that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n.   object copy with the current parameters but with empty train data."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_create_cls, METH_CLASS), "create(descriptorMatcherType) -> retval\n.   @brief Creates a descriptor matcher of a given type with the default parameters (using default\n.   constructor).\n.   \n.   @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are\n.   supported:\n.   -   `BruteForce` (it uses L2 )\n.   -   `BruteForce-L1`\n.   -   `BruteForce-Hamming`\n.   -   `BruteForce-Hamming(2)`\n.   -   `FlannBased`\n\n\n\ncreate(matcherType) -> retval\n."},
    {"empty", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_empty, 0), "empty() -> retval\n.   @brief Returns true if there are no train descriptors in the both collections."},
    {"getTrainDescriptors", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_getTrainDescriptors, 0), "getTrainDescriptors() -> retval\n.   @brief Returns a constant link to the train descriptor collection trainDescCollection ."},
    {"isMaskSupported", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_isMaskSupported, 0), "isMaskSupported() -> retval\n.   @brief Returns true if the descriptor matcher supports masking permissible matches."},
    {"knnMatch", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_knnMatch, 0), "knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) -> matches\n.   @brief Finds the k best matches for each descriptor from a query set.\n.   \n.   @param queryDescriptors Query set of descriptors.\n.   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n.   collection stored in the class object.\n.   @param mask Mask specifying permissible matches between an input query and train matrices of\n.   descriptors.\n.   @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n.   @param k Count of best matches found per each query descriptor or less if a query descriptor has\n.   less than k possible matches in total.\n.   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.   false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.   the matches vector does not contain matches for fully masked-out query descriptors.\n.   \n.   These extended variants of DescriptorMatcher::match methods find several best matches for each query\n.   descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match\n.   for the details about query and train descriptors.\n\n\n\nknnMatch(queryDescriptors, k[, masks[, compactResult]]) -> matches\n.   @overload\n.   @param queryDescriptors Query set of descriptors.\n.   @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n.   @param k Count of best matches found per each query descriptor or less if a query descriptor has\n.   less than k possible matches in total.\n.   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n.   descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n.   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.   false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.   the matches vector does not contain matches for fully masked-out query descriptors."},
    {"match", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_match, 0), "match(queryDescriptors, trainDescriptors[, mask]) -> matches\n.   @brief Finds the best match for each descriptor from a query set.\n.   \n.   @param queryDescriptors Query set of descriptors.\n.   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n.   collection stored in the class object.\n.   @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n.   descriptor. So, matches size may be smaller than the query descriptors count.\n.   @param mask Mask specifying permissible matches between an input query and train matrices of\n.   descriptors.\n.   \n.   In the first variant of this method, the train descriptors are passed as an input argument. In the\n.   second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is\n.   used. Optional mask (or masks) can be passed to specify which query and training descriptors can be\n.   matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n.   mask.at\\<uchar\\>(i,j) is non-zero.\n\n\n\nmatch(queryDescriptors[, masks]) -> matches\n.   @overload\n.   @param queryDescriptors Query set of descriptors.\n.   @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n.   descriptor. So, matches size may be smaller than the query descriptors count.\n.   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n.   descriptors and stored train descriptors from the i-th image trainDescCollection[i]."},
    {"radiusMatch", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_radiusMatch, 0), "radiusMatch(queryDescriptors, trainDescriptors, maxDistance[, mask[, compactResult]]) -> matches\n.   @brief For each query descriptor, finds the training descriptors not farther than the specified distance.\n.   \n.   @param queryDescriptors Query set of descriptors.\n.   @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n.   collection stored in the class object.\n.   @param matches Found matches.\n.   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.   false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.   the matches vector does not contain matches for fully masked-out query descriptors.\n.   @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n.   metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n.   in Pixels)!\n.   @param mask Mask specifying permissible matches between an input query and train matrices of\n.   descriptors.\n.   \n.   For each query descriptor, the methods find such training descriptors that the distance between the\n.   query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are\n.   returned in the distance increasing order.\n\n\n\nradiusMatch(queryDescriptors, maxDistance[, masks[, compactResult]]) -> matches\n.   @overload\n.   @param queryDescriptors Query set of descriptors.\n.   @param matches Found matches.\n.   @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n.   metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n.   in Pixels)!\n.   @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n.   descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n.   @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n.   false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n.   the matches vector does not contain matches for fully masked-out query descriptors."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_read, 0), "read(fileName) -> None\n.   \n\n\n\nread(arg1) -> None\n."},
    {"train", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_train, 0), "train() -> None\n.   @brief Trains a descriptor matcher\n.   \n.   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n.   train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher)\n.   have an empty implementation of this method. Other matchers really train their inner structures (for\n.   example, FlannBasedMatcher trains flann::Index )."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_DescriptorMatcher_write, 0), "write(fileName) -> None\n.   \n\n\n\nwrite(fs[, name]) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_DescriptorMatcher_specials(void)
{
    pyopencv_DescriptorMatcher_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_DescriptorMatcher_Type.tp_dealloc = pyopencv_DescriptorMatcher_dealloc;
    pyopencv_DescriptorMatcher_Type.tp_repr = pyopencv_DescriptorMatcher_repr;
    pyopencv_DescriptorMatcher_Type.tp_getset = pyopencv_DescriptorMatcher_getseters;
    pyopencv_DescriptorMatcher_Type.tp_init = (initproc)0;
    pyopencv_DescriptorMatcher_Type.tp_methods = pyopencv_DescriptorMatcher_methods;
}

static PyObject* pyopencv_BFMatcher_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BFMatcher %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BFMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_BFMatcher_BFMatcher(pyopencv_BFMatcher_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int normType=NORM_L2;
    bool crossCheck=false;

    const char* keywords[] = { "normType", "crossCheck", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ib:BFMatcher", (char**)keywords, &normType, &crossCheck) )
    {
        new (&(self->v)) Ptr<cv::BFMatcher>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::BFMatcher(normType, crossCheck)));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_BFMatcher_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int normType=NORM_L2;
    bool crossCheck=false;
    Ptr<BFMatcher> retval;

    const char* keywords[] = { "normType", "crossCheck", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ib:BFMatcher.create", (char**)keywords, &normType, &crossCheck) )
    {
        ERRWRAP2(retval = cv::BFMatcher::create(normType, crossCheck));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_BFMatcher_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_BFMatcher_create_cls, METH_CLASS), "create([, normType[, crossCheck]]) -> retval\n.   @brief Brute-force matcher create method.\n.   @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are\n.   preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and\n.   BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor\n.   description).\n.   @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k\n.   nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with\n.   k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the\n.   matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent\n.   pairs. Such technique usually produces best results with minimal number of outliers when there are\n.   enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper."},

    {NULL,          NULL}
};

static void pyopencv_BFMatcher_specials(void)
{
    pyopencv_BFMatcher_Type.tp_base = &pyopencv_DescriptorMatcher_Type;
    pyopencv_BFMatcher_Type.tp_dealloc = pyopencv_BFMatcher_dealloc;
    pyopencv_BFMatcher_Type.tp_repr = pyopencv_BFMatcher_repr;
    pyopencv_BFMatcher_Type.tp_getset = pyopencv_BFMatcher_getseters;
    pyopencv_BFMatcher_Type.tp_init = (initproc)pyopencv_cv_BFMatcher_BFMatcher;
    pyopencv_BFMatcher_Type.tp_methods = pyopencv_BFMatcher_methods;
}

static PyObject* pyopencv_FlannBasedMatcher_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<FlannBasedMatcher %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_FlannBasedMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_FlannBasedMatcher_FlannBasedMatcher(pyopencv_FlannBasedMatcher_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_indexParams = NULL;
    Ptr<flann::IndexParams> indexParams=makePtr<flann::KDTreeIndexParams>();
    PyObject* pyobj_searchParams = NULL;
    Ptr<flann::SearchParams> searchParams=makePtr<flann::SearchParams>();

    const char* keywords[] = { "indexParams", "searchParams", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:FlannBasedMatcher", (char**)keywords, &pyobj_indexParams, &pyobj_searchParams) &&
        pyopencv_to(pyobj_indexParams, indexParams, ArgInfo("indexParams", 0)) &&
        pyopencv_to(pyobj_searchParams, searchParams, ArgInfo("searchParams", 0)) )
    {
        new (&(self->v)) Ptr<cv::FlannBasedMatcher>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::FlannBasedMatcher(indexParams, searchParams)));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_FlannBasedMatcher_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    Ptr<FlannBasedMatcher> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::FlannBasedMatcher::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_FlannBasedMatcher_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_FlannBasedMatcher_create_cls, METH_CLASS), "create() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_FlannBasedMatcher_specials(void)
{
    pyopencv_FlannBasedMatcher_Type.tp_base = &pyopencv_DescriptorMatcher_Type;
    pyopencv_FlannBasedMatcher_Type.tp_dealloc = pyopencv_FlannBasedMatcher_dealloc;
    pyopencv_FlannBasedMatcher_Type.tp_repr = pyopencv_FlannBasedMatcher_repr;
    pyopencv_FlannBasedMatcher_Type.tp_getset = pyopencv_FlannBasedMatcher_getseters;
    pyopencv_FlannBasedMatcher_Type.tp_init = (initproc)pyopencv_cv_FlannBasedMatcher_FlannBasedMatcher;
    pyopencv_FlannBasedMatcher_Type.tp_methods = pyopencv_FlannBasedMatcher_methods;
}

static PyObject* pyopencv_BOWTrainer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BOWTrainer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BOWTrainer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_BOWTrainer_add(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWTrainer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWTrainer_Type))
        _self_ = ((pyopencv_BOWTrainer_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    {
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWTrainer.add", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(_self_->add(descriptors));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWTrainer.add", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(_self_->add(descriptors));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWTrainer_clear(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWTrainer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWTrainer_Type))
        _self_ = ((pyopencv_BOWTrainer_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clear());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWTrainer_cluster(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWTrainer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWTrainer_Type))
        _self_ = ((pyopencv_BOWTrainer_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    {
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->cluster());
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;
    Mat retval;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWTrainer.cluster", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(retval = _self_->cluster(descriptors));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;
    Mat retval;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWTrainer.cluster", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(retval = _self_->cluster(descriptors));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWTrainer_descriptorsCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWTrainer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWTrainer_Type))
        _self_ = ((pyopencv_BOWTrainer_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->descriptorsCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWTrainer_getDescriptors(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWTrainer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWTrainer_Type))
        _self_ = ((pyopencv_BOWTrainer_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWTrainer' or its derivative)");
    std::vector<Mat> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDescriptors());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_BOWTrainer_methods[] =
{
    {"add", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWTrainer_add, 0), "add(descriptors) -> None\n.   @brief Adds descriptors to a training set.\n.   \n.   @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a\n.   descriptor.\n.   \n.   The training set is clustered using clustermethod to construct the vocabulary."},
    {"clear", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWTrainer_clear, 0), "clear() -> None\n."},
    {"cluster", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWTrainer_cluster, 0), "cluster() -> retval\n.   @overload\n\n\n\ncluster(descriptors) -> retval\n.   @brief Clusters train descriptors.\n.   \n.   @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.\n.   Descriptors are not added to the inner train descriptor set.\n.   \n.   The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first\n.   variant of the method, train descriptors stored in the object are clustered. In the second variant,\n.   input descriptors are clustered."},
    {"descriptorsCount", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWTrainer_descriptorsCount, 0), "descriptorsCount() -> retval\n.   @brief Returns the count of all descriptors stored in the training set."},
    {"getDescriptors", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWTrainer_getDescriptors, 0), "getDescriptors() -> retval\n.   @brief Returns a training set of descriptors."},

    {NULL,          NULL}
};

static void pyopencv_BOWTrainer_specials(void)
{
    pyopencv_BOWTrainer_Type.tp_base = NULL;
    pyopencv_BOWTrainer_Type.tp_dealloc = pyopencv_BOWTrainer_dealloc;
    pyopencv_BOWTrainer_Type.tp_repr = pyopencv_BOWTrainer_repr;
    pyopencv_BOWTrainer_Type.tp_getset = pyopencv_BOWTrainer_getseters;
    pyopencv_BOWTrainer_Type.tp_init = (initproc)0;
    pyopencv_BOWTrainer_Type.tp_methods = pyopencv_BOWTrainer_methods;
}

static PyObject* pyopencv_BOWKMeansTrainer_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BOWKMeansTrainer %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BOWKMeansTrainer_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_BOWKMeansTrainer_BOWKMeansTrainer(pyopencv_BOWKMeansTrainer_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int clusterCount=0;
    PyObject* pyobj_termcrit = NULL;
    TermCriteria termcrit;
    int attempts=3;
    int flags=KMEANS_PP_CENTERS;

    const char* keywords[] = { "clusterCount", "termcrit", "attempts", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|Oii:BOWKMeansTrainer", (char**)keywords, &clusterCount, &pyobj_termcrit, &attempts, &flags) &&
        pyopencv_to(pyobj_termcrit, termcrit, ArgInfo("termcrit", 0)) )
    {
        new (&(self->v)) Ptr<cv::BOWKMeansTrainer>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::BOWKMeansTrainer(clusterCount, termcrit, attempts, flags)));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_BOWKMeansTrainer_cluster(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWKMeansTrainer* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWKMeansTrainer_Type))
        _self_ = ((pyopencv_BOWKMeansTrainer_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWKMeansTrainer' or its derivative)");
    {
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->cluster());
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;
    Mat retval;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWKMeansTrainer.cluster", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(retval = _self_->cluster(descriptors));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_descriptors = NULL;
    Mat descriptors;
    Mat retval;

    const char* keywords[] = { "descriptors", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWKMeansTrainer.cluster", (char**)keywords, &pyobj_descriptors) &&
        pyopencv_to(pyobj_descriptors, descriptors, ArgInfo("descriptors", 0)) )
    {
        ERRWRAP2(retval = _self_->cluster(descriptors));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_BOWKMeansTrainer_methods[] =
{
    {"cluster", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWKMeansTrainer_cluster, 0), "cluster() -> retval\n.   \n\n\n\ncluster(descriptors) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_BOWKMeansTrainer_specials(void)
{
    pyopencv_BOWKMeansTrainer_Type.tp_base = &pyopencv_BOWTrainer_Type;
    pyopencv_BOWKMeansTrainer_Type.tp_dealloc = pyopencv_BOWKMeansTrainer_dealloc;
    pyopencv_BOWKMeansTrainer_Type.tp_repr = pyopencv_BOWKMeansTrainer_repr;
    pyopencv_BOWKMeansTrainer_Type.tp_getset = pyopencv_BOWKMeansTrainer_getseters;
    pyopencv_BOWKMeansTrainer_Type.tp_init = (initproc)pyopencv_cv_BOWKMeansTrainer_BOWKMeansTrainer;
    pyopencv_BOWKMeansTrainer_Type.tp_methods = pyopencv_BOWKMeansTrainer_methods;
}

static PyObject* pyopencv_BOWImgDescriptorExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<BOWImgDescriptorExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_BOWImgDescriptorExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_BOWImgDescriptorExtractor_BOWImgDescriptorExtractor(pyopencv_BOWImgDescriptorExtractor_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    PyObject* pyobj_dextractor = NULL;
    Ptr<DescriptorExtractor> dextractor;
    PyObject* pyobj_dmatcher = NULL;
    Ptr<DescriptorMatcher> dmatcher;

    const char* keywords[] = { "dextractor", "dmatcher", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:BOWImgDescriptorExtractor", (char**)keywords, &pyobj_dextractor, &pyobj_dmatcher) &&
        pyopencv_to(pyobj_dextractor, dextractor, ArgInfo("dextractor", 0)) &&
        pyopencv_to(pyobj_dmatcher, dmatcher, ArgInfo("dmatcher", 0)) )
    {
        new (&(self->v)) Ptr<cv::BOWImgDescriptorExtractor>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::BOWImgDescriptorExtractor(dextractor, dmatcher)));
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_BOWImgDescriptorExtractor_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWImgDescriptorExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWImgDescriptorExtractor_Type))
        _self_ = ((pyopencv_BOWImgDescriptorExtractor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    PyObject* pyobj_imgDescriptor = NULL;
    Mat imgDescriptor;

    const char* keywords[] = { "image", "keypoints", "imgDescriptor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:BOWImgDescriptorExtractor.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_imgDescriptor) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 0)) &&
        pyopencv_to(pyobj_imgDescriptor, imgDescriptor, ArgInfo("imgDescriptor", 1)) )
    {
        ERRWRAP2(_self_->compute2(image, keypoints, imgDescriptor));
        return pyopencv_from(imgDescriptor);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_keypoints = NULL;
    vector_KeyPoint keypoints;
    PyObject* pyobj_imgDescriptor = NULL;
    Mat imgDescriptor;

    const char* keywords[] = { "image", "keypoints", "imgDescriptor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:BOWImgDescriptorExtractor.compute", (char**)keywords, &pyobj_image, &pyobj_keypoints, &pyobj_imgDescriptor) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_keypoints, keypoints, ArgInfo("keypoints", 0)) &&
        pyopencv_to(pyobj_imgDescriptor, imgDescriptor, ArgInfo("imgDescriptor", 1)) )
    {
        ERRWRAP2(_self_->compute2(image, keypoints, imgDescriptor));
        return pyopencv_from(imgDescriptor);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWImgDescriptorExtractor_descriptorSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWImgDescriptorExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWImgDescriptorExtractor_Type))
        _self_ = ((pyopencv_BOWImgDescriptorExtractor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->descriptorSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWImgDescriptorExtractor_descriptorType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWImgDescriptorExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWImgDescriptorExtractor_Type))
        _self_ = ((pyopencv_BOWImgDescriptorExtractor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->descriptorType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWImgDescriptorExtractor_getVocabulary(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWImgDescriptorExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWImgDescriptorExtractor_Type))
        _self_ = ((pyopencv_BOWImgDescriptorExtractor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVocabulary());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_BOWImgDescriptorExtractor_setVocabulary(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::BOWImgDescriptorExtractor* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_BOWImgDescriptorExtractor_Type))
        _self_ = ((pyopencv_BOWImgDescriptorExtractor_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'BOWImgDescriptorExtractor' or its derivative)");
    {
    PyObject* pyobj_vocabulary = NULL;
    Mat vocabulary;

    const char* keywords[] = { "vocabulary", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWImgDescriptorExtractor.setVocabulary", (char**)keywords, &pyobj_vocabulary) &&
        pyopencv_to(pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) )
    {
        ERRWRAP2(_self_->setVocabulary(vocabulary));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_vocabulary = NULL;
    Mat vocabulary;

    const char* keywords[] = { "vocabulary", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:BOWImgDescriptorExtractor.setVocabulary", (char**)keywords, &pyobj_vocabulary) &&
        pyopencv_to(pyobj_vocabulary, vocabulary, ArgInfo("vocabulary", 0)) )
    {
        ERRWRAP2(_self_->setVocabulary(vocabulary));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_BOWImgDescriptorExtractor_methods[] =
{
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_compute, 0), "compute(image, keypoints[, imgDescriptor]) -> imgDescriptor\n.   @overload\n.   @param keypointDescriptors Computed descriptors to match with vocabulary.\n.   @param imgDescriptor Computed output image descriptor.\n.   @param pointIdxsOfClusters Indices of keypoints that belong to the cluster. This means that\n.   pointIdxsOfClusters[i] are keypoint indices that belong to the i -th cluster (word of vocabulary)\n.   returned if it is non-zero."},
    {"descriptorSize", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_descriptorSize, 0), "descriptorSize() -> retval\n.   @brief Returns an image descriptor size if the vocabulary is set. Otherwise, it returns 0."},
    {"descriptorType", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_descriptorType, 0), "descriptorType() -> retval\n.   @brief Returns an image descriptor type."},
    {"getVocabulary", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_getVocabulary, 0), "getVocabulary() -> retval\n.   @brief Returns the set vocabulary."},
    {"setVocabulary", CV_PY_FN_WITH_KW_(pyopencv_cv_BOWImgDescriptorExtractor_setVocabulary, 0), "setVocabulary(vocabulary) -> None\n.   @brief Sets a visual vocabulary.\n.   \n.   @param vocabulary Vocabulary (can be trained using the inheritor of BOWTrainer ). Each row of the\n.   vocabulary is a visual word (cluster center)."},

    {NULL,          NULL}
};

static void pyopencv_BOWImgDescriptorExtractor_specials(void)
{
    pyopencv_BOWImgDescriptorExtractor_Type.tp_base = NULL;
    pyopencv_BOWImgDescriptorExtractor_Type.tp_dealloc = pyopencv_BOWImgDescriptorExtractor_dealloc;
    pyopencv_BOWImgDescriptorExtractor_Type.tp_repr = pyopencv_BOWImgDescriptorExtractor_repr;
    pyopencv_BOWImgDescriptorExtractor_Type.tp_getset = pyopencv_BOWImgDescriptorExtractor_getseters;
    pyopencv_BOWImgDescriptorExtractor_Type.tp_init = (initproc)pyopencv_cv_BOWImgDescriptorExtractor_BOWImgDescriptorExtractor;
    pyopencv_BOWImgDescriptorExtractor_Type.tp_methods = pyopencv_BOWImgDescriptorExtractor_methods;
}

static PyObject* pyopencv_line_descriptor_KeyLine_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<line_descriptor_KeyLine %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_line_descriptor_KeyLine_get_angle(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.angle);
}

static int pyopencv_line_descriptor_KeyLine_set_angle(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the angle attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.angle) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_class_id(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.class_id);
}

static int pyopencv_line_descriptor_KeyLine_set_class_id(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the class_id attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.class_id) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_ePointInOctaveX(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.ePointInOctaveX);
}

static int pyopencv_line_descriptor_KeyLine_set_ePointInOctaveX(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the ePointInOctaveX attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.ePointInOctaveX) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_ePointInOctaveY(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.ePointInOctaveY);
}

static int pyopencv_line_descriptor_KeyLine_set_ePointInOctaveY(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the ePointInOctaveY attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.ePointInOctaveY) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_endPointX(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.endPointX);
}

static int pyopencv_line_descriptor_KeyLine_set_endPointX(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the endPointX attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.endPointX) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_endPointY(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.endPointY);
}

static int pyopencv_line_descriptor_KeyLine_set_endPointY(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the endPointY attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.endPointY) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_lineLength(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.lineLength);
}

static int pyopencv_line_descriptor_KeyLine_set_lineLength(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the lineLength attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.lineLength) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_numOfPixels(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.numOfPixels);
}

static int pyopencv_line_descriptor_KeyLine_set_numOfPixels(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the numOfPixels attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.numOfPixels) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_octave(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.octave);
}

static int pyopencv_line_descriptor_KeyLine_set_octave(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the octave attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.octave) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_pt(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.pt);
}

static int pyopencv_line_descriptor_KeyLine_set_pt(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the pt attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.pt) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_response(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.response);
}

static int pyopencv_line_descriptor_KeyLine_set_response(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the response attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.response) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_sPointInOctaveX(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.sPointInOctaveX);
}

static int pyopencv_line_descriptor_KeyLine_set_sPointInOctaveX(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the sPointInOctaveX attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.sPointInOctaveX) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_sPointInOctaveY(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.sPointInOctaveY);
}

static int pyopencv_line_descriptor_KeyLine_set_sPointInOctaveY(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the sPointInOctaveY attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.sPointInOctaveY) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_size(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.size);
}

static int pyopencv_line_descriptor_KeyLine_set_size(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the size attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.size) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_startPointX(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.startPointX);
}

static int pyopencv_line_descriptor_KeyLine_set_startPointX(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the startPointX attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.startPointX) ? 0 : -1;
}

static PyObject* pyopencv_line_descriptor_KeyLine_get_startPointY(pyopencv_line_descriptor_KeyLine_t* p, void *closure)
{
    return pyopencv_from(p->v.startPointY);
}

static int pyopencv_line_descriptor_KeyLine_set_startPointY(pyopencv_line_descriptor_KeyLine_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the startPointY attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.startPointY) ? 0 : -1;
}


static PyGetSetDef pyopencv_line_descriptor_KeyLine_getseters[] =
{
    {(char*)"angle", (getter)pyopencv_line_descriptor_KeyLine_get_angle, (setter)pyopencv_line_descriptor_KeyLine_set_angle, (char*)"angle", NULL},
    {(char*)"class_id", (getter)pyopencv_line_descriptor_KeyLine_get_class_id, (setter)pyopencv_line_descriptor_KeyLine_set_class_id, (char*)"class_id", NULL},
    {(char*)"ePointInOctaveX", (getter)pyopencv_line_descriptor_KeyLine_get_ePointInOctaveX, (setter)pyopencv_line_descriptor_KeyLine_set_ePointInOctaveX, (char*)"ePointInOctaveX", NULL},
    {(char*)"ePointInOctaveY", (getter)pyopencv_line_descriptor_KeyLine_get_ePointInOctaveY, (setter)pyopencv_line_descriptor_KeyLine_set_ePointInOctaveY, (char*)"ePointInOctaveY", NULL},
    {(char*)"endPointX", (getter)pyopencv_line_descriptor_KeyLine_get_endPointX, (setter)pyopencv_line_descriptor_KeyLine_set_endPointX, (char*)"endPointX", NULL},
    {(char*)"endPointY", (getter)pyopencv_line_descriptor_KeyLine_get_endPointY, (setter)pyopencv_line_descriptor_KeyLine_set_endPointY, (char*)"endPointY", NULL},
    {(char*)"lineLength", (getter)pyopencv_line_descriptor_KeyLine_get_lineLength, (setter)pyopencv_line_descriptor_KeyLine_set_lineLength, (char*)"lineLength", NULL},
    {(char*)"numOfPixels", (getter)pyopencv_line_descriptor_KeyLine_get_numOfPixels, (setter)pyopencv_line_descriptor_KeyLine_set_numOfPixels, (char*)"numOfPixels", NULL},
    {(char*)"octave", (getter)pyopencv_line_descriptor_KeyLine_get_octave, (setter)pyopencv_line_descriptor_KeyLine_set_octave, (char*)"octave", NULL},
    {(char*)"pt", (getter)pyopencv_line_descriptor_KeyLine_get_pt, (setter)pyopencv_line_descriptor_KeyLine_set_pt, (char*)"pt", NULL},
    {(char*)"response", (getter)pyopencv_line_descriptor_KeyLine_get_response, (setter)pyopencv_line_descriptor_KeyLine_set_response, (char*)"response", NULL},
    {(char*)"sPointInOctaveX", (getter)pyopencv_line_descriptor_KeyLine_get_sPointInOctaveX, (setter)pyopencv_line_descriptor_KeyLine_set_sPointInOctaveX, (char*)"sPointInOctaveX", NULL},
    {(char*)"sPointInOctaveY", (getter)pyopencv_line_descriptor_KeyLine_get_sPointInOctaveY, (setter)pyopencv_line_descriptor_KeyLine_set_sPointInOctaveY, (char*)"sPointInOctaveY", NULL},
    {(char*)"size", (getter)pyopencv_line_descriptor_KeyLine_get_size, (setter)pyopencv_line_descriptor_KeyLine_set_size, (char*)"size", NULL},
    {(char*)"startPointX", (getter)pyopencv_line_descriptor_KeyLine_get_startPointX, (setter)pyopencv_line_descriptor_KeyLine_set_startPointX, (char*)"startPointX", NULL},
    {(char*)"startPointY", (getter)pyopencv_line_descriptor_KeyLine_get_startPointY, (setter)pyopencv_line_descriptor_KeyLine_set_startPointY, (char*)"startPointY", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_line_descriptor_line_descriptor_KeyLine_KeyLine(pyopencv_line_descriptor_KeyLine_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::line_descriptor::KeyLine());
        return 0;
    }

    return -1;
}

static PyObject* pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getEndPoint(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;

    cv::line_descriptor::KeyLine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_line_descriptor_KeyLine_Type))
        _self_ = &((pyopencv_line_descriptor_KeyLine_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'line_descriptor_KeyLine' or its derivative)");
    Point2f retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEndPoint());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getEndPointInOctave(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;

    cv::line_descriptor::KeyLine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_line_descriptor_KeyLine_Type))
        _self_ = &((pyopencv_line_descriptor_KeyLine_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'line_descriptor_KeyLine' or its derivative)");
    Point2f retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEndPointInOctave());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getStartPoint(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;

    cv::line_descriptor::KeyLine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_line_descriptor_KeyLine_Type))
        _self_ = &((pyopencv_line_descriptor_KeyLine_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'line_descriptor_KeyLine' or its derivative)");
    Point2f retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getStartPoint());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getStartPointInOctave(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;

    cv::line_descriptor::KeyLine* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_line_descriptor_KeyLine_Type))
        _self_ = &((pyopencv_line_descriptor_KeyLine_t*)self)->v;
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'line_descriptor_KeyLine' or its derivative)");
    Point2f retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getStartPointInOctave());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_line_descriptor_KeyLine_methods[] =
{
    {"getEndPoint", CV_PY_FN_WITH_KW_(pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getEndPoint, 0), "getEndPoint() -> retval\n.   Returns the end point of the line in the original image"},
    {"getEndPointInOctave", CV_PY_FN_WITH_KW_(pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getEndPointInOctave, 0), "getEndPointInOctave() -> retval\n.   Returns the end point of the line in the octave it was extracted from"},
    {"getStartPoint", CV_PY_FN_WITH_KW_(pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getStartPoint, 0), "getStartPoint() -> retval\n.   Returns the start point of the line in the original image"},
    {"getStartPointInOctave", CV_PY_FN_WITH_KW_(pyopencv_cv_line_descriptor_line_descriptor_KeyLine_getStartPointInOctave, 0), "getStartPointInOctave() -> retval\n.   Returns the start point of the line in the octave it was extracted from"},

    {NULL,          NULL}
};

static void pyopencv_line_descriptor_KeyLine_specials(void)
{
    pyopencv_line_descriptor_KeyLine_Type.tp_base = NULL;
    pyopencv_line_descriptor_KeyLine_Type.tp_dealloc = pyopencv_line_descriptor_KeyLine_dealloc;
    pyopencv_line_descriptor_KeyLine_Type.tp_repr = pyopencv_line_descriptor_KeyLine_repr;
    pyopencv_line_descriptor_KeyLine_Type.tp_getset = pyopencv_line_descriptor_KeyLine_getseters;
    pyopencv_line_descriptor_KeyLine_Type.tp_init = (initproc)pyopencv_cv_line_descriptor_line_descriptor_KeyLine_KeyLine;
    pyopencv_line_descriptor_KeyLine_Type.tp_methods = pyopencv_line_descriptor_KeyLine_methods;
}

static PyObject* pyopencv_line_descriptor_LSDDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<line_descriptor_LSDDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_line_descriptor_LSDDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_line_descriptor_line_descriptor_LSDDetector_createLSDDetector_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;

    Ptr<LSDDetector> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::line_descriptor::LSDDetector::createLSDDetector());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_line_descriptor_line_descriptor_LSDDetector_detect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::line_descriptor;

    cv::line_descriptor::LSDDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_line_descriptor_LSDDetector_Type))
        _self_ = dynamic_cast<cv::line_descriptor::LSDDetector*>(((pyopencv_line_descriptor_LSDDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'line_descriptor_LSDDetector' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_KeyLine keypoints;
    int scale=0;
    int numOctaves=0;
    PyObject* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "image", "scale", "numOctaves", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oii|O:line_descriptor_LSDDetector.detect", (char**)keywords, &pyobj_image, &scale, &numOctaves, &pyobj_mask) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->detect(image, keypoints, scale, numOctaves, mask));
        return pyopencv_from(keypoints);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    Mat image;
    vector_KeyLine keypoints;
    int scale=0;
    int numOctaves=0;
    PyObject* pyobj_mask = NULL;
    Mat mask;

    const char* keywords[] = { "image", "scale", "numOctaves", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oii|O:line_descriptor_LSDDetector.detect", (char**)keywords, &pyobj_image, &scale, &numOctaves, &pyobj_mask) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(_self_->detect(image, keypoints, scale, numOctaves, mask));
        return pyopencv_from(keypoints);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_keylines = NULL;
    vector_vector_KeyLine keylines;
    int scale=0;
    int numOctaves=0;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks=std::vector<Mat>();

    const char* keywords[] = { "images", "keylines", "scale", "numOctaves", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOii|O:line_descriptor_LSDDetector.detect", (char**)keywords, &pyobj_images, &pyobj_keylines, &scale, &numOctaves, &pyobj_masks) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_keylines, keylines, ArgInfo("keylines", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->detect(images, keylines, scale, numOctaves, masks));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_keylines = NULL;
    vector_vector_KeyLine keylines;
    int scale=0;
    int numOctaves=0;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks=std::vector<Mat>();

    const char* keywords[] = { "images", "keylines", "scale", "numOctaves", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOii|O:line_descriptor_LSDDetector.detect", (char**)keywords, &pyobj_images, &pyobj_keylines, &scale, &numOctaves, &pyobj_masks) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_keylines, keylines, ArgInfo("keylines", 0)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->detect(images, keylines, scale, numOctaves, masks));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_line_descriptor_LSDDetector_methods[] =
{
    {"createLSDDetector", CV_PY_FN_WITH_KW_(pyopencv_cv_line_descriptor_line_descriptor_LSDDetector_createLSDDetector_cls, METH_CLASS), "createLSDDetector() -> retval\n.   @brief Creates ad LSDDetector object, using smart pointers."},
    {"detect", CV_PY_FN_WITH_KW_(pyopencv_cv_line_descriptor_line_descriptor_LSDDetector_detect, 0), "detect(image, scale, numOctaves[, mask]) -> keypoints\n.   @brief Detect lines inside an image.\n.   \n.   @param image input image\n.   @param keypoints vector that will store extracted lines for one or more images\n.   @param scale scale factor used in pyramids generation\n.   @param numOctaves number of octaves inside pyramid\n.   @param mask mask matrix to detect only KeyLines of interest\n\n\n\ndetect(images, keylines, scale, numOctaves[, masks]) -> None\n.   @overload\n.   @param images input images\n.   @param keylines set of vectors that will store extracted lines for one or more images\n.   @param scale scale factor used in pyramids generation\n.   @param numOctaves number of octaves inside pyramid\n.   @param masks vector of mask matrices to detect only KeyLines of interest from each input image"},

    {NULL,          NULL}
};

static void pyopencv_line_descriptor_LSDDetector_specials(void)
{
    pyopencv_line_descriptor_LSDDetector_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_line_descriptor_LSDDetector_Type.tp_dealloc = pyopencv_line_descriptor_LSDDetector_dealloc;
    pyopencv_line_descriptor_LSDDetector_Type.tp_repr = pyopencv_line_descriptor_LSDDetector_repr;
    pyopencv_line_descriptor_LSDDetector_Type.tp_getset = pyopencv_line_descriptor_LSDDetector_getseters;
    pyopencv_line_descriptor_LSDDetector_Type.tp_init = (initproc)0;
    pyopencv_line_descriptor_LSDDetector_Type.tp_methods = pyopencv_line_descriptor_LSDDetector_methods;
}

static PyObject* pyopencv_saliency_Saliency_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_Saliency %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_Saliency_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_saliency_saliency_Saliency_computeSaliency(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::Saliency* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_Saliency_Type))
        _self_ = dynamic_cast<cv::saliency::Saliency*>(((pyopencv_saliency_Saliency_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_Saliency' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_saliencyMap = NULL;
    Mat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_Saliency.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_saliencyMap = NULL;
    UMat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_Saliency.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_saliency_Saliency_methods[] =
{
    {"computeSaliency", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_Saliency_computeSaliency, 0), "computeSaliency(image[, saliencyMap]) -> retval, saliencyMap\n.   * \\brief Compute the saliency\n.   * \\param image        The image.\n.   * \\param saliencyMap      The computed saliency map.\n.   * \\return true if the saliency map is computed, false otherwise"},

    {NULL,          NULL}
};

static void pyopencv_saliency_Saliency_specials(void)
{
    pyopencv_saliency_Saliency_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_saliency_Saliency_Type.tp_dealloc = pyopencv_saliency_Saliency_dealloc;
    pyopencv_saliency_Saliency_Type.tp_repr = pyopencv_saliency_Saliency_repr;
    pyopencv_saliency_Saliency_Type.tp_getset = pyopencv_saliency_Saliency_getseters;
    pyopencv_saliency_Saliency_Type.tp_init = (initproc)0;
    pyopencv_saliency_Saliency_Type.tp_methods = pyopencv_saliency_Saliency_methods;
}

static PyObject* pyopencv_saliency_StaticSaliency_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_StaticSaliency %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_StaticSaliency_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliency_computeBinaryMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliency* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliency_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliency*>(((pyopencv_saliency_StaticSaliency_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliency' or its derivative)");
    {
    PyObject* pyobj__saliencyMap = NULL;
    Mat _saliencyMap;
    PyObject* pyobj__binaryMap = NULL;
    Mat _binaryMap;
    bool retval;

    const char* keywords[] = { "_saliencyMap", "_binaryMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_StaticSaliency.computeBinaryMap", (char**)keywords, &pyobj__saliencyMap, &pyobj__binaryMap) &&
        pyopencv_to(pyobj__saliencyMap, _saliencyMap, ArgInfo("_saliencyMap", 0)) &&
        pyopencv_to(pyobj__binaryMap, _binaryMap, ArgInfo("_binaryMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeBinaryMap(_saliencyMap, _binaryMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(_binaryMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__saliencyMap = NULL;
    UMat _saliencyMap;
    PyObject* pyobj__binaryMap = NULL;
    UMat _binaryMap;
    bool retval;

    const char* keywords[] = { "_saliencyMap", "_binaryMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_StaticSaliency.computeBinaryMap", (char**)keywords, &pyobj__saliencyMap, &pyobj__binaryMap) &&
        pyopencv_to(pyobj__saliencyMap, _saliencyMap, ArgInfo("_saliencyMap", 0)) &&
        pyopencv_to(pyobj__binaryMap, _binaryMap, ArgInfo("_binaryMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeBinaryMap(_saliencyMap, _binaryMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(_binaryMap));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_saliency_StaticSaliency_methods[] =
{
    {"computeBinaryMap", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliency_computeBinaryMap, 0), "computeBinaryMap(_saliencyMap[, _binaryMap]) -> retval, _binaryMap\n.   @brief This function perform a binary map of given saliency map. This is obtained in this\n.   way:\n.   \n.   In a first step, to improve the definition of interest areas and facilitate identification of\n.   targets, a segmentation by clustering is performed, using *K-means algorithm*. Then, to gain a\n.   binary representation of clustered saliency map, since values of the map can vary according to\n.   the characteristics of frame under analysis, it is not convenient to use a fixed threshold. So,\n.   *Otsu's algorithm* is used, which assumes that the image to be thresholded contains two classes\n.   of pixels or bi-modal histograms (e.g. foreground and back-ground pixels); later on, the\n.   algorithm calculates the optimal threshold separating those two classes, so that their\n.   intra-class variance is minimal.\n.   \n.   @param _saliencyMap the saliency map obtained through one of the specialized algorithms\n.   @param _binaryMap the binary map"},

    {NULL,          NULL}
};

static void pyopencv_saliency_StaticSaliency_specials(void)
{
    pyopencv_saliency_StaticSaliency_Type.tp_base = &pyopencv_saliency_Saliency_Type;
    pyopencv_saliency_StaticSaliency_Type.tp_dealloc = pyopencv_saliency_StaticSaliency_dealloc;
    pyopencv_saliency_StaticSaliency_Type.tp_repr = pyopencv_saliency_StaticSaliency_repr;
    pyopencv_saliency_StaticSaliency_Type.tp_getset = pyopencv_saliency_StaticSaliency_getseters;
    pyopencv_saliency_StaticSaliency_Type.tp_init = (initproc)0;
    pyopencv_saliency_StaticSaliency_Type.tp_methods = pyopencv_saliency_StaticSaliency_methods;
}

static PyObject* pyopencv_saliency_MotionSaliency_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_MotionSaliency %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_MotionSaliency_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_saliency_MotionSaliency_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_saliency_MotionSaliency_specials(void)
{
    pyopencv_saliency_MotionSaliency_Type.tp_base = &pyopencv_saliency_Saliency_Type;
    pyopencv_saliency_MotionSaliency_Type.tp_dealloc = pyopencv_saliency_MotionSaliency_dealloc;
    pyopencv_saliency_MotionSaliency_Type.tp_repr = pyopencv_saliency_MotionSaliency_repr;
    pyopencv_saliency_MotionSaliency_Type.tp_getset = pyopencv_saliency_MotionSaliency_getseters;
    pyopencv_saliency_MotionSaliency_Type.tp_init = (initproc)0;
    pyopencv_saliency_MotionSaliency_Type.tp_methods = pyopencv_saliency_MotionSaliency_methods;
}

static PyObject* pyopencv_saliency_Objectness_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_Objectness %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_Objectness_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_saliency_Objectness_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_saliency_Objectness_specials(void)
{
    pyopencv_saliency_Objectness_Type.tp_base = &pyopencv_saliency_Saliency_Type;
    pyopencv_saliency_Objectness_Type.tp_dealloc = pyopencv_saliency_Objectness_dealloc;
    pyopencv_saliency_Objectness_Type.tp_repr = pyopencv_saliency_Objectness_repr;
    pyopencv_saliency_Objectness_Type.tp_getset = pyopencv_saliency_Objectness_getseters;
    pyopencv_saliency_Objectness_Type.tp_init = (initproc)0;
    pyopencv_saliency_Objectness_Type.tp_methods = pyopencv_saliency_Objectness_methods;
}

static PyObject* pyopencv_saliency_StaticSaliencySpectralResidual_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_StaticSaliencySpectralResidual %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_StaticSaliencySpectralResidual_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_computeSaliency(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencySpectralResidual* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencySpectralResidual*>(((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencySpectralResidual' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_saliencyMap = NULL;
    Mat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_StaticSaliencySpectralResidual.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_saliencyMap = NULL;
    UMat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_StaticSaliencySpectralResidual.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    Ptr<StaticSaliencySpectralResidual> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::saliency::StaticSaliencySpectralResidual::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_getImageHeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencySpectralResidual* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencySpectralResidual*>(((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencySpectralResidual' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getImageHeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_getImageWidth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencySpectralResidual* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencySpectralResidual*>(((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencySpectralResidual' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getImageWidth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencySpectralResidual* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencySpectralResidual*>(((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencySpectralResidual' or its derivative)");
    PyObject* pyobj_fn = NULL;
    FileNode fn;

    const char* keywords[] = { "fn", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:saliency_StaticSaliencySpectralResidual.read", (char**)keywords, &pyobj_fn) &&
        pyopencv_to(pyobj_fn, fn, ArgInfo("fn", 0)) )
    {
        ERRWRAP2(_self_->read(fn));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_setImageHeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencySpectralResidual* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencySpectralResidual*>(((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencySpectralResidual' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:saliency_StaticSaliencySpectralResidual.setImageHeight", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setImageHeight(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_setImageWidth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencySpectralResidual* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencySpectralResidual_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencySpectralResidual*>(((pyopencv_saliency_StaticSaliencySpectralResidual_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencySpectralResidual' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:saliency_StaticSaliencySpectralResidual.setImageWidth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setImageWidth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_saliency_StaticSaliencySpectralResidual_methods[] =
{
    {"computeSaliency", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_computeSaliency, 0), "computeSaliency(image[, saliencyMap]) -> retval, saliencyMap\n."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_create_cls, METH_CLASS), "create() -> retval\n."},
    {"getImageHeight", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_getImageHeight, 0), "getImageHeight() -> retval\n."},
    {"getImageWidth", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_getImageWidth, 0), "getImageWidth() -> retval\n."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_read, 0), "read(fn) -> None\n."},
    {"setImageHeight", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_setImageHeight, 0), "setImageHeight(val) -> None\n."},
    {"setImageWidth", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencySpectralResidual_setImageWidth, 0), "setImageWidth(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_saliency_StaticSaliencySpectralResidual_specials(void)
{
    pyopencv_saliency_StaticSaliencySpectralResidual_Type.tp_base = &pyopencv_saliency_StaticSaliency_Type;
    pyopencv_saliency_StaticSaliencySpectralResidual_Type.tp_dealloc = pyopencv_saliency_StaticSaliencySpectralResidual_dealloc;
    pyopencv_saliency_StaticSaliencySpectralResidual_Type.tp_repr = pyopencv_saliency_StaticSaliencySpectralResidual_repr;
    pyopencv_saliency_StaticSaliencySpectralResidual_Type.tp_getset = pyopencv_saliency_StaticSaliencySpectralResidual_getseters;
    pyopencv_saliency_StaticSaliencySpectralResidual_Type.tp_init = (initproc)0;
    pyopencv_saliency_StaticSaliencySpectralResidual_Type.tp_methods = pyopencv_saliency_StaticSaliencySpectralResidual_methods;
}

static PyObject* pyopencv_saliency_StaticSaliencyFineGrained_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_StaticSaliencyFineGrained %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_StaticSaliencyFineGrained_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencyFineGrained_computeSaliency(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::StaticSaliencyFineGrained* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_StaticSaliencyFineGrained_Type))
        _self_ = dynamic_cast<cv::saliency::StaticSaliencyFineGrained*>(((pyopencv_saliency_StaticSaliencyFineGrained_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_StaticSaliencyFineGrained' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_saliencyMap = NULL;
    Mat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_StaticSaliencyFineGrained.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_saliencyMap = NULL;
    UMat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_StaticSaliencyFineGrained.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_StaticSaliencyFineGrained_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    Ptr<StaticSaliencyFineGrained> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::saliency::StaticSaliencyFineGrained::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_saliency_StaticSaliencyFineGrained_methods[] =
{
    {"computeSaliency", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencyFineGrained_computeSaliency, 0), "computeSaliency(image[, saliencyMap]) -> retval, saliencyMap\n."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_StaticSaliencyFineGrained_create_cls, METH_CLASS), "create() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_saliency_StaticSaliencyFineGrained_specials(void)
{
    pyopencv_saliency_StaticSaliencyFineGrained_Type.tp_base = &pyopencv_saliency_StaticSaliency_Type;
    pyopencv_saliency_StaticSaliencyFineGrained_Type.tp_dealloc = pyopencv_saliency_StaticSaliencyFineGrained_dealloc;
    pyopencv_saliency_StaticSaliencyFineGrained_Type.tp_repr = pyopencv_saliency_StaticSaliencyFineGrained_repr;
    pyopencv_saliency_StaticSaliencyFineGrained_Type.tp_getset = pyopencv_saliency_StaticSaliencyFineGrained_getseters;
    pyopencv_saliency_StaticSaliencyFineGrained_Type.tp_init = (initproc)0;
    pyopencv_saliency_StaticSaliencyFineGrained_Type.tp_methods = pyopencv_saliency_StaticSaliencyFineGrained_methods;
}

static PyObject* pyopencv_saliency_MotionSaliencyBinWangApr2014_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_MotionSaliencyBinWangApr2014 %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_MotionSaliencyBinWangApr2014_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_computeSaliency(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_saliencyMap = NULL;
    Mat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_MotionSaliencyBinWangApr2014.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_saliencyMap = NULL;
    UMat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_MotionSaliencyBinWangApr2014.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    Ptr<MotionSaliencyBinWangApr2014> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::saliency::MotionSaliencyBinWangApr2014::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_getImageHeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getImageHeight());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_getImageWidth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getImageWidth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_init(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->init());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_setImageHeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:saliency_MotionSaliencyBinWangApr2014.setImageHeight", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setImageHeight(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_setImageWidth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:saliency_MotionSaliencyBinWangApr2014.setImageWidth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setImageWidth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_setImagesize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::MotionSaliencyBinWangApr2014* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_MotionSaliencyBinWangApr2014_Type))
        _self_ = dynamic_cast<cv::saliency::MotionSaliencyBinWangApr2014*>(((pyopencv_saliency_MotionSaliencyBinWangApr2014_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_MotionSaliencyBinWangApr2014' or its derivative)");
    int W=0;
    int H=0;

    const char* keywords[] = { "W", "H", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:saliency_MotionSaliencyBinWangApr2014.setImagesize", (char**)keywords, &W, &H) )
    {
        ERRWRAP2(_self_->setImagesize(W, H));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_saliency_MotionSaliencyBinWangApr2014_methods[] =
{
    {"computeSaliency", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_computeSaliency, 0), "computeSaliency(image[, saliencyMap]) -> retval, saliencyMap\n."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_create_cls, METH_CLASS), "create() -> retval\n."},
    {"getImageHeight", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_getImageHeight, 0), "getImageHeight() -> retval\n."},
    {"getImageWidth", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_getImageWidth, 0), "getImageWidth() -> retval\n."},
    {"init", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_init, 0), "init() -> retval\n.   @brief This function allows the correct initialization of all data structures that will be used by the\n.   algorithm."},
    {"setImageHeight", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_setImageHeight, 0), "setImageHeight(val) -> None\n."},
    {"setImageWidth", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_setImageWidth, 0), "setImageWidth(val) -> None\n."},
    {"setImagesize", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_MotionSaliencyBinWangApr2014_setImagesize, 0), "setImagesize(W, H) -> None\n.   @brief This is a utility function that allows to set the correct size (taken from the input image) in the\n.   corresponding variables that will be used to size the data structures of the algorithm.\n.   @param W width of input image\n.   @param H height of input image"},

    {NULL,          NULL}
};

static void pyopencv_saliency_MotionSaliencyBinWangApr2014_specials(void)
{
    pyopencv_saliency_MotionSaliencyBinWangApr2014_Type.tp_base = &pyopencv_saliency_MotionSaliency_Type;
    pyopencv_saliency_MotionSaliencyBinWangApr2014_Type.tp_dealloc = pyopencv_saliency_MotionSaliencyBinWangApr2014_dealloc;
    pyopencv_saliency_MotionSaliencyBinWangApr2014_Type.tp_repr = pyopencv_saliency_MotionSaliencyBinWangApr2014_repr;
    pyopencv_saliency_MotionSaliencyBinWangApr2014_Type.tp_getset = pyopencv_saliency_MotionSaliencyBinWangApr2014_getseters;
    pyopencv_saliency_MotionSaliencyBinWangApr2014_Type.tp_init = (initproc)0;
    pyopencv_saliency_MotionSaliencyBinWangApr2014_Type.tp_methods = pyopencv_saliency_MotionSaliencyBinWangApr2014_methods;
}

static PyObject* pyopencv_saliency_ObjectnessBING_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<saliency_ObjectnessBING %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_saliency_ObjectnessBING_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_computeSaliency(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_saliencyMap = NULL;
    Mat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_ObjectnessBING.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_saliencyMap = NULL;
    UMat saliencyMap;
    bool retval;

    const char* keywords[] = { "image", "saliencyMap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:saliency_ObjectnessBING.computeSaliency", (char**)keywords, &pyobj_image, &pyobj_saliencyMap) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_saliencyMap, saliencyMap, ArgInfo("saliencyMap", 1)) )
    {
        ERRWRAP2(retval = _self_->computeSaliency(image, saliencyMap));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(saliencyMap));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    Ptr<ObjectnessBING> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::saliency::ObjectnessBING::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_getBase(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBase());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_getNSS(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNSS());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_getW(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getW());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_getobjectnessValues(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    std::vector<float> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getobjectnessValues());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->read());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_setBBResDir(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    PyObject* pyobj_resultsDir = NULL;
    String resultsDir;

    const char* keywords[] = { "resultsDir", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:saliency_ObjectnessBING.setBBResDir", (char**)keywords, &pyobj_resultsDir) &&
        pyopencv_to(pyobj_resultsDir, resultsDir, ArgInfo("resultsDir", 0)) )
    {
        ERRWRAP2(_self_->setBBResDir(resultsDir));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_setBase(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:saliency_ObjectnessBING.setBase", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setBase(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_setNSS(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:saliency_ObjectnessBING.setNSS", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setNSS(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_setTrainingPath(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    PyObject* pyobj_trainingPath = NULL;
    String trainingPath;

    const char* keywords[] = { "trainingPath", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:saliency_ObjectnessBING.setTrainingPath", (char**)keywords, &pyobj_trainingPath) &&
        pyopencv_to(pyobj_trainingPath, trainingPath, ArgInfo("trainingPath", 0)) )
    {
        ERRWRAP2(_self_->setTrainingPath(trainingPath));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_setW(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:saliency_ObjectnessBING.setW", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setW(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_saliency_saliency_ObjectnessBING_write(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::saliency;

    cv::saliency::ObjectnessBING* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_saliency_ObjectnessBING_Type))
        _self_ = dynamic_cast<cv::saliency::ObjectnessBING*>(((pyopencv_saliency_ObjectnessBING_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'saliency_ObjectnessBING' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->write());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_saliency_ObjectnessBING_methods[] =
{
    {"computeSaliency", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_computeSaliency, 0), "computeSaliency(image[, saliencyMap]) -> retval, saliencyMap\n."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_create_cls, METH_CLASS), "create() -> retval\n."},
    {"getBase", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_getBase, 0), "getBase() -> retval\n."},
    {"getNSS", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_getNSS, 0), "getNSS() -> retval\n."},
    {"getW", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_getW, 0), "getW() -> retval\n."},
    {"getobjectnessValues", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_getobjectnessValues, 0), "getobjectnessValues() -> retval\n.   @brief Return the list of the rectangles' objectness value,\n.   \n.   in the same order as the *vector\\<Vec4i\\> objectnessBoundingBox* returned by the algorithm (in\n.   computeSaliencyImpl function). The bigger value these scores are, it is more likely to be an\n.   object window."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_read, 0), "read() -> None\n."},
    {"setBBResDir", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_setBBResDir, 0), "setBBResDir(resultsDir) -> None\n.   @brief This is a utility function that allows to set an arbitrary path in which the algorithm will save the\n.   optional results\n.   \n.   (ie writing on file the total number and the list of rectangles returned by objectess, one for\n.   each row).\n.   @param resultsDir results' folder path"},
    {"setBase", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_setBase, 0), "setBase(val) -> None\n."},
    {"setNSS", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_setNSS, 0), "setNSS(val) -> None\n."},
    {"setTrainingPath", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_setTrainingPath, 0), "setTrainingPath(trainingPath) -> None\n.   @brief This is a utility function that allows to set the correct path from which the algorithm will load\n.   the trained model.\n.   @param trainingPath trained model path"},
    {"setW", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_setW, 0), "setW(val) -> None\n."},
    {"write", CV_PY_FN_WITH_KW_(pyopencv_cv_saliency_saliency_ObjectnessBING_write, 0), "write() -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_saliency_ObjectnessBING_specials(void)
{
    pyopencv_saliency_ObjectnessBING_Type.tp_base = &pyopencv_saliency_Objectness_Type;
    pyopencv_saliency_ObjectnessBING_Type.tp_dealloc = pyopencv_saliency_ObjectnessBING_dealloc;
    pyopencv_saliency_ObjectnessBING_Type.tp_repr = pyopencv_saliency_ObjectnessBING_repr;
    pyopencv_saliency_ObjectnessBING_Type.tp_getset = pyopencv_saliency_ObjectnessBING_getseters;
    pyopencv_saliency_ObjectnessBING_Type.tp_init = (initproc)0;
    pyopencv_saliency_ObjectnessBING_Type.tp_methods = pyopencv_saliency_ObjectnessBING_methods;
}

static PyObject* pyopencv_CirclesGridFinderParameters_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CirclesGridFinderParameters %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_CirclesGridFinderParameters_get_convexHullFactor(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.convexHullFactor);
}

static int pyopencv_CirclesGridFinderParameters_set_convexHullFactor(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the convexHullFactor attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.convexHullFactor) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_densityNeighborhoodSize(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.densityNeighborhoodSize);
}

static int pyopencv_CirclesGridFinderParameters_set_densityNeighborhoodSize(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the densityNeighborhoodSize attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.densityNeighborhoodSize) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_edgeGain(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.edgeGain);
}

static int pyopencv_CirclesGridFinderParameters_set_edgeGain(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the edgeGain attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.edgeGain) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_edgePenalty(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.edgePenalty);
}

static int pyopencv_CirclesGridFinderParameters_set_edgePenalty(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the edgePenalty attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.edgePenalty) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_existingVertexGain(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.existingVertexGain);
}

static int pyopencv_CirclesGridFinderParameters_set_existingVertexGain(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the existingVertexGain attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.existingVertexGain) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_keypointScale(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.keypointScale);
}

static int pyopencv_CirclesGridFinderParameters_set_keypointScale(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the keypointScale attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.keypointScale) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_kmeansAttempts(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.kmeansAttempts);
}

static int pyopencv_CirclesGridFinderParameters_set_kmeansAttempts(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the kmeansAttempts attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.kmeansAttempts) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minDensity(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.minDensity);
}

static int pyopencv_CirclesGridFinderParameters_set_minDensity(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDensity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minDensity) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minDistanceToAddKeypoint(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.minDistanceToAddKeypoint);
}

static int pyopencv_CirclesGridFinderParameters_set_minDistanceToAddKeypoint(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDistanceToAddKeypoint attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minDistanceToAddKeypoint) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minGraphConfidence(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.minGraphConfidence);
}

static int pyopencv_CirclesGridFinderParameters_set_minGraphConfidence(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minGraphConfidence attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minGraphConfidence) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_minRNGEdgeSwitchDist(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.minRNGEdgeSwitchDist);
}

static int pyopencv_CirclesGridFinderParameters_set_minRNGEdgeSwitchDist(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minRNGEdgeSwitchDist attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.minRNGEdgeSwitchDist) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_vertexGain(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.vertexGain);
}

static int pyopencv_CirclesGridFinderParameters_set_vertexGain(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the vertexGain attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.vertexGain) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters_get_vertexPenalty(pyopencv_CirclesGridFinderParameters_t* p, void *closure)
{
    return pyopencv_from(p->v.vertexPenalty);
}

static int pyopencv_CirclesGridFinderParameters_set_vertexPenalty(pyopencv_CirclesGridFinderParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the vertexPenalty attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.vertexPenalty) ? 0 : -1;
}


static PyGetSetDef pyopencv_CirclesGridFinderParameters_getseters[] =
{
    {(char*)"convexHullFactor", (getter)pyopencv_CirclesGridFinderParameters_get_convexHullFactor, (setter)pyopencv_CirclesGridFinderParameters_set_convexHullFactor, (char*)"convexHullFactor", NULL},
    {(char*)"densityNeighborhoodSize", (getter)pyopencv_CirclesGridFinderParameters_get_densityNeighborhoodSize, (setter)pyopencv_CirclesGridFinderParameters_set_densityNeighborhoodSize, (char*)"densityNeighborhoodSize", NULL},
    {(char*)"edgeGain", (getter)pyopencv_CirclesGridFinderParameters_get_edgeGain, (setter)pyopencv_CirclesGridFinderParameters_set_edgeGain, (char*)"edgeGain", NULL},
    {(char*)"edgePenalty", (getter)pyopencv_CirclesGridFinderParameters_get_edgePenalty, (setter)pyopencv_CirclesGridFinderParameters_set_edgePenalty, (char*)"edgePenalty", NULL},
    {(char*)"existingVertexGain", (getter)pyopencv_CirclesGridFinderParameters_get_existingVertexGain, (setter)pyopencv_CirclesGridFinderParameters_set_existingVertexGain, (char*)"existingVertexGain", NULL},
    {(char*)"keypointScale", (getter)pyopencv_CirclesGridFinderParameters_get_keypointScale, (setter)pyopencv_CirclesGridFinderParameters_set_keypointScale, (char*)"keypointScale", NULL},
    {(char*)"kmeansAttempts", (getter)pyopencv_CirclesGridFinderParameters_get_kmeansAttempts, (setter)pyopencv_CirclesGridFinderParameters_set_kmeansAttempts, (char*)"kmeansAttempts", NULL},
    {(char*)"minDensity", (getter)pyopencv_CirclesGridFinderParameters_get_minDensity, (setter)pyopencv_CirclesGridFinderParameters_set_minDensity, (char*)"minDensity", NULL},
    {(char*)"minDistanceToAddKeypoint", (getter)pyopencv_CirclesGridFinderParameters_get_minDistanceToAddKeypoint, (setter)pyopencv_CirclesGridFinderParameters_set_minDistanceToAddKeypoint, (char*)"minDistanceToAddKeypoint", NULL},
    {(char*)"minGraphConfidence", (getter)pyopencv_CirclesGridFinderParameters_get_minGraphConfidence, (setter)pyopencv_CirclesGridFinderParameters_set_minGraphConfidence, (char*)"minGraphConfidence", NULL},
    {(char*)"minRNGEdgeSwitchDist", (getter)pyopencv_CirclesGridFinderParameters_get_minRNGEdgeSwitchDist, (setter)pyopencv_CirclesGridFinderParameters_set_minRNGEdgeSwitchDist, (char*)"minRNGEdgeSwitchDist", NULL},
    {(char*)"vertexGain", (getter)pyopencv_CirclesGridFinderParameters_get_vertexGain, (setter)pyopencv_CirclesGridFinderParameters_set_vertexGain, (char*)"vertexGain", NULL},
    {(char*)"vertexPenalty", (getter)pyopencv_CirclesGridFinderParameters_get_vertexPenalty, (setter)pyopencv_CirclesGridFinderParameters_set_vertexPenalty, (char*)"vertexPenalty", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_CirclesGridFinderParameters_CirclesGridFinderParameters(pyopencv_CirclesGridFinderParameters_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::CirclesGridFinderParameters());
        return 0;
    }

    return -1;
}



static PyMethodDef pyopencv_CirclesGridFinderParameters_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_CirclesGridFinderParameters_specials(void)
{
    pyopencv_CirclesGridFinderParameters_Type.tp_base = NULL;
    pyopencv_CirclesGridFinderParameters_Type.tp_dealloc = pyopencv_CirclesGridFinderParameters_dealloc;
    pyopencv_CirclesGridFinderParameters_Type.tp_repr = pyopencv_CirclesGridFinderParameters_repr;
    pyopencv_CirclesGridFinderParameters_Type.tp_getset = pyopencv_CirclesGridFinderParameters_getseters;
    pyopencv_CirclesGridFinderParameters_Type.tp_init = (initproc)pyopencv_cv_CirclesGridFinderParameters_CirclesGridFinderParameters;
    pyopencv_CirclesGridFinderParameters_Type.tp_methods = pyopencv_CirclesGridFinderParameters_methods;
}

static PyObject* pyopencv_CirclesGridFinderParameters2_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<CirclesGridFinderParameters2 %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_CirclesGridFinderParameters2_get_maxRectifiedDistance(pyopencv_CirclesGridFinderParameters2_t* p, void *closure)
{
    return pyopencv_from(p->v.maxRectifiedDistance);
}

static int pyopencv_CirclesGridFinderParameters2_set_maxRectifiedDistance(pyopencv_CirclesGridFinderParameters2_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxRectifiedDistance attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.maxRectifiedDistance) ? 0 : -1;
}

static PyObject* pyopencv_CirclesGridFinderParameters2_get_squareSize(pyopencv_CirclesGridFinderParameters2_t* p, void *closure)
{
    return pyopencv_from(p->v.squareSize);
}

static int pyopencv_CirclesGridFinderParameters2_set_squareSize(pyopencv_CirclesGridFinderParameters2_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the squareSize attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.squareSize) ? 0 : -1;
}


static PyGetSetDef pyopencv_CirclesGridFinderParameters2_getseters[] =
{
    {(char*)"maxRectifiedDistance", (getter)pyopencv_CirclesGridFinderParameters2_get_maxRectifiedDistance, (setter)pyopencv_CirclesGridFinderParameters2_set_maxRectifiedDistance, (char*)"maxRectifiedDistance", NULL},
    {(char*)"squareSize", (getter)pyopencv_CirclesGridFinderParameters2_get_squareSize, (setter)pyopencv_CirclesGridFinderParameters2_set_squareSize, (char*)"squareSize", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_CirclesGridFinderParameters2_CirclesGridFinderParameters2(pyopencv_CirclesGridFinderParameters2_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::CirclesGridFinderParameters2());
        return 0;
    }

    return -1;
}



static PyMethodDef pyopencv_CirclesGridFinderParameters2_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_CirclesGridFinderParameters2_specials(void)
{
    pyopencv_CirclesGridFinderParameters2_Type.tp_base = &pyopencv_CirclesGridFinderParameters_Type;
    pyopencv_CirclesGridFinderParameters2_Type.tp_dealloc = pyopencv_CirclesGridFinderParameters2_dealloc;
    pyopencv_CirclesGridFinderParameters2_Type.tp_repr = pyopencv_CirclesGridFinderParameters2_repr;
    pyopencv_CirclesGridFinderParameters2_Type.tp_getset = pyopencv_CirclesGridFinderParameters2_getseters;
    pyopencv_CirclesGridFinderParameters2_Type.tp_init = (initproc)pyopencv_cv_CirclesGridFinderParameters2_CirclesGridFinderParameters2;
    pyopencv_CirclesGridFinderParameters2_Type.tp_methods = pyopencv_CirclesGridFinderParameters2_methods;
}

static PyObject* pyopencv_StereoMatcher_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<StereoMatcher %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_StereoMatcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_StereoMatcher_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    {
    PyObject* pyobj_left = NULL;
    Mat left;
    PyObject* pyobj_right = NULL;
    Mat right;
    PyObject* pyobj_disparity = NULL;
    Mat disparity;

    const char* keywords[] = { "left", "right", "disparity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:StereoMatcher.compute", (char**)keywords, &pyobj_left, &pyobj_right, &pyobj_disparity) &&
        pyopencv_to(pyobj_left, left, ArgInfo("left", 0)) &&
        pyopencv_to(pyobj_right, right, ArgInfo("right", 0)) &&
        pyopencv_to(pyobj_disparity, disparity, ArgInfo("disparity", 1)) )
    {
        ERRWRAP2(_self_->compute(left, right, disparity));
        return pyopencv_from(disparity);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_left = NULL;
    UMat left;
    PyObject* pyobj_right = NULL;
    UMat right;
    PyObject* pyobj_disparity = NULL;
    UMat disparity;

    const char* keywords[] = { "left", "right", "disparity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:StereoMatcher.compute", (char**)keywords, &pyobj_left, &pyobj_right, &pyobj_disparity) &&
        pyopencv_to(pyobj_left, left, ArgInfo("left", 0)) &&
        pyopencv_to(pyobj_right, right, ArgInfo("right", 0)) &&
        pyopencv_to(pyobj_disparity, disparity, ArgInfo("disparity", 1)) )
    {
        ERRWRAP2(_self_->compute(left, right, disparity));
        return pyopencv_from(disparity);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_getBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBlockSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_getDisp12MaxDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDisp12MaxDiff());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_getMinDisparity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinDisparity());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_getNumDisparities(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumDisparities());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_getSpeckleRange(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSpeckleRange());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_getSpeckleWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSpeckleWindowSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_setBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int blockSize=0;

    const char* keywords[] = { "blockSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoMatcher.setBlockSize", (char**)keywords, &blockSize) )
    {
        ERRWRAP2(_self_->setBlockSize(blockSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_setDisp12MaxDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int disp12MaxDiff=0;

    const char* keywords[] = { "disp12MaxDiff", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoMatcher.setDisp12MaxDiff", (char**)keywords, &disp12MaxDiff) )
    {
        ERRWRAP2(_self_->setDisp12MaxDiff(disp12MaxDiff));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_setMinDisparity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int minDisparity=0;

    const char* keywords[] = { "minDisparity", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoMatcher.setMinDisparity", (char**)keywords, &minDisparity) )
    {
        ERRWRAP2(_self_->setMinDisparity(minDisparity));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_setNumDisparities(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int numDisparities=0;

    const char* keywords[] = { "numDisparities", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoMatcher.setNumDisparities", (char**)keywords, &numDisparities) )
    {
        ERRWRAP2(_self_->setNumDisparities(numDisparities));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_setSpeckleRange(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int speckleRange=0;

    const char* keywords[] = { "speckleRange", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoMatcher.setSpeckleRange", (char**)keywords, &speckleRange) )
    {
        ERRWRAP2(_self_->setSpeckleRange(speckleRange));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoMatcher_setSpeckleWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoMatcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoMatcher_Type))
        _self_ = dynamic_cast<cv::StereoMatcher*>(((pyopencv_StereoMatcher_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoMatcher' or its derivative)");
    int speckleWindowSize=0;

    const char* keywords[] = { "speckleWindowSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoMatcher.setSpeckleWindowSize", (char**)keywords, &speckleWindowSize) )
    {
        ERRWRAP2(_self_->setSpeckleWindowSize(speckleWindowSize));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_StereoMatcher_methods[] =
{
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_compute, 0), "compute(left, right[, disparity]) -> disparity\n.   @brief Computes disparity map for the specified stereo pair\n.   \n.   @param left Left 8-bit single-channel image.\n.   @param right Right image of the same size and the same type as the left one.\n.   @param disparity Output disparity map. It has the same size as the input images. Some algorithms,\n.   like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value\n.   has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map."},
    {"getBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getBlockSize, 0), "getBlockSize() -> retval\n."},
    {"getDisp12MaxDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getDisp12MaxDiff, 0), "getDisp12MaxDiff() -> retval\n."},
    {"getMinDisparity", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getMinDisparity, 0), "getMinDisparity() -> retval\n."},
    {"getNumDisparities", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getNumDisparities, 0), "getNumDisparities() -> retval\n."},
    {"getSpeckleRange", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getSpeckleRange, 0), "getSpeckleRange() -> retval\n."},
    {"getSpeckleWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_getSpeckleWindowSize, 0), "getSpeckleWindowSize() -> retval\n."},
    {"setBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setBlockSize, 0), "setBlockSize(blockSize) -> None\n."},
    {"setDisp12MaxDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setDisp12MaxDiff, 0), "setDisp12MaxDiff(disp12MaxDiff) -> None\n."},
    {"setMinDisparity", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setMinDisparity, 0), "setMinDisparity(minDisparity) -> None\n."},
    {"setNumDisparities", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setNumDisparities, 0), "setNumDisparities(numDisparities) -> None\n."},
    {"setSpeckleRange", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setSpeckleRange, 0), "setSpeckleRange(speckleRange) -> None\n."},
    {"setSpeckleWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoMatcher_setSpeckleWindowSize, 0), "setSpeckleWindowSize(speckleWindowSize) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_StereoMatcher_specials(void)
{
    pyopencv_StereoMatcher_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_StereoMatcher_Type.tp_dealloc = pyopencv_StereoMatcher_dealloc;
    pyopencv_StereoMatcher_Type.tp_repr = pyopencv_StereoMatcher_repr;
    pyopencv_StereoMatcher_Type.tp_getset = pyopencv_StereoMatcher_getseters;
    pyopencv_StereoMatcher_Type.tp_init = (initproc)0;
    pyopencv_StereoMatcher_Type.tp_methods = pyopencv_StereoMatcher_methods;
}

static PyObject* pyopencv_StereoBM_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<StereoBM %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_StereoBM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_StereoBM_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int numDisparities=0;
    int blockSize=21;
    Ptr<StereoBM> retval;

    const char* keywords[] = { "numDisparities", "blockSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ii:StereoBM.create", (char**)keywords, &numDisparities, &blockSize) )
    {
        ERRWRAP2(retval = cv::StereoBM::create(numDisparities, blockSize));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getPreFilterCap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPreFilterCap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getPreFilterSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPreFilterSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getPreFilterType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPreFilterType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getROI1(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Rect retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getROI1());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getROI2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    Rect retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getROI2());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getSmallerBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSmallerBlockSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getTextureThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTextureThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_getUniquenessRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUniquenessRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setPreFilterCap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int preFilterCap=0;

    const char* keywords[] = { "preFilterCap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoBM.setPreFilterCap", (char**)keywords, &preFilterCap) )
    {
        ERRWRAP2(_self_->setPreFilterCap(preFilterCap));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setPreFilterSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int preFilterSize=0;

    const char* keywords[] = { "preFilterSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoBM.setPreFilterSize", (char**)keywords, &preFilterSize) )
    {
        ERRWRAP2(_self_->setPreFilterSize(preFilterSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setPreFilterType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int preFilterType=0;

    const char* keywords[] = { "preFilterType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoBM.setPreFilterType", (char**)keywords, &preFilterType) )
    {
        ERRWRAP2(_self_->setPreFilterType(preFilterType));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setROI1(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    PyObject* pyobj_roi1 = NULL;
    Rect roi1;

    const char* keywords[] = { "roi1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:StereoBM.setROI1", (char**)keywords, &pyobj_roi1) &&
        pyopencv_to(pyobj_roi1, roi1, ArgInfo("roi1", 0)) )
    {
        ERRWRAP2(_self_->setROI1(roi1));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setROI2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    PyObject* pyobj_roi2 = NULL;
    Rect roi2;

    const char* keywords[] = { "roi2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:StereoBM.setROI2", (char**)keywords, &pyobj_roi2) &&
        pyopencv_to(pyobj_roi2, roi2, ArgInfo("roi2", 0)) )
    {
        ERRWRAP2(_self_->setROI2(roi2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setSmallerBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int blockSize=0;

    const char* keywords[] = { "blockSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoBM.setSmallerBlockSize", (char**)keywords, &blockSize) )
    {
        ERRWRAP2(_self_->setSmallerBlockSize(blockSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setTextureThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int textureThreshold=0;

    const char* keywords[] = { "textureThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoBM.setTextureThreshold", (char**)keywords, &textureThreshold) )
    {
        ERRWRAP2(_self_->setTextureThreshold(textureThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoBM_setUniquenessRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoBM_Type))
        _self_ = dynamic_cast<cv::StereoBM*>(((pyopencv_StereoBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoBM' or its derivative)");
    int uniquenessRatio=0;

    const char* keywords[] = { "uniquenessRatio", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoBM.setUniquenessRatio", (char**)keywords, &uniquenessRatio) )
    {
        ERRWRAP2(_self_->setUniquenessRatio(uniquenessRatio));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_StereoBM_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_create_cls, METH_CLASS), "create([, numDisparities[, blockSize]]) -> retval\n.   @brief Creates StereoBM object\n.   \n.   @param numDisparities the disparity search range. For each pixel algorithm will find the best\n.   disparity from 0 (default minimum disparity) to numDisparities. The search range can then be\n.   shifted by changing the minimum disparity.\n.   @param blockSize the linear size of the blocks compared by the algorithm. The size should be odd\n.   (as the block is centered at the current pixel). Larger block size implies smoother, though less\n.   accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher\n.   chance for algorithm to find a wrong correspondence.\n.   \n.   The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for\n.   a specific stereo pair."},
    {"getPreFilterCap", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getPreFilterCap, 0), "getPreFilterCap() -> retval\n."},
    {"getPreFilterSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getPreFilterSize, 0), "getPreFilterSize() -> retval\n."},
    {"getPreFilterType", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getPreFilterType, 0), "getPreFilterType() -> retval\n."},
    {"getROI1", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getROI1, 0), "getROI1() -> retval\n."},
    {"getROI2", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getROI2, 0), "getROI2() -> retval\n."},
    {"getSmallerBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getSmallerBlockSize, 0), "getSmallerBlockSize() -> retval\n."},
    {"getTextureThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getTextureThreshold, 0), "getTextureThreshold() -> retval\n."},
    {"getUniquenessRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_getUniquenessRatio, 0), "getUniquenessRatio() -> retval\n."},
    {"setPreFilterCap", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setPreFilterCap, 0), "setPreFilterCap(preFilterCap) -> None\n."},
    {"setPreFilterSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setPreFilterSize, 0), "setPreFilterSize(preFilterSize) -> None\n."},
    {"setPreFilterType", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setPreFilterType, 0), "setPreFilterType(preFilterType) -> None\n."},
    {"setROI1", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setROI1, 0), "setROI1(roi1) -> None\n."},
    {"setROI2", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setROI2, 0), "setROI2(roi2) -> None\n."},
    {"setSmallerBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setSmallerBlockSize, 0), "setSmallerBlockSize(blockSize) -> None\n."},
    {"setTextureThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setTextureThreshold, 0), "setTextureThreshold(textureThreshold) -> None\n."},
    {"setUniquenessRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoBM_setUniquenessRatio, 0), "setUniquenessRatio(uniquenessRatio) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_StereoBM_specials(void)
{
    pyopencv_StereoBM_Type.tp_base = &pyopencv_StereoMatcher_Type;
    pyopencv_StereoBM_Type.tp_dealloc = pyopencv_StereoBM_dealloc;
    pyopencv_StereoBM_Type.tp_repr = pyopencv_StereoBM_repr;
    pyopencv_StereoBM_Type.tp_getset = pyopencv_StereoBM_getseters;
    pyopencv_StereoBM_Type.tp_init = (initproc)0;
    pyopencv_StereoBM_Type.tp_methods = pyopencv_StereoBM_methods;
}

static PyObject* pyopencv_StereoSGBM_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<StereoSGBM %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_StereoSGBM_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_StereoSGBM_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    int minDisparity=0;
    int numDisparities=16;
    int blockSize=3;
    int P1=0;
    int P2=0;
    int disp12MaxDiff=0;
    int preFilterCap=0;
    int uniquenessRatio=0;
    int speckleWindowSize=0;
    int speckleRange=0;
    int mode=StereoSGBM::MODE_SGBM;
    Ptr<StereoSGBM> retval;

    const char* keywords[] = { "minDisparity", "numDisparities", "blockSize", "P1", "P2", "disp12MaxDiff", "preFilterCap", "uniquenessRatio", "speckleWindowSize", "speckleRange", "mode", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiiiiiiiiii:StereoSGBM.create", (char**)keywords, &minDisparity, &numDisparities, &blockSize, &P1, &P2, &disp12MaxDiff, &preFilterCap, &uniquenessRatio, &speckleWindowSize, &speckleRange, &mode) )
    {
        ERRWRAP2(retval = cv::StereoSGBM::create(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio, speckleWindowSize, speckleRange, mode));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_getMode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMode());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_getP1(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getP1());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_getP2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getP2());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_getPreFilterCap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPreFilterCap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_getUniquenessRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUniquenessRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_setMode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int mode=0;

    const char* keywords[] = { "mode", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoSGBM.setMode", (char**)keywords, &mode) )
    {
        ERRWRAP2(_self_->setMode(mode));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_setP1(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int P1=0;

    const char* keywords[] = { "P1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoSGBM.setP1", (char**)keywords, &P1) )
    {
        ERRWRAP2(_self_->setP1(P1));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_setP2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int P2=0;

    const char* keywords[] = { "P2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoSGBM.setP2", (char**)keywords, &P2) )
    {
        ERRWRAP2(_self_->setP2(P2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_setPreFilterCap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int preFilterCap=0;

    const char* keywords[] = { "preFilterCap", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoSGBM.setPreFilterCap", (char**)keywords, &preFilterCap) )
    {
        ERRWRAP2(_self_->setPreFilterCap(preFilterCap));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_StereoSGBM_setUniquenessRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::StereoSGBM* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_StereoSGBM_Type))
        _self_ = dynamic_cast<cv::StereoSGBM*>(((pyopencv_StereoSGBM_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'StereoSGBM' or its derivative)");
    int uniquenessRatio=0;

    const char* keywords[] = { "uniquenessRatio", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:StereoSGBM.setUniquenessRatio", (char**)keywords, &uniquenessRatio) )
    {
        ERRWRAP2(_self_->setUniquenessRatio(uniquenessRatio));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_StereoSGBM_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_create_cls, METH_CLASS), "create([, minDisparity[, numDisparities[, blockSize[, P1[, P2[, disp12MaxDiff[, preFilterCap[, uniquenessRatio[, speckleWindowSize[, speckleRange[, mode]]]]]]]]]]]) -> retval\n.   @brief Creates StereoSGBM object\n.   \n.   @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes\n.   rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.\n.   @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than\n.   zero. In the current implementation, this parameter must be divisible by 16.\n.   @param blockSize Matched block size. It must be an odd number \\>=1 . Normally, it should be\n.   somewhere in the 3..11 range.\n.   @param P1 The first parameter controlling the disparity smoothness. See below.\n.   @param P2 The second parameter controlling the disparity smoothness. The larger the values are,\n.   the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1\n.   between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor\n.   pixels. The algorithm requires P2 \\> P1 . See stereo_match.cpp sample where some reasonably good\n.   P1 and P2 values are shown (like 8\\*number_of_image_channels\\*SADWindowSize\\*SADWindowSize and\n.   32\\*number_of_image_channels\\*SADWindowSize\\*SADWindowSize , respectively).\n.   @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right\n.   disparity check. Set it to a non-positive value to disable the check.\n.   @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first\n.   computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.\n.   The result values are passed to the Birchfield-Tomasi pixel cost function.\n.   @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function\n.   value should \"win\" the second best value to consider the found match correct. Normally, a value\n.   within the 5-15 range is good enough.\n.   @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles\n.   and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the\n.   50-200 range.\n.   @param speckleRange Maximum disparity variation within each connected component. If you do speckle\n.   filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.\n.   Normally, 1 or 2 is good enough.\n.   @param mode Set it to StereoSGBM::MODE_HH to run the full-scale two-pass dynamic programming\n.   algorithm. It will consume O(W\\*H\\*numDisparities) bytes, which is large for 640x480 stereo and\n.   huge for HD-size pictures. By default, it is set to false .\n.   \n.   The first constructor initializes StereoSGBM with all the default parameters. So, you only have to\n.   set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter\n.   to a custom value."},
    {"getMode", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getMode, 0), "getMode() -> retval\n."},
    {"getP1", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getP1, 0), "getP1() -> retval\n."},
    {"getP2", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getP2, 0), "getP2() -> retval\n."},
    {"getPreFilterCap", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getPreFilterCap, 0), "getPreFilterCap() -> retval\n."},
    {"getUniquenessRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_getUniquenessRatio, 0), "getUniquenessRatio() -> retval\n."},
    {"setMode", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setMode, 0), "setMode(mode) -> None\n."},
    {"setP1", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setP1, 0), "setP1(P1) -> None\n."},
    {"setP2", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setP2, 0), "setP2(P2) -> None\n."},
    {"setPreFilterCap", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setPreFilterCap, 0), "setPreFilterCap(preFilterCap) -> None\n."},
    {"setUniquenessRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_StereoSGBM_setUniquenessRatio, 0), "setUniquenessRatio(uniquenessRatio) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_StereoSGBM_specials(void)
{
    pyopencv_StereoSGBM_Type.tp_base = &pyopencv_StereoMatcher_Type;
    pyopencv_StereoSGBM_Type.tp_dealloc = pyopencv_StereoSGBM_dealloc;
    pyopencv_StereoSGBM_Type.tp_repr = pyopencv_StereoSGBM_repr;
    pyopencv_StereoSGBM_Type.tp_getset = pyopencv_StereoSGBM_getseters;
    pyopencv_StereoSGBM_Type.tp_init = (initproc)0;
    pyopencv_StereoSGBM_Type.tp_methods = pyopencv_StereoSGBM_methods;
}

static PyObject* pyopencv_rgbd_RgbdNormals_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_RgbdNormals %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_RgbdNormals_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    {
    int rows=0;
    int cols=0;
    int depth=0;
    PyObject* pyobj_K = NULL;
    Mat K;
    int window_size=5;
    int method=RgbdNormals::RGBD_NORMALS_METHOD_FALS;
    Ptr<RgbdNormals> retval;

    const char* keywords[] = { "rows", "cols", "depth", "K", "window_size", "method", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiiO|ii:rgbd_RgbdNormals.create", (char**)keywords, &rows, &cols, &depth, &pyobj_K, &window_size, &method) &&
        pyopencv_to(pyobj_K, K, ArgInfo("K", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdNormals::create(rows, cols, depth, K, window_size, method));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    int rows=0;
    int cols=0;
    int depth=0;
    PyObject* pyobj_K = NULL;
    UMat K;
    int window_size=5;
    int method=RgbdNormals::RGBD_NORMALS_METHOD_FALS;
    Ptr<RgbdNormals> retval;

    const char* keywords[] = { "rows", "cols", "depth", "K", "window_size", "method", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiiO|ii:rgbd_RgbdNormals.create", (char**)keywords, &rows, &cols, &depth, &pyobj_K, &window_size, &method) &&
        pyopencv_to(pyobj_K, K, ArgInfo("K", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdNormals::create(rows, cols, depth, K, window_size, method));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_getCols(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCols());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_getDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_getK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getK());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_getMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMethod());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_getRows(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getRows());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_getWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWindowSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_initialize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->initialize());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_setCols(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdNormals.setCols", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setCols(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_setDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdNormals.setDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_setK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdNormals.setK", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setK(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdNormals.setK", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setK(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_setMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdNormals.setMethod", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMethod(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_setRows(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdNormals.setRows", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setRows(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdNormals_setWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdNormals* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdNormals_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdNormals*>(((pyopencv_rgbd_RgbdNormals_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdNormals' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdNormals.setWindowSize", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setWindowSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_RgbdNormals_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_create_cls, METH_CLASS), "create(rows, cols, depth, K[, window_size[, method]]) -> retval\n.   Constructor\n.   * @param rows the number of rows of the depth image normals will be computed on\n.   * @param cols the number of cols of the depth image normals will be computed on\n.   * @param depth the depth of the normals (only CV_32F or CV_64F)\n.   * @param K the calibration matrix to use\n.   * @param window_size the window size to compute the normals: can only be 1,3,5 or 7\n.   * @param method one of the methods to use: RGBD_NORMALS_METHOD_SRI, RGBD_NORMALS_METHOD_FALS"},
    {"getCols", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_getCols, 0), "getCols() -> retval\n."},
    {"getDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_getDepth, 0), "getDepth() -> retval\n."},
    {"getK", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_getK, 0), "getK() -> retval\n."},
    {"getMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_getMethod, 0), "getMethod() -> retval\n."},
    {"getRows", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_getRows, 0), "getRows() -> retval\n."},
    {"getWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_getWindowSize, 0), "getWindowSize() -> retval\n."},
    {"initialize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_initialize, 0), "initialize() -> None\n.   Initializes some data that is cached for later computation\n.   * If that function is not called, it will be called the first time normals are computed"},
    {"setCols", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_setCols, 0), "setCols(val) -> None\n."},
    {"setDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_setDepth, 0), "setDepth(val) -> None\n."},
    {"setK", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_setK, 0), "setK(val) -> None\n."},
    {"setMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_setMethod, 0), "setMethod(val) -> None\n."},
    {"setRows", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_setRows, 0), "setRows(val) -> None\n."},
    {"setWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdNormals_setWindowSize, 0), "setWindowSize(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_RgbdNormals_specials(void)
{
    pyopencv_rgbd_RgbdNormals_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_rgbd_RgbdNormals_Type.tp_dealloc = pyopencv_rgbd_RgbdNormals_dealloc;
    pyopencv_rgbd_RgbdNormals_Type.tp_repr = pyopencv_rgbd_RgbdNormals_repr;
    pyopencv_rgbd_RgbdNormals_Type.tp_getset = pyopencv_rgbd_RgbdNormals_getseters;
    pyopencv_rgbd_RgbdNormals_Type.tp_init = (initproc)0;
    pyopencv_rgbd_RgbdNormals_Type.tp_methods = pyopencv_rgbd_RgbdNormals_methods;
}

static PyObject* pyopencv_rgbd_DepthCleaner_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_DepthCleaner %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_DepthCleaner_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    int depth=0;
    int window_size=5;
    int method=DepthCleaner::DEPTH_CLEANER_NIL;
    Ptr<DepthCleaner> retval;

    const char* keywords[] = { "depth", "window_size", "method", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i|ii:rgbd_DepthCleaner.create", (char**)keywords, &depth, &window_size, &method) )
    {
        ERRWRAP2(retval = cv::rgbd::DepthCleaner::create(depth, window_size, method));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_getDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_getMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMethod());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_getWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWindowSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_initialize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->initialize());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_setDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_DepthCleaner.setDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_setMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_DepthCleaner.setMethod", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMethod(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_DepthCleaner_setWindowSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::DepthCleaner* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_DepthCleaner_Type))
        _self_ = dynamic_cast<cv::rgbd::DepthCleaner*>(((pyopencv_rgbd_DepthCleaner_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_DepthCleaner' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_DepthCleaner.setWindowSize", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setWindowSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_DepthCleaner_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_create_cls, METH_CLASS), "create(depth[, window_size[, method]]) -> retval\n.   Constructor\n.   * @param depth the depth of the normals (only CV_32F or CV_64F)\n.   * @param window_size the window size to compute the normals: can only be 1,3,5 or 7\n.   * @param method one of the methods to use: RGBD_NORMALS_METHOD_SRI, RGBD_NORMALS_METHOD_FALS"},
    {"getDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_getDepth, 0), "getDepth() -> retval\n."},
    {"getMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_getMethod, 0), "getMethod() -> retval\n."},
    {"getWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_getWindowSize, 0), "getWindowSize() -> retval\n."},
    {"initialize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_initialize, 0), "initialize() -> None\n.   Initializes some data that is cached for later computation\n.   * If that function is not called, it will be called the first time normals are computed"},
    {"setDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_setDepth, 0), "setDepth(val) -> None\n."},
    {"setMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_setMethod, 0), "setMethod(val) -> None\n."},
    {"setWindowSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_DepthCleaner_setWindowSize, 0), "setWindowSize(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_DepthCleaner_specials(void)
{
    pyopencv_rgbd_DepthCleaner_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_rgbd_DepthCleaner_Type.tp_dealloc = pyopencv_rgbd_DepthCleaner_dealloc;
    pyopencv_rgbd_DepthCleaner_Type.tp_repr = pyopencv_rgbd_DepthCleaner_repr;
    pyopencv_rgbd_DepthCleaner_Type.tp_getset = pyopencv_rgbd_DepthCleaner_getseters;
    pyopencv_rgbd_DepthCleaner_Type.tp_init = (initproc)0;
    pyopencv_rgbd_DepthCleaner_Type.tp_methods = pyopencv_rgbd_DepthCleaner_methods;
}

static PyObject* pyopencv_rgbd_RgbdPlane_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_RgbdPlane %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_RgbdPlane_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBlockSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMethod());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getMinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getSensorErrorA(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSensorErrorA());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getSensorErrorB(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSensorErrorB());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getSensorErrorC(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSensorErrorC());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_getThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setBlockSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdPlane.setBlockSize", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setBlockSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setMethod(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdPlane.setMethod", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMethod(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setMinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdPlane.setMinSize", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMinSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setSensorErrorA(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdPlane.setSensorErrorA", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setSensorErrorA(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setSensorErrorB(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdPlane.setSensorErrorB", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setSensorErrorB(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setSensorErrorC(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdPlane.setSensorErrorC", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setSensorErrorC(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdPlane_setThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdPlane* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdPlane_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdPlane*>(((pyopencv_rgbd_RgbdPlane_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdPlane' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdPlane.setThreshold", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setThreshold(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_RgbdPlane_methods[] =
{
    {"getBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getBlockSize, 0), "getBlockSize() -> retval\n.   Find The planes in a depth image but without doing a normal check, which is faster but less accurate\n.   * @param points3d the 3d points organized like the depth image: rows x cols with 3 channels\n.   * @param mask An image where each pixel is labeled with the plane it belongs to\n.   *        and 255 if it does not belong to any plane\n.   * @param plane_coefficients the coefficients of the corresponding planes (a,b,c,d) such that ax+by+cz+d=0"},
    {"getMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getMethod, 0), "getMethod() -> retval\n."},
    {"getMinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getMinSize, 0), "getMinSize() -> retval\n."},
    {"getSensorErrorA", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getSensorErrorA, 0), "getSensorErrorA() -> retval\n."},
    {"getSensorErrorB", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getSensorErrorB, 0), "getSensorErrorB() -> retval\n."},
    {"getSensorErrorC", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getSensorErrorC, 0), "getSensorErrorC() -> retval\n."},
    {"getThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_getThreshold, 0), "getThreshold() -> retval\n."},
    {"setBlockSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setBlockSize, 0), "setBlockSize(val) -> None\n."},
    {"setMethod", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setMethod, 0), "setMethod(val) -> None\n."},
    {"setMinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setMinSize, 0), "setMinSize(val) -> None\n."},
    {"setSensorErrorA", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setSensorErrorA, 0), "setSensorErrorA(val) -> None\n."},
    {"setSensorErrorB", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setSensorErrorB, 0), "setSensorErrorB(val) -> None\n."},
    {"setSensorErrorC", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setSensorErrorC, 0), "setSensorErrorC(val) -> None\n."},
    {"setThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdPlane_setThreshold, 0), "setThreshold(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_RgbdPlane_specials(void)
{
    pyopencv_rgbd_RgbdPlane_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_rgbd_RgbdPlane_Type.tp_dealloc = pyopencv_rgbd_RgbdPlane_dealloc;
    pyopencv_rgbd_RgbdPlane_Type.tp_repr = pyopencv_rgbd_RgbdPlane_repr;
    pyopencv_rgbd_RgbdPlane_Type.tp_getset = pyopencv_rgbd_RgbdPlane_getseters;
    pyopencv_rgbd_RgbdPlane_Type.tp_init = (initproc)0;
    pyopencv_rgbd_RgbdPlane_Type.tp_methods = pyopencv_rgbd_RgbdPlane_methods;
}

static PyObject* pyopencv_rgbd_RgbdFrame_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_RgbdFrame %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_rgbd_RgbdFrame_get_ID(pyopencv_rgbd_RgbdFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->ID);
}

static PyObject* pyopencv_rgbd_RgbdFrame_get_depth(pyopencv_rgbd_RgbdFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->depth);
}

static PyObject* pyopencv_rgbd_RgbdFrame_get_image(pyopencv_rgbd_RgbdFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->image);
}

static PyObject* pyopencv_rgbd_RgbdFrame_get_mask(pyopencv_rgbd_RgbdFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->mask);
}

static PyObject* pyopencv_rgbd_RgbdFrame_get_normals(pyopencv_rgbd_RgbdFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->normals);
}


static PyGetSetDef pyopencv_rgbd_RgbdFrame_getseters[] =
{
    {(char*)"ID", (getter)pyopencv_rgbd_RgbdFrame_get_ID, NULL, (char*)"ID", NULL},
    {(char*)"depth", (getter)pyopencv_rgbd_RgbdFrame_get_depth, NULL, (char*)"depth", NULL},
    {(char*)"image", (getter)pyopencv_rgbd_RgbdFrame_get_image, NULL, (char*)"image", NULL},
    {(char*)"mask", (getter)pyopencv_rgbd_RgbdFrame_get_mask, NULL, (char*)"mask", NULL},
    {(char*)"normals", (getter)pyopencv_rgbd_RgbdFrame_get_normals, NULL, (char*)"normals", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdFrame_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_depth = NULL;
    Mat depth;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    PyObject* pyobj_normals = NULL;
    Mat normals;
    int ID=-1;
    Ptr<RgbdFrame> retval;

    const char* keywords[] = { "image", "depth", "mask", "normals", "ID", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OOOOi:rgbd_RgbdFrame.create", (char**)keywords, &pyobj_image, &pyobj_depth, &pyobj_mask, &pyobj_normals, &ID) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_depth, depth, ArgInfo("depth", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) &&
        pyopencv_to(pyobj_normals, normals, ArgInfo("normals", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdFrame::create(image, depth, mask, normals, ID));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_depth = NULL;
    Mat depth;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    PyObject* pyobj_normals = NULL;
    Mat normals;
    int ID=-1;
    Ptr<RgbdFrame> retval;

    const char* keywords[] = { "image", "depth", "mask", "normals", "ID", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OOOOi:rgbd_RgbdFrame.create", (char**)keywords, &pyobj_image, &pyobj_depth, &pyobj_mask, &pyobj_normals, &ID) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_depth, depth, ArgInfo("depth", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) &&
        pyopencv_to(pyobj_normals, normals, ArgInfo("normals", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdFrame::create(image, depth, mask, normals, ID));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdFrame_release(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdFrame* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdFrame_Type))
        _self_ = ((pyopencv_rgbd_RgbdFrame_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdFrame' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_RgbdFrame_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdFrame_create_cls, METH_CLASS), "create([, image[, depth[, mask[, normals[, ID]]]]]) -> retval\n."},
    {"release", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdFrame_release, 0), "release() -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_RgbdFrame_specials(void)
{
    pyopencv_rgbd_RgbdFrame_Type.tp_base = NULL;
    pyopencv_rgbd_RgbdFrame_Type.tp_dealloc = pyopencv_rgbd_RgbdFrame_dealloc;
    pyopencv_rgbd_RgbdFrame_Type.tp_repr = pyopencv_rgbd_RgbdFrame_repr;
    pyopencv_rgbd_RgbdFrame_Type.tp_getset = pyopencv_rgbd_RgbdFrame_getseters;
    pyopencv_rgbd_RgbdFrame_Type.tp_init = (initproc)0;
    pyopencv_rgbd_RgbdFrame_Type.tp_methods = pyopencv_rgbd_RgbdFrame_methods;
}

static PyObject* pyopencv_rgbd_OdometryFrame_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_OdometryFrame %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidCloud(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidCloud);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidDepth(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidDepth);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidImage(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidImage);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidMask(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidMask);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidNormals(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidNormals);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidNormalsMask(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidNormalsMask);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramidTexturedMask(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramidTexturedMask);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramid_dI_dx(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramid_dI_dx);
}

static PyObject* pyopencv_rgbd_OdometryFrame_get_pyramid_dI_dy(pyopencv_rgbd_OdometryFrame_t* p, void *closure)
{
    return pyopencv_from(p->v->pyramid_dI_dy);
}


static PyGetSetDef pyopencv_rgbd_OdometryFrame_getseters[] =
{
    {(char*)"pyramidCloud", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidCloud, NULL, (char*)"pyramidCloud", NULL},
    {(char*)"pyramidDepth", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidDepth, NULL, (char*)"pyramidDepth", NULL},
    {(char*)"pyramidImage", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidImage, NULL, (char*)"pyramidImage", NULL},
    {(char*)"pyramidMask", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidMask, NULL, (char*)"pyramidMask", NULL},
    {(char*)"pyramidNormals", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidNormals, NULL, (char*)"pyramidNormals", NULL},
    {(char*)"pyramidNormalsMask", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidNormalsMask, NULL, (char*)"pyramidNormalsMask", NULL},
    {(char*)"pyramidTexturedMask", (getter)pyopencv_rgbd_OdometryFrame_get_pyramidTexturedMask, NULL, (char*)"pyramidTexturedMask", NULL},
    {(char*)"pyramid_dI_dx", (getter)pyopencv_rgbd_OdometryFrame_get_pyramid_dI_dx, NULL, (char*)"pyramid_dI_dx", NULL},
    {(char*)"pyramid_dI_dy", (getter)pyopencv_rgbd_OdometryFrame_get_pyramid_dI_dy, NULL, (char*)"pyramid_dI_dy", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_OdometryFrame_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_depth = NULL;
    Mat depth;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    PyObject* pyobj_normals = NULL;
    Mat normals;
    int ID=-1;
    Ptr<OdometryFrame> retval;

    const char* keywords[] = { "image", "depth", "mask", "normals", "ID", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OOOOi:rgbd_OdometryFrame.create", (char**)keywords, &pyobj_image, &pyobj_depth, &pyobj_mask, &pyobj_normals, &ID) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_depth, depth, ArgInfo("depth", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) &&
        pyopencv_to(pyobj_normals, normals, ArgInfo("normals", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::OdometryFrame::create(image, depth, mask, normals, ID));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_depth = NULL;
    Mat depth;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    PyObject* pyobj_normals = NULL;
    Mat normals;
    int ID=-1;
    Ptr<OdometryFrame> retval;

    const char* keywords[] = { "image", "depth", "mask", "normals", "ID", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OOOOi:rgbd_OdometryFrame.create", (char**)keywords, &pyobj_image, &pyobj_depth, &pyobj_mask, &pyobj_normals, &ID) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_depth, depth, ArgInfo("depth", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) &&
        pyopencv_to(pyobj_normals, normals, ArgInfo("normals", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::OdometryFrame::create(image, depth, mask, normals, ID));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_OdometryFrame_release(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::OdometryFrame* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_OdometryFrame_Type))
        _self_ = ((pyopencv_rgbd_OdometryFrame_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_OdometryFrame' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->release());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_OdometryFrame_releasePyramids(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::OdometryFrame* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_OdometryFrame_Type))
        _self_ = ((pyopencv_rgbd_OdometryFrame_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_OdometryFrame' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->releasePyramids());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_OdometryFrame_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_OdometryFrame_create_cls, METH_CLASS), "create([, image[, depth[, mask[, normals[, ID]]]]]) -> retval\n."},
    {"release", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_OdometryFrame_release, 0), "release() -> None\n."},
    {"releasePyramids", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_OdometryFrame_releasePyramids, 0), "releasePyramids() -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_OdometryFrame_specials(void)
{
    pyopencv_rgbd_OdometryFrame_Type.tp_base = &pyopencv_rgbd_RgbdFrame_Type;
    pyopencv_rgbd_OdometryFrame_Type.tp_dealloc = pyopencv_rgbd_OdometryFrame_dealloc;
    pyopencv_rgbd_OdometryFrame_Type.tp_repr = pyopencv_rgbd_OdometryFrame_repr;
    pyopencv_rgbd_OdometryFrame_Type.tp_getset = pyopencv_rgbd_OdometryFrame_getseters;
    pyopencv_rgbd_OdometryFrame_Type.tp_init = (initproc)0;
    pyopencv_rgbd_OdometryFrame_Type.tp_methods = pyopencv_rgbd_OdometryFrame_methods;
}

static PyObject* pyopencv_rgbd_Odometry_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_Odometry %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_Odometry_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_DEPTH(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->DEFAULT_MAX_DEPTH());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_DEPTH_DIFF(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->DEFAULT_MAX_DEPTH_DIFF());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_POINTS_PART(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->DEFAULT_MAX_POINTS_PART());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_ROTATION(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->DEFAULT_MAX_ROTATION());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_TRANSLATION(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->DEFAULT_MAX_TRANSLATION());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MIN_DEPTH(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->DEFAULT_MIN_DEPTH());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_compute(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    {
    PyObject* pyobj_srcImage = NULL;
    Mat srcImage;
    PyObject* pyobj_srcDepth = NULL;
    Mat srcDepth;
    PyObject* pyobj_srcMask = NULL;
    Mat srcMask;
    PyObject* pyobj_dstImage = NULL;
    Mat dstImage;
    PyObject* pyobj_dstDepth = NULL;
    Mat dstDepth;
    PyObject* pyobj_dstMask = NULL;
    Mat dstMask;
    PyObject* pyobj_Rt = NULL;
    Mat Rt;
    PyObject* pyobj_initRt = NULL;
    Mat initRt;
    bool retval;

    const char* keywords[] = { "srcImage", "srcDepth", "srcMask", "dstImage", "dstDepth", "dstMask", "Rt", "initRt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOOOO|OO:rgbd_Odometry.compute", (char**)keywords, &pyobj_srcImage, &pyobj_srcDepth, &pyobj_srcMask, &pyobj_dstImage, &pyobj_dstDepth, &pyobj_dstMask, &pyobj_Rt, &pyobj_initRt) &&
        pyopencv_to(pyobj_srcImage, srcImage, ArgInfo("srcImage", 0)) &&
        pyopencv_to(pyobj_srcDepth, srcDepth, ArgInfo("srcDepth", 0)) &&
        pyopencv_to(pyobj_srcMask, srcMask, ArgInfo("srcMask", 0)) &&
        pyopencv_to(pyobj_dstImage, dstImage, ArgInfo("dstImage", 0)) &&
        pyopencv_to(pyobj_dstDepth, dstDepth, ArgInfo("dstDepth", 0)) &&
        pyopencv_to(pyobj_dstMask, dstMask, ArgInfo("dstMask", 0)) &&
        pyopencv_to(pyobj_Rt, Rt, ArgInfo("Rt", 1)) &&
        pyopencv_to(pyobj_initRt, initRt, ArgInfo("initRt", 0)) )
    {
        ERRWRAP2(retval = _self_->compute(srcImage, srcDepth, srcMask, dstImage, dstDepth, dstMask, Rt, initRt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(Rt));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_srcImage = NULL;
    Mat srcImage;
    PyObject* pyobj_srcDepth = NULL;
    Mat srcDepth;
    PyObject* pyobj_srcMask = NULL;
    Mat srcMask;
    PyObject* pyobj_dstImage = NULL;
    Mat dstImage;
    PyObject* pyobj_dstDepth = NULL;
    Mat dstDepth;
    PyObject* pyobj_dstMask = NULL;
    Mat dstMask;
    PyObject* pyobj_Rt = NULL;
    UMat Rt;
    PyObject* pyobj_initRt = NULL;
    Mat initRt;
    bool retval;

    const char* keywords[] = { "srcImage", "srcDepth", "srcMask", "dstImage", "dstDepth", "dstMask", "Rt", "initRt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOOOO|OO:rgbd_Odometry.compute", (char**)keywords, &pyobj_srcImage, &pyobj_srcDepth, &pyobj_srcMask, &pyobj_dstImage, &pyobj_dstDepth, &pyobj_dstMask, &pyobj_Rt, &pyobj_initRt) &&
        pyopencv_to(pyobj_srcImage, srcImage, ArgInfo("srcImage", 0)) &&
        pyopencv_to(pyobj_srcDepth, srcDepth, ArgInfo("srcDepth", 0)) &&
        pyopencv_to(pyobj_srcMask, srcMask, ArgInfo("srcMask", 0)) &&
        pyopencv_to(pyobj_dstImage, dstImage, ArgInfo("dstImage", 0)) &&
        pyopencv_to(pyobj_dstDepth, dstDepth, ArgInfo("dstDepth", 0)) &&
        pyopencv_to(pyobj_dstMask, dstMask, ArgInfo("dstMask", 0)) &&
        pyopencv_to(pyobj_Rt, Rt, ArgInfo("Rt", 1)) &&
        pyopencv_to(pyobj_initRt, initRt, ArgInfo("initRt", 0)) )
    {
        ERRWRAP2(retval = _self_->compute(srcImage, srcDepth, srcMask, dstImage, dstDepth, dstMask, Rt, initRt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(Rt));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_compute2(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    {
    PyObject* pyobj_srcFrame = NULL;
    Ptr<OdometryFrame> srcFrame;
    PyObject* pyobj_dstFrame = NULL;
    Ptr<OdometryFrame> dstFrame;
    PyObject* pyobj_Rt = NULL;
    Mat Rt;
    PyObject* pyobj_initRt = NULL;
    Mat initRt;
    bool retval;

    const char* keywords[] = { "srcFrame", "dstFrame", "Rt", "initRt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OO:rgbd_Odometry.compute2", (char**)keywords, &pyobj_srcFrame, &pyobj_dstFrame, &pyobj_Rt, &pyobj_initRt) &&
        pyopencv_to(pyobj_srcFrame, srcFrame, ArgInfo("srcFrame", 0)) &&
        pyopencv_to(pyobj_dstFrame, dstFrame, ArgInfo("dstFrame", 0)) &&
        pyopencv_to(pyobj_Rt, Rt, ArgInfo("Rt", 1)) &&
        pyopencv_to(pyobj_initRt, initRt, ArgInfo("initRt", 0)) )
    {
        ERRWRAP2(retval = _self_->compute(srcFrame, dstFrame, Rt, initRt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(Rt));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_srcFrame = NULL;
    Ptr<OdometryFrame> srcFrame;
    PyObject* pyobj_dstFrame = NULL;
    Ptr<OdometryFrame> dstFrame;
    PyObject* pyobj_Rt = NULL;
    UMat Rt;
    PyObject* pyobj_initRt = NULL;
    Mat initRt;
    bool retval;

    const char* keywords[] = { "srcFrame", "dstFrame", "Rt", "initRt", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OO:rgbd_Odometry.compute2", (char**)keywords, &pyobj_srcFrame, &pyobj_dstFrame, &pyobj_Rt, &pyobj_initRt) &&
        pyopencv_to(pyobj_srcFrame, srcFrame, ArgInfo("srcFrame", 0)) &&
        pyopencv_to(pyobj_dstFrame, dstFrame, ArgInfo("dstFrame", 0)) &&
        pyopencv_to(pyobj_Rt, Rt, ArgInfo("Rt", 1)) &&
        pyopencv_to(pyobj_initRt, initRt, ArgInfo("initRt", 0)) )
    {
        ERRWRAP2(retval = _self_->compute(srcFrame, dstFrame, Rt, initRt));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(Rt));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    PyObject* pyobj_odometryType = NULL;
    String odometryType;
    Ptr<Odometry> retval;

    const char* keywords[] = { "odometryType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_Odometry.create", (char**)keywords, &pyobj_odometryType) &&
        pyopencv_to(pyobj_odometryType, odometryType, ArgInfo("odometryType", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::Odometry::create(odometryType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_getCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCameraMatrix());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_getTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTransformType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_prepareFrameCache(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    PyObject* pyobj_frame = NULL;
    Ptr<OdometryFrame> frame;
    int cacheType=0;
    Size retval;

    const char* keywords[] = { "frame", "cacheType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:rgbd_Odometry.prepareFrameCache", (char**)keywords, &pyobj_frame, &cacheType) &&
        pyopencv_to(pyobj_frame, frame, ArgInfo("frame", 0)) )
    {
        ERRWRAP2(retval = _self_->prepareFrameCache(frame, cacheType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_setCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_Odometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_Odometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_Odometry_setTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::Odometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_Odometry_Type))
        _self_ = dynamic_cast<cv::rgbd::Odometry*>(((pyopencv_rgbd_Odometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_Odometry' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_Odometry.setTransformType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTransformType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_Odometry_methods[] =
{
    {"DEFAULT_MAX_DEPTH", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_DEPTH, 0), "DEFAULT_MAX_DEPTH() -> retval\n."},
    {"DEFAULT_MAX_DEPTH_DIFF", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_DEPTH_DIFF, 0), "DEFAULT_MAX_DEPTH_DIFF() -> retval\n."},
    {"DEFAULT_MAX_POINTS_PART", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_POINTS_PART, 0), "DEFAULT_MAX_POINTS_PART() -> retval\n."},
    {"DEFAULT_MAX_ROTATION", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_ROTATION, 0), "DEFAULT_MAX_ROTATION() -> retval\n."},
    {"DEFAULT_MAX_TRANSLATION", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MAX_TRANSLATION, 0), "DEFAULT_MAX_TRANSLATION() -> retval\n."},
    {"DEFAULT_MIN_DEPTH", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_DEFAULT_MIN_DEPTH, 0), "DEFAULT_MIN_DEPTH() -> retval\n."},
    {"compute", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_compute, 0), "compute(srcImage, srcDepth, srcMask, dstImage, dstDepth, dstMask[, Rt[, initRt]]) -> retval, Rt\n.   Method to compute a transformation from the source frame to the destination one.\n.   * Some odometry algorithms do not used some data of frames (eg. ICP does not use images).\n.   * In such case corresponding arguments can be set as empty Mat.\n.   * The method returns true if all internal computions were possible (e.g. there were enough correspondences,\n.   * system of equations has a solution, etc) and resulting transformation satisfies some test if it's provided\n.   * by the Odometry inheritor implementation (e.g. thresholds for maximum translation and rotation).\n.   * @param srcImage Image data of the source frame (CV_8UC1)\n.   * @param srcDepth Depth data of the source frame (CV_32FC1, in meters)\n.   * @param srcMask Mask that sets which pixels have to be used from the source frame (CV_8UC1)\n.   * @param dstImage Image data of the destination frame (CV_8UC1)\n.   * @param dstDepth Depth data of the destination frame (CV_32FC1, in meters)\n.   * @param dstMask Mask that sets which pixels have to be used from the destination frame (CV_8UC1)\n.   * @param Rt Resulting transformation from the source frame to the destination one (rigid body motion):\n.   dst_p = Rt * src_p, where dst_p is a homogeneous point in the destination frame and src_p is\n.   homogeneous point in the source frame,\n.   Rt is 4x4 matrix of CV_64FC1 type.\n.   * @param initRt Initial transformation from the source frame to the destination one (optional)"},
    {"compute2", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_compute2, 0), "compute2(srcFrame, dstFrame[, Rt[, initRt]]) -> retval, Rt\n.   One more method to compute a transformation from the source frame to the destination one.\n.   * It is designed to save on computing the frame data (image pyramids, normals, etc.)."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_create_cls, METH_CLASS), "create(odometryType) -> retval\n."},
    {"getCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_getCameraMatrix, 0), "getCameraMatrix() -> retval\n.   @see setCameraMatrix"},
    {"getTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_getTransformType, 0), "getTransformType() -> retval\n.   @see setTransformType"},
    {"prepareFrameCache", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_prepareFrameCache, 0), "prepareFrameCache(frame, cacheType) -> retval\n.   Prepare a cache for the frame. The function checks the precomputed/passed data (throws the error if this data\n.   * does not satisfy) and computes all remaining cache data needed for the frame. Returned size is a resolution\n.   * of the prepared frame.\n.   * @param frame The odometry which will process the frame.\n.   * @param cacheType The cache type: CACHE_SRC, CACHE_DST or CACHE_ALL."},
    {"setCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_setCameraMatrix, 0), "setCameraMatrix(val) -> None\n.   @copybrief getCameraMatrix @see getCameraMatrix"},
    {"setTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_Odometry_setTransformType, 0), "setTransformType(val) -> None\n.   @copybrief getTransformType @see getTransformType"},

    {NULL,          NULL}
};

static void pyopencv_rgbd_Odometry_specials(void)
{
    pyopencv_rgbd_Odometry_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_rgbd_Odometry_Type.tp_dealloc = pyopencv_rgbd_Odometry_dealloc;
    pyopencv_rgbd_Odometry_Type.tp_repr = pyopencv_rgbd_Odometry_repr;
    pyopencv_rgbd_Odometry_Type.tp_getset = pyopencv_rgbd_Odometry_getseters;
    pyopencv_rgbd_Odometry_Type.tp_init = (initproc)0;
    pyopencv_rgbd_Odometry_Type.tp_methods = pyopencv_rgbd_Odometry_methods;
}

static PyObject* pyopencv_rgbd_RgbdOdometry_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_RgbdOdometry %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_RgbdOdometry_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    {
    PyObject* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    float minDepth=Odometry::DEFAULT_MIN_DEPTH();
    float maxDepth=Odometry::DEFAULT_MAX_DEPTH();
    float maxDepthDiff=Odometry::DEFAULT_MAX_DEPTH_DIFF();
    PyObject* pyobj_iterCounts = NULL;
    vector_int iterCounts=std::vector<int>();
    PyObject* pyobj_minGradientMagnitudes = NULL;
    vector_float minGradientMagnitudes=std::vector<float>();
    float maxPointsPart=Odometry::DEFAULT_MAX_POINTS_PART();
    int transformType=Odometry::RIGID_BODY_MOTION;
    Ptr<RgbdOdometry> retval;

    const char* keywords[] = { "cameraMatrix", "minDepth", "maxDepth", "maxDepthDiff", "iterCounts", "minGradientMagnitudes", "maxPointsPart", "transformType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OfffOOfi:rgbd_RgbdOdometry.create", (char**)keywords, &pyobj_cameraMatrix, &minDepth, &maxDepth, &maxDepthDiff, &pyobj_iterCounts, &pyobj_minGradientMagnitudes, &maxPointsPart, &transformType) &&
        pyopencv_to(pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        pyopencv_to(pyobj_iterCounts, iterCounts, ArgInfo("iterCounts", 0)) &&
        pyopencv_to(pyobj_minGradientMagnitudes, minGradientMagnitudes, ArgInfo("minGradientMagnitudes", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdOdometry::create(cameraMatrix, minDepth, maxDepth, maxDepthDiff, iterCounts, minGradientMagnitudes, maxPointsPart, transformType));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    float minDepth=Odometry::DEFAULT_MIN_DEPTH();
    float maxDepth=Odometry::DEFAULT_MAX_DEPTH();
    float maxDepthDiff=Odometry::DEFAULT_MAX_DEPTH_DIFF();
    PyObject* pyobj_iterCounts = NULL;
    vector_int iterCounts=std::vector<int>();
    PyObject* pyobj_minGradientMagnitudes = NULL;
    vector_float minGradientMagnitudes=std::vector<float>();
    float maxPointsPart=Odometry::DEFAULT_MAX_POINTS_PART();
    int transformType=Odometry::RIGID_BODY_MOTION;
    Ptr<RgbdOdometry> retval;

    const char* keywords[] = { "cameraMatrix", "minDepth", "maxDepth", "maxDepthDiff", "iterCounts", "minGradientMagnitudes", "maxPointsPart", "transformType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OfffOOfi:rgbd_RgbdOdometry.create", (char**)keywords, &pyobj_cameraMatrix, &minDepth, &maxDepth, &maxDepthDiff, &pyobj_iterCounts, &pyobj_minGradientMagnitudes, &maxPointsPart, &transformType) &&
        pyopencv_to(pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        pyopencv_to(pyobj_iterCounts, iterCounts, ArgInfo("iterCounts", 0)) &&
        pyopencv_to(pyobj_minGradientMagnitudes, minGradientMagnitudes, ArgInfo("minGradientMagnitudes", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdOdometry::create(cameraMatrix, minDepth, maxDepth, maxDepthDiff, iterCounts, minGradientMagnitudes, maxPointsPart, transformType));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCameraMatrix());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getIterationCounts(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIterationCounts());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxDepthDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepthDiff());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxPointsPart(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxPointsPart());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxRotation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxRotation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxTranslation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMinDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMinGradientMagnitudes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinGradientMagnitudes());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_getTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTransformType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_prepareFrameCache(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    PyObject* pyobj_frame = NULL;
    Ptr<OdometryFrame> frame;
    int cacheType=0;
    Size retval;

    const char* keywords[] = { "frame", "cacheType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:rgbd_RgbdOdometry.prepareFrameCache", (char**)keywords, &pyobj_frame, &cacheType) &&
        pyopencv_to(pyobj_frame, frame, ArgInfo("frame", 0)) )
    {
        ERRWRAP2(retval = _self_->prepareFrameCache(frame, cacheType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdOdometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdOdometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setIterationCounts(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdOdometry.setIterationCounts", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setIterationCounts(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdOdometry.setIterationCounts", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setIterationCounts(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdOdometry.setMaxDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxDepthDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdOdometry.setMaxDepthDiff", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepthDiff(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxPointsPart(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdOdometry.setMaxPointsPart", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxPointsPart(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxRotation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdOdometry.setMaxRotation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxRotation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdOdometry.setMaxTranslation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxTranslation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMinDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdOdometry.setMinDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMinDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMinGradientMagnitudes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdOdometry.setMinGradientMagnitudes", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setMinGradientMagnitudes(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdOdometry.setMinGradientMagnitudes", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setMinGradientMagnitudes(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdOdometry_setTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdOdometry*>(((pyopencv_rgbd_RgbdOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdOdometry' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdOdometry.setTransformType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTransformType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_RgbdOdometry_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_create_cls, METH_CLASS), "create([, cameraMatrix[, minDepth[, maxDepth[, maxDepthDiff[, iterCounts[, minGradientMagnitudes[, maxPointsPart[, transformType]]]]]]]]) -> retval\n.   Constructor.\n.   * @param cameraMatrix Camera matrix\n.   * @param minDepth Pixels with depth less than minDepth will not be used (in meters)\n.   * @param maxDepth Pixels with depth larger than maxDepth will not be used (in meters)\n.   * @param maxDepthDiff Correspondences between pixels of two given frames will be filtered out\n.   *                     if their depth difference is larger than maxDepthDiff (in meters)\n.   * @param iterCounts Count of iterations on each pyramid level.\n.   * @param minGradientMagnitudes For each pyramid level the pixels will be filtered out\n.   *                              if they have gradient magnitude less than minGradientMagnitudes[level].\n.   * @param maxPointsPart The method uses a random pixels subset of size frameWidth x frameHeight x pointsPart\n.   * @param transformType Class of transformation"},
    {"getCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getCameraMatrix, 0), "getCameraMatrix() -> retval\n."},
    {"getIterationCounts", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getIterationCounts, 0), "getIterationCounts() -> retval\n."},
    {"getMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxDepth, 0), "getMaxDepth() -> retval\n."},
    {"getMaxDepthDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxDepthDiff, 0), "getMaxDepthDiff() -> retval\n."},
    {"getMaxPointsPart", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxPointsPart, 0), "getMaxPointsPart() -> retval\n."},
    {"getMaxRotation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxRotation, 0), "getMaxRotation() -> retval\n."},
    {"getMaxTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMaxTranslation, 0), "getMaxTranslation() -> retval\n."},
    {"getMinDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMinDepth, 0), "getMinDepth() -> retval\n."},
    {"getMinGradientMagnitudes", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getMinGradientMagnitudes, 0), "getMinGradientMagnitudes() -> retval\n."},
    {"getTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_getTransformType, 0), "getTransformType() -> retval\n."},
    {"prepareFrameCache", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_prepareFrameCache, 0), "prepareFrameCache(frame, cacheType) -> retval\n."},
    {"setCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setCameraMatrix, 0), "setCameraMatrix(val) -> None\n."},
    {"setIterationCounts", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setIterationCounts, 0), "setIterationCounts(val) -> None\n."},
    {"setMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxDepth, 0), "setMaxDepth(val) -> None\n."},
    {"setMaxDepthDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxDepthDiff, 0), "setMaxDepthDiff(val) -> None\n."},
    {"setMaxPointsPart", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxPointsPart, 0), "setMaxPointsPart(val) -> None\n."},
    {"setMaxRotation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxRotation, 0), "setMaxRotation(val) -> None\n."},
    {"setMaxTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMaxTranslation, 0), "setMaxTranslation(val) -> None\n."},
    {"setMinDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMinDepth, 0), "setMinDepth(val) -> None\n."},
    {"setMinGradientMagnitudes", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setMinGradientMagnitudes, 0), "setMinGradientMagnitudes(val) -> None\n."},
    {"setTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdOdometry_setTransformType, 0), "setTransformType(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_RgbdOdometry_specials(void)
{
    pyopencv_rgbd_RgbdOdometry_Type.tp_base = &pyopencv_rgbd_Odometry_Type;
    pyopencv_rgbd_RgbdOdometry_Type.tp_dealloc = pyopencv_rgbd_RgbdOdometry_dealloc;
    pyopencv_rgbd_RgbdOdometry_Type.tp_repr = pyopencv_rgbd_RgbdOdometry_repr;
    pyopencv_rgbd_RgbdOdometry_Type.tp_getset = pyopencv_rgbd_RgbdOdometry_getseters;
    pyopencv_rgbd_RgbdOdometry_Type.tp_init = (initproc)0;
    pyopencv_rgbd_RgbdOdometry_Type.tp_methods = pyopencv_rgbd_RgbdOdometry_methods;
}

static PyObject* pyopencv_rgbd_ICPOdometry_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_ICPOdometry %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_ICPOdometry_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    {
    PyObject* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    float minDepth=Odometry::DEFAULT_MIN_DEPTH();
    float maxDepth=Odometry::DEFAULT_MAX_DEPTH();
    float maxDepthDiff=Odometry::DEFAULT_MAX_DEPTH_DIFF();
    float maxPointsPart=Odometry::DEFAULT_MAX_POINTS_PART();
    PyObject* pyobj_iterCounts = NULL;
    vector_int iterCounts=std::vector<int>();
    int transformType=Odometry::RIGID_BODY_MOTION;
    Ptr<ICPOdometry> retval;

    const char* keywords[] = { "cameraMatrix", "minDepth", "maxDepth", "maxDepthDiff", "maxPointsPart", "iterCounts", "transformType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OffffOi:rgbd_ICPOdometry.create", (char**)keywords, &pyobj_cameraMatrix, &minDepth, &maxDepth, &maxDepthDiff, &maxPointsPart, &pyobj_iterCounts, &transformType) &&
        pyopencv_to(pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        pyopencv_to(pyobj_iterCounts, iterCounts, ArgInfo("iterCounts", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::ICPOdometry::create(cameraMatrix, minDepth, maxDepth, maxDepthDiff, maxPointsPart, iterCounts, transformType));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    float minDepth=Odometry::DEFAULT_MIN_DEPTH();
    float maxDepth=Odometry::DEFAULT_MAX_DEPTH();
    float maxDepthDiff=Odometry::DEFAULT_MAX_DEPTH_DIFF();
    float maxPointsPart=Odometry::DEFAULT_MAX_POINTS_PART();
    PyObject* pyobj_iterCounts = NULL;
    vector_int iterCounts=std::vector<int>();
    int transformType=Odometry::RIGID_BODY_MOTION;
    Ptr<ICPOdometry> retval;

    const char* keywords[] = { "cameraMatrix", "minDepth", "maxDepth", "maxDepthDiff", "maxPointsPart", "iterCounts", "transformType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OffffOi:rgbd_ICPOdometry.create", (char**)keywords, &pyobj_cameraMatrix, &minDepth, &maxDepth, &maxDepthDiff, &maxPointsPart, &pyobj_iterCounts, &transformType) &&
        pyopencv_to(pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        pyopencv_to(pyobj_iterCounts, iterCounts, ArgInfo("iterCounts", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::ICPOdometry::create(cameraMatrix, minDepth, maxDepth, maxDepthDiff, maxPointsPart, iterCounts, transformType));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCameraMatrix());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getIterationCounts(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIterationCounts());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxDepthDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepthDiff());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxPointsPart(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxPointsPart());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxRotation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxRotation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxTranslation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getMinDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getNormalsComputer(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    Ptr<RgbdNormals> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNormalsComputer());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_getTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTransformType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_prepareFrameCache(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    PyObject* pyobj_frame = NULL;
    Ptr<OdometryFrame> frame;
    int cacheType=0;
    Size retval;

    const char* keywords[] = { "frame", "cacheType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:rgbd_ICPOdometry.prepareFrameCache", (char**)keywords, &pyobj_frame, &cacheType) &&
        pyopencv_to(pyobj_frame, frame, ArgInfo("frame", 0)) )
    {
        ERRWRAP2(retval = _self_->prepareFrameCache(frame, cacheType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_ICPOdometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_ICPOdometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setIterationCounts(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_ICPOdometry.setIterationCounts", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setIterationCounts(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_ICPOdometry.setIterationCounts", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setIterationCounts(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_ICPOdometry.setMaxDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxDepthDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_ICPOdometry.setMaxDepthDiff", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepthDiff(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxPointsPart(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_ICPOdometry.setMaxPointsPart", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxPointsPart(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxRotation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_ICPOdometry.setMaxRotation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxRotation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_ICPOdometry.setMaxTranslation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxTranslation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setMinDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_ICPOdometry.setMinDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMinDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_ICPOdometry_setTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::ICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_ICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::ICPOdometry*>(((pyopencv_rgbd_ICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_ICPOdometry' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_ICPOdometry.setTransformType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTransformType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_ICPOdometry_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_create_cls, METH_CLASS), "create([, cameraMatrix[, minDepth[, maxDepth[, maxDepthDiff[, maxPointsPart[, iterCounts[, transformType]]]]]]]) -> retval\n.   Constructor.\n.   * @param cameraMatrix Camera matrix\n.   * @param minDepth Pixels with depth less than minDepth will not be used\n.   * @param maxDepth Pixels with depth larger than maxDepth will not be used\n.   * @param maxDepthDiff Correspondences between pixels of two given frames will be filtered out\n.   *                     if their depth difference is larger than maxDepthDiff\n.   * @param maxPointsPart The method uses a random pixels subset of size frameWidth x frameHeight x pointsPart\n.   * @param iterCounts Count of iterations on each pyramid level.\n.   * @param transformType Class of trasformation"},
    {"getCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getCameraMatrix, 0), "getCameraMatrix() -> retval\n."},
    {"getIterationCounts", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getIterationCounts, 0), "getIterationCounts() -> retval\n."},
    {"getMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxDepth, 0), "getMaxDepth() -> retval\n."},
    {"getMaxDepthDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxDepthDiff, 0), "getMaxDepthDiff() -> retval\n."},
    {"getMaxPointsPart", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxPointsPart, 0), "getMaxPointsPart() -> retval\n."},
    {"getMaxRotation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxRotation, 0), "getMaxRotation() -> retval\n."},
    {"getMaxTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getMaxTranslation, 0), "getMaxTranslation() -> retval\n."},
    {"getMinDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getMinDepth, 0), "getMinDepth() -> retval\n."},
    {"getNormalsComputer", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getNormalsComputer, 0), "getNormalsComputer() -> retval\n."},
    {"getTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_getTransformType, 0), "getTransformType() -> retval\n."},
    {"prepareFrameCache", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_prepareFrameCache, 0), "prepareFrameCache(frame, cacheType) -> retval\n."},
    {"setCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setCameraMatrix, 0), "setCameraMatrix(val) -> None\n."},
    {"setIterationCounts", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setIterationCounts, 0), "setIterationCounts(val) -> None\n."},
    {"setMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxDepth, 0), "setMaxDepth(val) -> None\n."},
    {"setMaxDepthDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxDepthDiff, 0), "setMaxDepthDiff(val) -> None\n."},
    {"setMaxPointsPart", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxPointsPart, 0), "setMaxPointsPart(val) -> None\n."},
    {"setMaxRotation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxRotation, 0), "setMaxRotation(val) -> None\n."},
    {"setMaxTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setMaxTranslation, 0), "setMaxTranslation(val) -> None\n."},
    {"setMinDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setMinDepth, 0), "setMinDepth(val) -> None\n."},
    {"setTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_ICPOdometry_setTransformType, 0), "setTransformType(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_ICPOdometry_specials(void)
{
    pyopencv_rgbd_ICPOdometry_Type.tp_base = &pyopencv_rgbd_Odometry_Type;
    pyopencv_rgbd_ICPOdometry_Type.tp_dealloc = pyopencv_rgbd_ICPOdometry_dealloc;
    pyopencv_rgbd_ICPOdometry_Type.tp_repr = pyopencv_rgbd_ICPOdometry_repr;
    pyopencv_rgbd_ICPOdometry_Type.tp_getset = pyopencv_rgbd_ICPOdometry_getseters;
    pyopencv_rgbd_ICPOdometry_Type.tp_init = (initproc)0;
    pyopencv_rgbd_ICPOdometry_Type.tp_methods = pyopencv_rgbd_ICPOdometry_methods;
}

static PyObject* pyopencv_rgbd_RgbdICPOdometry_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<rgbd_RgbdICPOdometry %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_rgbd_RgbdICPOdometry_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    {
    PyObject* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    float minDepth=Odometry::DEFAULT_MIN_DEPTH();
    float maxDepth=Odometry::DEFAULT_MAX_DEPTH();
    float maxDepthDiff=Odometry::DEFAULT_MAX_DEPTH_DIFF();
    float maxPointsPart=Odometry::DEFAULT_MAX_POINTS_PART();
    PyObject* pyobj_iterCounts = NULL;
    vector_int iterCounts=std::vector<int>();
    PyObject* pyobj_minGradientMagnitudes = NULL;
    vector_float minGradientMagnitudes=std::vector<float>();
    int transformType=Odometry::RIGID_BODY_MOTION;
    Ptr<RgbdICPOdometry> retval;

    const char* keywords[] = { "cameraMatrix", "minDepth", "maxDepth", "maxDepthDiff", "maxPointsPart", "iterCounts", "minGradientMagnitudes", "transformType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OffffOOi:rgbd_RgbdICPOdometry.create", (char**)keywords, &pyobj_cameraMatrix, &minDepth, &maxDepth, &maxDepthDiff, &maxPointsPart, &pyobj_iterCounts, &pyobj_minGradientMagnitudes, &transformType) &&
        pyopencv_to(pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        pyopencv_to(pyobj_iterCounts, iterCounts, ArgInfo("iterCounts", 0)) &&
        pyopencv_to(pyobj_minGradientMagnitudes, minGradientMagnitudes, ArgInfo("minGradientMagnitudes", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdICPOdometry::create(cameraMatrix, minDepth, maxDepth, maxDepthDiff, maxPointsPart, iterCounts, minGradientMagnitudes, transformType));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_cameraMatrix = NULL;
    Mat cameraMatrix;
    float minDepth=Odometry::DEFAULT_MIN_DEPTH();
    float maxDepth=Odometry::DEFAULT_MAX_DEPTH();
    float maxDepthDiff=Odometry::DEFAULT_MAX_DEPTH_DIFF();
    float maxPointsPart=Odometry::DEFAULT_MAX_POINTS_PART();
    PyObject* pyobj_iterCounts = NULL;
    vector_int iterCounts=std::vector<int>();
    PyObject* pyobj_minGradientMagnitudes = NULL;
    vector_float minGradientMagnitudes=std::vector<float>();
    int transformType=Odometry::RIGID_BODY_MOTION;
    Ptr<RgbdICPOdometry> retval;

    const char* keywords[] = { "cameraMatrix", "minDepth", "maxDepth", "maxDepthDiff", "maxPointsPart", "iterCounts", "minGradientMagnitudes", "transformType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OffffOOi:rgbd_RgbdICPOdometry.create", (char**)keywords, &pyobj_cameraMatrix, &minDepth, &maxDepth, &maxDepthDiff, &maxPointsPart, &pyobj_iterCounts, &pyobj_minGradientMagnitudes, &transformType) &&
        pyopencv_to(pyobj_cameraMatrix, cameraMatrix, ArgInfo("cameraMatrix", 0)) &&
        pyopencv_to(pyobj_iterCounts, iterCounts, ArgInfo("iterCounts", 0)) &&
        pyopencv_to(pyobj_minGradientMagnitudes, minGradientMagnitudes, ArgInfo("minGradientMagnitudes", 0)) )
    {
        ERRWRAP2(retval = cv::rgbd::RgbdICPOdometry::create(cameraMatrix, minDepth, maxDepth, maxDepthDiff, maxPointsPart, iterCounts, minGradientMagnitudes, transformType));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCameraMatrix());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getIterationCounts(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIterationCounts());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxDepthDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxDepthDiff());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxPointsPart(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxPointsPart());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxRotation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxRotation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxTranslation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMinDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinDepth());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMinGradientMagnitudes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    cv::Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinGradientMagnitudes());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getNormalsComputer(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    Ptr<RgbdNormals> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNormalsComputer());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getTransformType());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_prepareFrameCache(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    PyObject* pyobj_frame = NULL;
    Ptr<OdometryFrame> frame;
    int cacheType=0;
    Size retval;

    const char* keywords[] = { "frame", "cacheType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:rgbd_RgbdICPOdometry.prepareFrameCache", (char**)keywords, &pyobj_frame, &cacheType) &&
        pyopencv_to(pyobj_frame, frame, ArgInfo("frame", 0)) )
    {
        ERRWRAP2(retval = _self_->prepareFrameCache(frame, cacheType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setCameraMatrix(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdICPOdometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdICPOdometry.setCameraMatrix", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setCameraMatrix(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setIterationCounts(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdICPOdometry.setIterationCounts", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setIterationCounts(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdICPOdometry.setIterationCounts", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setIterationCounts(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdICPOdometry.setMaxDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxDepthDiff(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdICPOdometry.setMaxDepthDiff", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxDepthDiff(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxPointsPart(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdICPOdometry.setMaxPointsPart", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxPointsPart(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxRotation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdICPOdometry.setMaxRotation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxRotation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdICPOdometry.setMaxTranslation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxTranslation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMinDepth(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:rgbd_RgbdICPOdometry.setMinDepth", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMinDepth(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMinGradientMagnitudes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdICPOdometry.setMinGradientMagnitudes", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setMinGradientMagnitudes(val));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_val = NULL;
    Mat val;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:rgbd_RgbdICPOdometry.setMinGradientMagnitudes", (char**)keywords, &pyobj_val) &&
        pyopencv_to(pyobj_val, val, ArgInfo("val", 0)) )
    {
        ERRWRAP2(_self_->setMinGradientMagnitudes(val));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setTransformType(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::rgbd;

    cv::rgbd::RgbdICPOdometry* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_rgbd_RgbdICPOdometry_Type))
        _self_ = dynamic_cast<cv::rgbd::RgbdICPOdometry*>(((pyopencv_rgbd_RgbdICPOdometry_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'rgbd_RgbdICPOdometry' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:rgbd_RgbdICPOdometry.setTransformType", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setTransformType(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_rgbd_RgbdICPOdometry_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_create_cls, METH_CLASS), "create([, cameraMatrix[, minDepth[, maxDepth[, maxDepthDiff[, maxPointsPart[, iterCounts[, minGradientMagnitudes[, transformType]]]]]]]]) -> retval\n.   Constructor.\n.   * @param cameraMatrix Camera matrix\n.   * @param minDepth Pixels with depth less than minDepth will not be used\n.   * @param maxDepth Pixels with depth larger than maxDepth will not be used\n.   * @param maxDepthDiff Correspondences between pixels of two given frames will be filtered out\n.   *                     if their depth difference is larger than maxDepthDiff\n.   * @param maxPointsPart The method uses a random pixels subset of size frameWidth x frameHeight x pointsPart\n.   * @param iterCounts Count of iterations on each pyramid level.\n.   * @param minGradientMagnitudes For each pyramid level the pixels will be filtered out\n.   *                              if they have gradient magnitude less than minGradientMagnitudes[level].\n.   * @param transformType Class of trasformation"},
    {"getCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getCameraMatrix, 0), "getCameraMatrix() -> retval\n."},
    {"getIterationCounts", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getIterationCounts, 0), "getIterationCounts() -> retval\n."},
    {"getMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxDepth, 0), "getMaxDepth() -> retval\n."},
    {"getMaxDepthDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxDepthDiff, 0), "getMaxDepthDiff() -> retval\n."},
    {"getMaxPointsPart", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxPointsPart, 0), "getMaxPointsPart() -> retval\n."},
    {"getMaxRotation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxRotation, 0), "getMaxRotation() -> retval\n."},
    {"getMaxTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMaxTranslation, 0), "getMaxTranslation() -> retval\n."},
    {"getMinDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMinDepth, 0), "getMinDepth() -> retval\n."},
    {"getMinGradientMagnitudes", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getMinGradientMagnitudes, 0), "getMinGradientMagnitudes() -> retval\n."},
    {"getNormalsComputer", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getNormalsComputer, 0), "getNormalsComputer() -> retval\n."},
    {"getTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_getTransformType, 0), "getTransformType() -> retval\n."},
    {"prepareFrameCache", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_prepareFrameCache, 0), "prepareFrameCache(frame, cacheType) -> retval\n."},
    {"setCameraMatrix", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setCameraMatrix, 0), "setCameraMatrix(val) -> None\n."},
    {"setIterationCounts", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setIterationCounts, 0), "setIterationCounts(val) -> None\n."},
    {"setMaxDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxDepth, 0), "setMaxDepth(val) -> None\n."},
    {"setMaxDepthDiff", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxDepthDiff, 0), "setMaxDepthDiff(val) -> None\n."},
    {"setMaxPointsPart", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxPointsPart, 0), "setMaxPointsPart(val) -> None\n."},
    {"setMaxRotation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxRotation, 0), "setMaxRotation(val) -> None\n."},
    {"setMaxTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMaxTranslation, 0), "setMaxTranslation(val) -> None\n."},
    {"setMinDepth", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMinDepth, 0), "setMinDepth(val) -> None\n."},
    {"setMinGradientMagnitudes", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setMinGradientMagnitudes, 0), "setMinGradientMagnitudes(val) -> None\n."},
    {"setTransformType", CV_PY_FN_WITH_KW_(pyopencv_cv_rgbd_rgbd_RgbdICPOdometry_setTransformType, 0), "setTransformType(val) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_rgbd_RgbdICPOdometry_specials(void)
{
    pyopencv_rgbd_RgbdICPOdometry_Type.tp_base = &pyopencv_rgbd_Odometry_Type;
    pyopencv_rgbd_RgbdICPOdometry_Type.tp_dealloc = pyopencv_rgbd_RgbdICPOdometry_dealloc;
    pyopencv_rgbd_RgbdICPOdometry_Type.tp_repr = pyopencv_rgbd_RgbdICPOdometry_repr;
    pyopencv_rgbd_RgbdICPOdometry_Type.tp_getset = pyopencv_rgbd_RgbdICPOdometry_getseters;
    pyopencv_rgbd_RgbdICPOdometry_Type.tp_init = (initproc)0;
    pyopencv_rgbd_RgbdICPOdometry_Type.tp_methods = pyopencv_rgbd_RgbdICPOdometry_methods;
}

static PyObject* pyopencv_linemod_Feature_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<linemod_Feature %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_linemod_Feature_get_label(pyopencv_linemod_Feature_t* p, void *closure)
{
    return pyopencv_from(p->v.label);
}

static int pyopencv_linemod_Feature_set_label(pyopencv_linemod_Feature_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the label attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.label) ? 0 : -1;
}

static PyObject* pyopencv_linemod_Feature_get_x(pyopencv_linemod_Feature_t* p, void *closure)
{
    return pyopencv_from(p->v.x);
}

static int pyopencv_linemod_Feature_set_x(pyopencv_linemod_Feature_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the x attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.x) ? 0 : -1;
}

static PyObject* pyopencv_linemod_Feature_get_y(pyopencv_linemod_Feature_t* p, void *closure)
{
    return pyopencv_from(p->v.y);
}

static int pyopencv_linemod_Feature_set_y(pyopencv_linemod_Feature_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the y attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.y) ? 0 : -1;
}


static PyGetSetDef pyopencv_linemod_Feature_getseters[] =
{
    {(char*)"label", (getter)pyopencv_linemod_Feature_get_label, (setter)pyopencv_linemod_Feature_set_label, (char*)"label", NULL},
    {(char*)"x", (getter)pyopencv_linemod_Feature_get_x, (setter)pyopencv_linemod_Feature_set_x, (char*)"x", NULL},
    {(char*)"y", (getter)pyopencv_linemod_Feature_get_y, (setter)pyopencv_linemod_Feature_set_y, (char*)"y", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_linemod_linemod_Feature_Feature(pyopencv_linemod_Feature_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::linemod::Feature());
        return 0;
    }
    }
    PyErr_Clear();

    {
    int x=0;
    int y=0;
    int label=0;

    const char* keywords[] = { "x", "y", "label", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iii:Feature", (char**)keywords, &x, &y, &label) )
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::linemod::Feature(x, y, label));
        return 0;
    }
    }

    return -1;
}



static PyMethodDef pyopencv_linemod_Feature_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_linemod_Feature_specials(void)
{
    pyopencv_linemod_Feature_Type.tp_base = NULL;
    pyopencv_linemod_Feature_Type.tp_dealloc = pyopencv_linemod_Feature_dealloc;
    pyopencv_linemod_Feature_Type.tp_repr = pyopencv_linemod_Feature_repr;
    pyopencv_linemod_Feature_Type.tp_getset = pyopencv_linemod_Feature_getseters;
    pyopencv_linemod_Feature_Type.tp_init = (initproc)pyopencv_cv_linemod_linemod_Feature_Feature;
    pyopencv_linemod_Feature_Type.tp_methods = pyopencv_linemod_Feature_methods;
}

static PyObject* pyopencv_linemod_Template_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<linemod_Template %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_linemod_Template_get_features(pyopencv_linemod_Template_t* p, void *closure)
{
    return pyopencv_from(p->v.features);
}

static PyObject* pyopencv_linemod_Template_get_height(pyopencv_linemod_Template_t* p, void *closure)
{
    return pyopencv_from(p->v.height);
}

static PyObject* pyopencv_linemod_Template_get_pyramid_level(pyopencv_linemod_Template_t* p, void *closure)
{
    return pyopencv_from(p->v.pyramid_level);
}

static PyObject* pyopencv_linemod_Template_get_width(pyopencv_linemod_Template_t* p, void *closure)
{
    return pyopencv_from(p->v.width);
}


static PyGetSetDef pyopencv_linemod_Template_getseters[] =
{
    {(char*)"features", (getter)pyopencv_linemod_Template_get_features, NULL, (char*)"features", NULL},
    {(char*)"height", (getter)pyopencv_linemod_Template_get_height, NULL, (char*)"height", NULL},
    {(char*)"pyramid_level", (getter)pyopencv_linemod_Template_get_pyramid_level, NULL, (char*)"pyramid_level", NULL},
    {(char*)"width", (getter)pyopencv_linemod_Template_get_width, NULL, (char*)"width", NULL},
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_linemod_Template_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_linemod_Template_specials(void)
{
    pyopencv_linemod_Template_Type.tp_base = NULL;
    pyopencv_linemod_Template_Type.tp_dealloc = pyopencv_linemod_Template_dealloc;
    pyopencv_linemod_Template_Type.tp_repr = pyopencv_linemod_Template_repr;
    pyopencv_linemod_Template_Type.tp_getset = pyopencv_linemod_Template_getseters;
    pyopencv_linemod_Template_Type.tp_init = (initproc)0;
    pyopencv_linemod_Template_Type.tp_methods = pyopencv_linemod_Template_methods;
}

static PyObject* pyopencv_linemod_QuantizedPyramid_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<linemod_QuantizedPyramid %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_linemod_QuantizedPyramid_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_linemod_linemod_QuantizedPyramid_extractTemplate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::QuantizedPyramid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_QuantizedPyramid_Type))
        _self_ = ((pyopencv_linemod_QuantizedPyramid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_QuantizedPyramid' or its derivative)");
    Template templ;
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->extractTemplate(templ));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(templ));
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_QuantizedPyramid_pyrDown(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::QuantizedPyramid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_QuantizedPyramid_Type))
        _self_ = ((pyopencv_linemod_QuantizedPyramid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_QuantizedPyramid' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->pyrDown());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_QuantizedPyramid_quantize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::QuantizedPyramid* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_QuantizedPyramid_Type))
        _self_ = ((pyopencv_linemod_QuantizedPyramid_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_QuantizedPyramid' or its derivative)");
    {
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:linemod_QuantizedPyramid.quantize", (char**)keywords, &pyobj_dst) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->quantize(dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:linemod_QuantizedPyramid.quantize", (char**)keywords, &pyobj_dst) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->quantize(dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_linemod_QuantizedPyramid_methods[] =
{
    {"extractTemplate", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_QuantizedPyramid_extractTemplate, 0), "extractTemplate() -> retval, templ\n.   * \\brief Extract most discriminant features at current pyramid level to form a new template.\n.   *\n.   * \\param[out] templ The new template."},
    {"pyrDown", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_QuantizedPyramid_pyrDown, 0), "pyrDown() -> None\n.   * \\brief Go to the next pyramid level.\n.   *\n.   * \\todo Allow pyramid scale factor other than 2"},
    {"quantize", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_QuantizedPyramid_quantize, 0), "quantize([, dst]) -> dst\n.   * \\brief Compute quantized image at current pyramid level for online detection.\n.   *\n.   * \\param[out] dst The destination 8-bit image. For each pixel at most one bit is set,\n.   *                 representing its classification."},

    {NULL,          NULL}
};

static void pyopencv_linemod_QuantizedPyramid_specials(void)
{
    pyopencv_linemod_QuantizedPyramid_Type.tp_base = NULL;
    pyopencv_linemod_QuantizedPyramid_Type.tp_dealloc = pyopencv_linemod_QuantizedPyramid_dealloc;
    pyopencv_linemod_QuantizedPyramid_Type.tp_repr = pyopencv_linemod_QuantizedPyramid_repr;
    pyopencv_linemod_QuantizedPyramid_Type.tp_getset = pyopencv_linemod_QuantizedPyramid_getseters;
    pyopencv_linemod_QuantizedPyramid_Type.tp_init = (initproc)0;
    pyopencv_linemod_QuantizedPyramid_Type.tp_methods = pyopencv_linemod_QuantizedPyramid_methods;
}

static PyObject* pyopencv_linemod_Modality_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<linemod_Modality %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_linemod_Modality_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_linemod_linemod_Modality_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    {
    PyObject* pyobj_modality_type = NULL;
    String modality_type;
    Ptr<Modality> retval;

    const char* keywords[] = { "modality_type", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:linemod_Modality.create", (char**)keywords, &pyobj_modality_type) &&
        pyopencv_to(pyobj_modality_type, modality_type, ArgInfo("modality_type", 0)) )
    {
        ERRWRAP2(retval = cv::linemod::Modality::create(modality_type));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_fn = NULL;
    FileNode fn;
    Ptr<Modality> retval;

    const char* keywords[] = { "fn", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:linemod_Modality.create", (char**)keywords, &pyobj_fn) &&
        pyopencv_to(pyobj_fn, fn, ArgInfo("fn", 0)) )
    {
        ERRWRAP2(retval = cv::linemod::Modality::create(fn));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Modality_name(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Modality* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Modality_Type))
        _self_ = ((pyopencv_linemod_Modality_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Modality' or its derivative)");
    String retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->name());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Modality_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Modality* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Modality_Type))
        _self_ = ((pyopencv_linemod_Modality_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Modality' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    Ptr<QuantizedPyramid> retval;

    const char* keywords[] = { "src", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:linemod_Modality.process", (char**)keywords, &pyobj_src, &pyobj_mask) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(retval = _self_->process(src, mask));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_mask = NULL;
    Mat mask;
    Ptr<QuantizedPyramid> retval;

    const char* keywords[] = { "src", "mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:linemod_Modality.process", (char**)keywords, &pyobj_src, &pyobj_mask) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_mask, mask, ArgInfo("mask", 0)) )
    {
        ERRWRAP2(retval = _self_->process(src, mask));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Modality_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Modality* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Modality_Type))
        _self_ = ((pyopencv_linemod_Modality_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Modality' or its derivative)");
    PyObject* pyobj_fn = NULL;
    FileNode fn;

    const char* keywords[] = { "fn", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:linemod_Modality.read", (char**)keywords, &pyobj_fn) &&
        pyopencv_to(pyobj_fn, fn, ArgInfo("fn", 0)) )
    {
        ERRWRAP2(_self_->read(fn));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_linemod_Modality_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Modality_create_cls, METH_CLASS), "create(modality_type) -> retval\n.   * \\brief Create modality by name.\n.   *\n.   * The following modality types are supported:\n.   * - \"ColorGradient\"\n.   * - \"DepthNormal\"\n\n\n\ncreate(fn) -> retval\n.   * \\brief Load a modality from file."},
    {"name", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Modality_name, 0), "name() -> retval\n."},
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Modality_process, 0), "process(src[, mask]) -> retval\n.   * \\brief Form a quantized image pyramid from a source image.\n.   *\n.   * \\param[in] src  The source image. Type depends on the modality.\n.   * \\param[in] mask Optional mask. If not empty, unmasked pixels are set to zero\n.   *                 in quantized image and cannot be extracted as features."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Modality_read, 0), "read(fn) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_linemod_Modality_specials(void)
{
    pyopencv_linemod_Modality_Type.tp_base = NULL;
    pyopencv_linemod_Modality_Type.tp_dealloc = pyopencv_linemod_Modality_dealloc;
    pyopencv_linemod_Modality_Type.tp_repr = pyopencv_linemod_Modality_repr;
    pyopencv_linemod_Modality_Type.tp_getset = pyopencv_linemod_Modality_getseters;
    pyopencv_linemod_Modality_Type.tp_init = (initproc)0;
    pyopencv_linemod_Modality_Type.tp_methods = pyopencv_linemod_Modality_methods;
}

static PyObject* pyopencv_linemod_Match_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<linemod_Match %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_linemod_Match_get_class_id(pyopencv_linemod_Match_t* p, void *closure)
{
    return pyopencv_from(p->v.class_id);
}

static int pyopencv_linemod_Match_set_class_id(pyopencv_linemod_Match_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the class_id attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.class_id) ? 0 : -1;
}

static PyObject* pyopencv_linemod_Match_get_similarity(pyopencv_linemod_Match_t* p, void *closure)
{
    return pyopencv_from(p->v.similarity);
}

static int pyopencv_linemod_Match_set_similarity(pyopencv_linemod_Match_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the similarity attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.similarity) ? 0 : -1;
}

static PyObject* pyopencv_linemod_Match_get_template_id(pyopencv_linemod_Match_t* p, void *closure)
{
    return pyopencv_from(p->v.template_id);
}

static int pyopencv_linemod_Match_set_template_id(pyopencv_linemod_Match_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the template_id attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.template_id) ? 0 : -1;
}

static PyObject* pyopencv_linemod_Match_get_x(pyopencv_linemod_Match_t* p, void *closure)
{
    return pyopencv_from(p->v.x);
}

static int pyopencv_linemod_Match_set_x(pyopencv_linemod_Match_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the x attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.x) ? 0 : -1;
}

static PyObject* pyopencv_linemod_Match_get_y(pyopencv_linemod_Match_t* p, void *closure)
{
    return pyopencv_from(p->v.y);
}

static int pyopencv_linemod_Match_set_y(pyopencv_linemod_Match_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the y attribute");
        return -1;
    }
    return pyopencv_to(value, p->v.y) ? 0 : -1;
}


static PyGetSetDef pyopencv_linemod_Match_getseters[] =
{
    {(char*)"class_id", (getter)pyopencv_linemod_Match_get_class_id, (setter)pyopencv_linemod_Match_set_class_id, (char*)"class_id", NULL},
    {(char*)"similarity", (getter)pyopencv_linemod_Match_get_similarity, (setter)pyopencv_linemod_Match_set_similarity, (char*)"similarity", NULL},
    {(char*)"template_id", (getter)pyopencv_linemod_Match_get_template_id, (setter)pyopencv_linemod_Match_set_template_id, (char*)"template_id", NULL},
    {(char*)"x", (getter)pyopencv_linemod_Match_get_x, (setter)pyopencv_linemod_Match_set_x, (char*)"x", NULL},
    {(char*)"y", (getter)pyopencv_linemod_Match_get_y, (setter)pyopencv_linemod_Match_set_y, (char*)"y", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_linemod_linemod_Match_Match(pyopencv_linemod_Match_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::linemod::Match());
        return 0;
    }
    }
    PyErr_Clear();

    {
    int x=0;
    int y=0;
    float similarity=0.f;
    PyObject* pyobj_class_id = NULL;
    String class_id;
    int template_id=0;

    const char* keywords[] = { "x", "y", "similarity", "class_id", "template_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iifOi:Match", (char**)keywords, &x, &y, &similarity, &pyobj_class_id, &template_id) &&
        pyopencv_to(pyobj_class_id, class_id, ArgInfo("class_id", 0)) )
    {
        if(self) ERRWRAP2(new (&(self->v)) cv::linemod::Match(x, y, similarity, class_id, template_id));
        return 0;
    }
    }

    return -1;
}



static PyMethodDef pyopencv_linemod_Match_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_linemod_Match_specials(void)
{
    pyopencv_linemod_Match_Type.tp_base = NULL;
    pyopencv_linemod_Match_Type.tp_dealloc = pyopencv_linemod_Match_dealloc;
    pyopencv_linemod_Match_Type.tp_repr = pyopencv_linemod_Match_repr;
    pyopencv_linemod_Match_Type.tp_getset = pyopencv_linemod_Match_getseters;
    pyopencv_linemod_Match_Type.tp_init = (initproc)pyopencv_cv_linemod_linemod_Match_Match;
    pyopencv_linemod_Match_Type.tp_methods = pyopencv_linemod_Match_methods;
}

static PyObject* pyopencv_linemod_Detector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<linemod_Detector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_linemod_Detector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_linemod_linemod_Detector_Detector(pyopencv_linemod_Detector_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    {

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::linemod::Detector>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::linemod::Detector()));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_modalities = NULL;
    vector_Ptr_Modality modalities;
    PyObject* pyobj_T_pyramid = NULL;
    vector_int T_pyramid;

    const char* keywords[] = { "modalities", "T_pyramid", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:Detector", (char**)keywords, &pyobj_modalities, &pyobj_T_pyramid) &&
        pyopencv_to(pyobj_modalities, modalities, ArgInfo("modalities", 0)) &&
        pyopencv_to(pyobj_T_pyramid, T_pyramid, ArgInfo("T_pyramid", 0)) )
    {
        new (&(self->v)) Ptr<cv::linemod::Detector>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::linemod::Detector(modalities, T_pyramid)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_addSyntheticTemplate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    PyObject* pyobj_templates = NULL;
    vector_Template templates;
    PyObject* pyobj_class_id = NULL;
    String class_id;
    int retval;

    const char* keywords[] = { "templates", "class_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:linemod_Detector.addSyntheticTemplate", (char**)keywords, &pyobj_templates, &pyobj_class_id) &&
        pyopencv_to(pyobj_templates, templates, ArgInfo("templates", 0)) &&
        pyopencv_to(pyobj_class_id, class_id, ArgInfo("class_id", 0)) )
    {
        ERRWRAP2(retval = _self_->addSyntheticTemplate(templates, class_id));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_addTemplate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    {
    PyObject* pyobj_sources = NULL;
    vector_Mat sources;
    PyObject* pyobj_class_id = NULL;
    String class_id;
    PyObject* pyobj_object_mask = NULL;
    Mat object_mask;
    Rect bounding_box;
    int retval;

    const char* keywords[] = { "sources", "class_id", "object_mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:linemod_Detector.addTemplate", (char**)keywords, &pyobj_sources, &pyobj_class_id, &pyobj_object_mask) &&
        pyopencv_to(pyobj_sources, sources, ArgInfo("sources", 0)) &&
        pyopencv_to(pyobj_class_id, class_id, ArgInfo("class_id", 0)) &&
        pyopencv_to(pyobj_object_mask, object_mask, ArgInfo("object_mask", 0)) )
    {
        ERRWRAP2(retval = _self_->addTemplate(sources, class_id, object_mask, &bounding_box));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(bounding_box));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_sources = NULL;
    vector_Mat sources;
    PyObject* pyobj_class_id = NULL;
    String class_id;
    PyObject* pyobj_object_mask = NULL;
    Mat object_mask;
    Rect bounding_box;
    int retval;

    const char* keywords[] = { "sources", "class_id", "object_mask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:linemod_Detector.addTemplate", (char**)keywords, &pyobj_sources, &pyobj_class_id, &pyobj_object_mask) &&
        pyopencv_to(pyobj_sources, sources, ArgInfo("sources", 0)) &&
        pyopencv_to(pyobj_class_id, class_id, ArgInfo("class_id", 0)) &&
        pyopencv_to(pyobj_object_mask, object_mask, ArgInfo("object_mask", 0)) )
    {
        ERRWRAP2(retval = _self_->addTemplate(sources, class_id, object_mask, &bounding_box));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(bounding_box));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_classIds(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    std::vector<String> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->classIds());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_getModalities(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    std::vector< Ptr<Modality> > retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getModalities());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_getT(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    int pyramid_level=0;
    int retval;

    const char* keywords[] = { "pyramid_level", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:linemod_Detector.getT", (char**)keywords, &pyramid_level) )
    {
        ERRWRAP2(retval = _self_->getT(pyramid_level));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_getTemplates(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    PyObject* pyobj_class_id = NULL;
    String class_id;
    int template_id=0;
    std::vector<Template> retval;

    const char* keywords[] = { "class_id", "template_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:linemod_Detector.getTemplates", (char**)keywords, &pyobj_class_id, &template_id) &&
        pyopencv_to(pyobj_class_id, class_id, ArgInfo("class_id", 0)) )
    {
        ERRWRAP2(retval = _self_->getTemplates(class_id, template_id));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_match(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    {
    PyObject* pyobj_sources = NULL;
    vector_Mat sources;
    float threshold=0.f;
    vector_Match matches;
    PyObject* pyobj_class_ids = NULL;
    vector_String class_ids=std::vector<String>();
    PyObject* pyobj_quantized_images = NULL;
    vector_Mat quantized_images;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks=std::vector<Mat>();

    const char* keywords[] = { "sources", "threshold", "class_ids", "quantized_images", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Of|OOO:linemod_Detector.match", (char**)keywords, &pyobj_sources, &threshold, &pyobj_class_ids, &pyobj_quantized_images, &pyobj_masks) &&
        pyopencv_to(pyobj_sources, sources, ArgInfo("sources", 0)) &&
        pyopencv_to(pyobj_class_ids, class_ids, ArgInfo("class_ids", 0)) &&
        pyopencv_to(pyobj_quantized_images, quantized_images, ArgInfo("quantized_images", 1)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->match(sources, threshold, matches, class_ids, quantized_images, masks));
        return Py_BuildValue("(NN)", pyopencv_from(matches), pyopencv_from(quantized_images));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_sources = NULL;
    vector_Mat sources;
    float threshold=0.f;
    vector_Match matches;
    PyObject* pyobj_class_ids = NULL;
    vector_String class_ids=std::vector<String>();
    PyObject* pyobj_quantized_images = NULL;
    vector_Mat quantized_images;
    PyObject* pyobj_masks = NULL;
    vector_Mat masks=std::vector<Mat>();

    const char* keywords[] = { "sources", "threshold", "class_ids", "quantized_images", "masks", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Of|OOO:linemod_Detector.match", (char**)keywords, &pyobj_sources, &threshold, &pyobj_class_ids, &pyobj_quantized_images, &pyobj_masks) &&
        pyopencv_to(pyobj_sources, sources, ArgInfo("sources", 0)) &&
        pyopencv_to(pyobj_class_ids, class_ids, ArgInfo("class_ids", 0)) &&
        pyopencv_to(pyobj_quantized_images, quantized_images, ArgInfo("quantized_images", 1)) &&
        pyopencv_to(pyobj_masks, masks, ArgInfo("masks", 0)) )
    {
        ERRWRAP2(_self_->match(sources, threshold, matches, class_ids, quantized_images, masks));
        return Py_BuildValue("(NN)", pyopencv_from(matches), pyopencv_from(quantized_images));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_numClasses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->numClasses());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_numTemplates(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    {
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->numTemplates());
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_class_id = NULL;
    String class_id;
    int retval;

    const char* keywords[] = { "class_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:linemod_Detector.numTemplates", (char**)keywords, &pyobj_class_id) &&
        pyopencv_to(pyobj_class_id, class_id, ArgInfo("class_id", 0)) )
    {
        ERRWRAP2(retval = _self_->numTemplates(class_id));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_pyramidLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->pyramidLevels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_read(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    PyObject* pyobj_fn = NULL;
    FileNode fn;

    const char* keywords[] = { "fn", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:linemod_Detector.read", (char**)keywords, &pyobj_fn) &&
        pyopencv_to(pyobj_fn, fn, ArgInfo("fn", 0)) )
    {
        ERRWRAP2(_self_->read(fn));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_readClasses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    PyObject* pyobj_class_ids = NULL;
    vector_String class_ids;
    PyObject* pyobj_format = NULL;
    String format="templates_%s.yml.gz";

    const char* keywords[] = { "class_ids", "format", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:linemod_Detector.readClasses", (char**)keywords, &pyobj_class_ids, &pyobj_format) &&
        pyopencv_to(pyobj_class_ids, class_ids, ArgInfo("class_ids", 0)) &&
        pyopencv_to(pyobj_format, format, ArgInfo("format", 0)) )
    {
        ERRWRAP2(_self_->readClasses(class_ids, format));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_linemod_linemod_Detector_writeClasses(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::linemod;

    cv::linemod::Detector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_linemod_Detector_Type))
        _self_ = ((pyopencv_linemod_Detector_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'linemod_Detector' or its derivative)");
    PyObject* pyobj_format = NULL;
    String format="templates_%s.yml.gz";

    const char* keywords[] = { "format", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:linemod_Detector.writeClasses", (char**)keywords, &pyobj_format) &&
        pyopencv_to(pyobj_format, format, ArgInfo("format", 0)) )
    {
        ERRWRAP2(_self_->writeClasses(format));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_linemod_Detector_methods[] =
{
    {"addSyntheticTemplate", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_addSyntheticTemplate, 0), "addSyntheticTemplate(templates, class_id) -> retval\n.   * \\brief Add a new object template computed by external means."},
    {"addTemplate", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_addTemplate, 0), "addTemplate(sources, class_id, object_mask) -> retval, bounding_box\n.   * \\brief Add new object template.\n.   *\n.   * \\param      sources      Source images, one for each modality.\n.   * \\param      class_id     Object class ID.\n.   * \\param      object_mask  Mask separating object from background.\n.   * \\param[out] bounding_box Optionally return bounding box of the extracted features.\n.   *\n.   * \\return Template ID, or -1 if failed to extract a valid template."},
    {"classIds", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_classIds, 0), "classIds() -> retval\n."},
    {"getModalities", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_getModalities, 0), "getModalities() -> retval\n.   * \\brief Get the modalities used by this detector.\n.   *\n.   * You are not permitted to add/remove modalities, but you may dynamic_cast them to\n.   * tweak parameters."},
    {"getT", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_getT, 0), "getT(pyramid_level) -> retval\n.   * \\brief Get sampling step T at pyramid_level."},
    {"getTemplates", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_getTemplates, 0), "getTemplates(class_id, template_id) -> retval\n.   * \\brief Get the template pyramid identified by template_id.\n.   *\n.   * For example, with 2 modalities (Gradient, Normal) and two pyramid levels\n.   * (L0, L1), the order is (GradientL0, NormalL0, GradientL1, NormalL1)."},
    {"match", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_match, 0), "match(sources, threshold[, class_ids[, quantized_images[, masks]]]) -> matches, quantized_images\n.   * \\brief Detect objects by template matching.\n.   *\n.   * Matches globally at the lowest pyramid level, then refines locally stepping up the pyramid.\n.   *\n.   * \\param      sources   Source images, one for each modality.\n.   * \\param      threshold Similarity threshold, a percentage between 0 and 100.\n.   * \\param[out] matches   Template matches, sorted by similarity score.\n.   * \\param      class_ids If non-empty, only search for the desired object classes.\n.   * \\param[out] quantized_images Optionally return vector<Mat> of quantized images.\n.   * \\param      masks     The masks for consideration during matching. The masks should be CV_8UC1\n.   *                       where 255 represents a valid pixel.  If non-empty, the vector must be\n.   *                       the same size as sources.  Each element must be\n.   *                       empty or the same size as its corresponding source."},
    {"numClasses", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_numClasses, 0), "numClasses() -> retval\n."},
    {"numTemplates", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_numTemplates, 0), "numTemplates() -> retval\n.   \n\n\n\nnumTemplates(class_id) -> retval\n."},
    {"pyramidLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_pyramidLevels, 0), "pyramidLevels() -> retval\n.   * \\brief Get number of pyramid levels used by this detector."},
    {"read", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_read, 0), "read(fn) -> None\n."},
    {"readClasses", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_readClasses, 0), "readClasses(class_ids[, format]) -> None\n."},
    {"writeClasses", CV_PY_FN_WITH_KW_(pyopencv_cv_linemod_linemod_Detector_writeClasses, 0), "writeClasses([, format]) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_linemod_Detector_specials(void)
{
    pyopencv_linemod_Detector_Type.tp_base = NULL;
    pyopencv_linemod_Detector_Type.tp_dealloc = pyopencv_linemod_Detector_dealloc;
    pyopencv_linemod_Detector_Type.tp_repr = pyopencv_linemod_Detector_repr;
    pyopencv_linemod_Detector_Type.tp_getset = pyopencv_linemod_Detector_getseters;
    pyopencv_linemod_Detector_Type.tp_init = (initproc)pyopencv_cv_linemod_linemod_Detector_Detector;
    pyopencv_linemod_Detector_Type.tp_methods = pyopencv_linemod_Detector_methods;
}

static PyObject* pyopencv_structured_light_GrayCodePattern_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<structured_light_GrayCodePattern %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_structured_light_GrayCodePattern_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_structured_light_structured_light_GrayCodePattern_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    int width=0;
    int height=0;
    Ptr<GrayCodePattern> retval;

    const char* keywords[] = { "width", "height", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:structured_light_GrayCodePattern.create", (char**)keywords, &width, &height) )
    {
        ERRWRAP2(retval = cv::structured_light::GrayCodePattern::create(width, height));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_GrayCodePattern_getImagesForShadowMasks(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::GrayCodePattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_GrayCodePattern_Type))
        _self_ = dynamic_cast<cv::structured_light::GrayCodePattern*>(((pyopencv_structured_light_GrayCodePattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_GrayCodePattern' or its derivative)");
    {
    PyObject* pyobj_blackImage = NULL;
    Mat blackImage;
    PyObject* pyobj_whiteImage = NULL;
    Mat whiteImage;

    const char* keywords[] = { "blackImage", "whiteImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:structured_light_GrayCodePattern.getImagesForShadowMasks", (char**)keywords, &pyobj_blackImage, &pyobj_whiteImage) &&
        pyopencv_to(pyobj_blackImage, blackImage, ArgInfo("blackImage", 1)) &&
        pyopencv_to(pyobj_whiteImage, whiteImage, ArgInfo("whiteImage", 1)) )
    {
        ERRWRAP2(_self_->getImagesForShadowMasks(blackImage, whiteImage));
        return Py_BuildValue("(NN)", pyopencv_from(blackImage), pyopencv_from(whiteImage));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_blackImage = NULL;
    UMat blackImage;
    PyObject* pyobj_whiteImage = NULL;
    UMat whiteImage;

    const char* keywords[] = { "blackImage", "whiteImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:structured_light_GrayCodePattern.getImagesForShadowMasks", (char**)keywords, &pyobj_blackImage, &pyobj_whiteImage) &&
        pyopencv_to(pyobj_blackImage, blackImage, ArgInfo("blackImage", 1)) &&
        pyopencv_to(pyobj_whiteImage, whiteImage, ArgInfo("whiteImage", 1)) )
    {
        ERRWRAP2(_self_->getImagesForShadowMasks(blackImage, whiteImage));
        return Py_BuildValue("(NN)", pyopencv_from(blackImage), pyopencv_from(whiteImage));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_GrayCodePattern_getNumberOfPatternImages(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::GrayCodePattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_GrayCodePattern_Type))
        _self_ = dynamic_cast<cv::structured_light::GrayCodePattern*>(((pyopencv_structured_light_GrayCodePattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_GrayCodePattern' or its derivative)");
    size_t retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumberOfPatternImages());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_GrayCodePattern_getProjPixel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::GrayCodePattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_GrayCodePattern_Type))
        _self_ = dynamic_cast<cv::structured_light::GrayCodePattern*>(((pyopencv_structured_light_GrayCodePattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_GrayCodePattern' or its derivative)");
    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    int x=0;
    int y=0;
    Point projPix;
    bool retval;

    const char* keywords[] = { "patternImages", "x", "y", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oii:structured_light_GrayCodePattern.getProjPixel", (char**)keywords, &pyobj_patternImages, &x, &y) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) )
    {
        ERRWRAP2(retval = _self_->getProjPixel(patternImages, x, y, projPix));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(projPix));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    int x=0;
    int y=0;
    Point projPix;
    bool retval;

    const char* keywords[] = { "patternImages", "x", "y", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oii:structured_light_GrayCodePattern.getProjPixel", (char**)keywords, &pyobj_patternImages, &x, &y) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) )
    {
        ERRWRAP2(retval = _self_->getProjPixel(patternImages, x, y, projPix));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(projPix));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_GrayCodePattern_setBlackThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::GrayCodePattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_GrayCodePattern_Type))
        _self_ = dynamic_cast<cv::structured_light::GrayCodePattern*>(((pyopencv_structured_light_GrayCodePattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_GrayCodePattern' or its derivative)");
    size_t value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "I:structured_light_GrayCodePattern.setBlackThreshold", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setBlackThreshold(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_GrayCodePattern_setWhiteThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::GrayCodePattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_GrayCodePattern_Type))
        _self_ = dynamic_cast<cv::structured_light::GrayCodePattern*>(((pyopencv_structured_light_GrayCodePattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_GrayCodePattern' or its derivative)");
    size_t value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "I:structured_light_GrayCodePattern.setWhiteThreshold", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setWhiteThreshold(value));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_structured_light_GrayCodePattern_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_GrayCodePattern_create_cls, METH_CLASS), "create(width, height) -> retval\n.   @brief Constructor\n.   @param parameters GrayCodePattern parameters GrayCodePattern::Params: the width and the height of the projector."},
    {"getImagesForShadowMasks", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_GrayCodePattern_getImagesForShadowMasks, 0), "getImagesForShadowMasks(blackImage, whiteImage) -> blackImage, whiteImage\n.   @brief Generates the all-black and all-white images needed for shadowMasks computation.\n.   *\n.   *  To identify shadow regions, the regions of two images where the pixels are not lit by projector's light and thus where there is not coded information,\n.   *  the 3DUNDERWORLD algorithm computes a shadow mask for the two cameras views, starting from a white and a black images captured by each camera.\n.   *  This method generates these two additional images to project.\n.   *\n.   *  @param blackImage The generated all-black CV_8U image, at projector's resolution.\n.   *  @param whiteImage The generated all-white CV_8U image, at projector's resolution."},
    {"getNumberOfPatternImages", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_GrayCodePattern_getNumberOfPatternImages, 0), "getNumberOfPatternImages() -> retval\n.   @brief Get the number of pattern images needed for the graycode pattern.\n.   *\n.   * @return The number of pattern images needed for the graycode pattern.\n.   *"},
    {"getProjPixel", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_GrayCodePattern_getProjPixel, 0), "getProjPixel(patternImages, x, y) -> retval, projPix\n.   @brief For a (x,y) pixel of a camera returns the corresponding projector pixel.\n.   *\n.   *  The function decodes each pixel in the pattern images acquired by a camera into their corresponding decimal numbers representing the projector's column and row,\n.   *  providing a mapping between camera's and projector's pixel.\n.   *\n.   *  @param patternImages The pattern images acquired by the camera, stored in a grayscale vector < Mat >.\n.   *  @param x x coordinate of the image pixel.\n.   *  @param y y coordinate of the image pixel.\n.   *  @param projPix Projector's pixel corresponding to the camera's pixel: projPix.x and projPix.y are the image coordinates of the projector's pixel corresponding to the pixel being decoded in a camera."},
    {"setBlackThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_GrayCodePattern_setBlackThreshold, 0), "setBlackThreshold(value) -> None\n.   @brief Sets the value for black threshold, needed for decoding (shadowsmasks computation).\n.   *\n.   *  Black threshold is a number between 0-255 that represents the minimum brightness difference required for valid pixels, between the fully illuminated (white) and the not illuminated images (black); used in computeShadowMasks method.\n.   *\n.   *  @param value The desired black threshold value.\n.   *"},
    {"setWhiteThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_GrayCodePattern_setWhiteThreshold, 0), "setWhiteThreshold(value) -> None\n.   @brief Sets the value for white threshold, needed for decoding.\n.   *\n.   *  White threshold is a number between 0-255 that represents the minimum brightness difference required for valid pixels, between the graycode pattern and its inverse images; used in getProjPixel method.\n.   *\n.   *  @param value The desired white threshold value.\n.   *"},

    {NULL,          NULL}
};

static void pyopencv_structured_light_GrayCodePattern_specials(void)
{
    pyopencv_structured_light_GrayCodePattern_Type.tp_base = &pyopencv_structured_light_StructuredLightPattern_Type;
    pyopencv_structured_light_GrayCodePattern_Type.tp_dealloc = pyopencv_structured_light_GrayCodePattern_dealloc;
    pyopencv_structured_light_GrayCodePattern_Type.tp_repr = pyopencv_structured_light_GrayCodePattern_repr;
    pyopencv_structured_light_GrayCodePattern_Type.tp_getset = pyopencv_structured_light_GrayCodePattern_getseters;
    pyopencv_structured_light_GrayCodePattern_Type.tp_init = (initproc)0;
    pyopencv_structured_light_GrayCodePattern_Type.tp_methods = pyopencv_structured_light_GrayCodePattern_methods;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<structured_light_SinusoidalPattern %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_structured_light_SinusoidalPattern_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_structured_light_structured_light_SinusoidalPattern_computeDataModulationTerm(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::SinusoidalPattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_SinusoidalPattern_Type))
        _self_ = dynamic_cast<cv::structured_light::SinusoidalPattern*>(((pyopencv_structured_light_SinusoidalPattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_SinusoidalPattern' or its derivative)");
    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    PyObject* pyobj_dataModulationTerm = NULL;
    Mat dataModulationTerm;
    PyObject* pyobj_shadowMask = NULL;
    Mat shadowMask;

    const char* keywords[] = { "patternImages", "shadowMask", "dataModulationTerm", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:structured_light_SinusoidalPattern.computeDataModulationTerm", (char**)keywords, &pyobj_patternImages, &pyobj_shadowMask, &pyobj_dataModulationTerm) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) &&
        pyopencv_to(pyobj_dataModulationTerm, dataModulationTerm, ArgInfo("dataModulationTerm", 1)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 0)) )
    {
        ERRWRAP2(_self_->computeDataModulationTerm(patternImages, dataModulationTerm, shadowMask));
        return pyopencv_from(dataModulationTerm);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    PyObject* pyobj_dataModulationTerm = NULL;
    UMat dataModulationTerm;
    PyObject* pyobj_shadowMask = NULL;
    UMat shadowMask;

    const char* keywords[] = { "patternImages", "shadowMask", "dataModulationTerm", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:structured_light_SinusoidalPattern.computeDataModulationTerm", (char**)keywords, &pyobj_patternImages, &pyobj_shadowMask, &pyobj_dataModulationTerm) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) &&
        pyopencv_to(pyobj_dataModulationTerm, dataModulationTerm, ArgInfo("dataModulationTerm", 1)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 0)) )
    {
        ERRWRAP2(_self_->computeDataModulationTerm(patternImages, dataModulationTerm, shadowMask));
        return pyopencv_from(dataModulationTerm);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_SinusoidalPattern_computePhaseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::SinusoidalPattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_SinusoidalPattern_Type))
        _self_ = dynamic_cast<cv::structured_light::SinusoidalPattern*>(((pyopencv_structured_light_SinusoidalPattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_SinusoidalPattern' or its derivative)");
    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    PyObject* pyobj_wrappedPhaseMap = NULL;
    Mat wrappedPhaseMap;
    PyObject* pyobj_shadowMask = NULL;
    Mat shadowMask;
    PyObject* pyobj_fundamental = NULL;
    Mat fundamental;

    const char* keywords[] = { "patternImages", "wrappedPhaseMap", "shadowMask", "fundamental", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOO:structured_light_SinusoidalPattern.computePhaseMap", (char**)keywords, &pyobj_patternImages, &pyobj_wrappedPhaseMap, &pyobj_shadowMask, &pyobj_fundamental) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) &&
        pyopencv_to(pyobj_wrappedPhaseMap, wrappedPhaseMap, ArgInfo("wrappedPhaseMap", 1)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 1)) &&
        pyopencv_to(pyobj_fundamental, fundamental, ArgInfo("fundamental", 0)) )
    {
        ERRWRAP2(_self_->computePhaseMap(patternImages, wrappedPhaseMap, shadowMask, fundamental));
        return Py_BuildValue("(NN)", pyopencv_from(wrappedPhaseMap), pyopencv_from(shadowMask));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    PyObject* pyobj_wrappedPhaseMap = NULL;
    UMat wrappedPhaseMap;
    PyObject* pyobj_shadowMask = NULL;
    UMat shadowMask;
    PyObject* pyobj_fundamental = NULL;
    UMat fundamental;

    const char* keywords[] = { "patternImages", "wrappedPhaseMap", "shadowMask", "fundamental", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOO:structured_light_SinusoidalPattern.computePhaseMap", (char**)keywords, &pyobj_patternImages, &pyobj_wrappedPhaseMap, &pyobj_shadowMask, &pyobj_fundamental) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) &&
        pyopencv_to(pyobj_wrappedPhaseMap, wrappedPhaseMap, ArgInfo("wrappedPhaseMap", 1)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 1)) &&
        pyopencv_to(pyobj_fundamental, fundamental, ArgInfo("fundamental", 0)) )
    {
        ERRWRAP2(_self_->computePhaseMap(patternImages, wrappedPhaseMap, shadowMask, fundamental));
        return Py_BuildValue("(NN)", pyopencv_from(wrappedPhaseMap), pyopencv_from(shadowMask));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_SinusoidalPattern_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    PyObject* pyobj_parameters = NULL;
    Ptr<SinusoidalPattern::Params> parameters=makePtr<SinusoidalPattern::Params>();
    Ptr<SinusoidalPattern> retval;

    const char* keywords[] = { "parameters", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:structured_light_SinusoidalPattern.create", (char**)keywords, &pyobj_parameters) &&
        pyopencv_to(pyobj_parameters, parameters, ArgInfo("parameters", 0)) )
    {
        ERRWRAP2(retval = cv::structured_light::SinusoidalPattern::create(parameters));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_SinusoidalPattern_findProCamMatches(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::SinusoidalPattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_SinusoidalPattern_Type))
        _self_ = dynamic_cast<cv::structured_light::SinusoidalPattern*>(((pyopencv_structured_light_SinusoidalPattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_SinusoidalPattern' or its derivative)");
    {
    PyObject* pyobj_projUnwrappedPhaseMap = NULL;
    Mat projUnwrappedPhaseMap;
    PyObject* pyobj_camUnwrappedPhaseMap = NULL;
    Mat camUnwrappedPhaseMap;
    PyObject* pyobj_matches = NULL;
    vector_Mat matches;

    const char* keywords[] = { "projUnwrappedPhaseMap", "camUnwrappedPhaseMap", "matches", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:structured_light_SinusoidalPattern.findProCamMatches", (char**)keywords, &pyobj_projUnwrappedPhaseMap, &pyobj_camUnwrappedPhaseMap, &pyobj_matches) &&
        pyopencv_to(pyobj_projUnwrappedPhaseMap, projUnwrappedPhaseMap, ArgInfo("projUnwrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_camUnwrappedPhaseMap, camUnwrappedPhaseMap, ArgInfo("camUnwrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_matches, matches, ArgInfo("matches", 1)) )
    {
        ERRWRAP2(_self_->findProCamMatches(projUnwrappedPhaseMap, camUnwrappedPhaseMap, matches));
        return pyopencv_from(matches);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_projUnwrappedPhaseMap = NULL;
    UMat projUnwrappedPhaseMap;
    PyObject* pyobj_camUnwrappedPhaseMap = NULL;
    UMat camUnwrappedPhaseMap;
    PyObject* pyobj_matches = NULL;
    vector_Mat matches;

    const char* keywords[] = { "projUnwrappedPhaseMap", "camUnwrappedPhaseMap", "matches", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|O:structured_light_SinusoidalPattern.findProCamMatches", (char**)keywords, &pyobj_projUnwrappedPhaseMap, &pyobj_camUnwrappedPhaseMap, &pyobj_matches) &&
        pyopencv_to(pyobj_projUnwrappedPhaseMap, projUnwrappedPhaseMap, ArgInfo("projUnwrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_camUnwrappedPhaseMap, camUnwrappedPhaseMap, ArgInfo("camUnwrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_matches, matches, ArgInfo("matches", 1)) )
    {
        ERRWRAP2(_self_->findProCamMatches(projUnwrappedPhaseMap, camUnwrappedPhaseMap, matches));
        return pyopencv_from(matches);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_SinusoidalPattern_unwrapPhaseMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::SinusoidalPattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_SinusoidalPattern_Type))
        _self_ = dynamic_cast<cv::structured_light::SinusoidalPattern*>(((pyopencv_structured_light_SinusoidalPattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_SinusoidalPattern' or its derivative)");
    {
    PyObject* pyobj_wrappedPhaseMap = NULL;
    vector_Mat wrappedPhaseMap;
    PyObject* pyobj_unwrappedPhaseMap = NULL;
    Mat unwrappedPhaseMap;
    PyObject* pyobj_camSize = NULL;
    Size camSize;
    PyObject* pyobj_shadowMask = NULL;
    Mat shadowMask;

    const char* keywords[] = { "wrappedPhaseMap", "camSize", "unwrappedPhaseMap", "shadowMask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OO:structured_light_SinusoidalPattern.unwrapPhaseMap", (char**)keywords, &pyobj_wrappedPhaseMap, &pyobj_camSize, &pyobj_unwrappedPhaseMap, &pyobj_shadowMask) &&
        pyopencv_to(pyobj_wrappedPhaseMap, wrappedPhaseMap, ArgInfo("wrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_unwrappedPhaseMap, unwrappedPhaseMap, ArgInfo("unwrappedPhaseMap", 1)) &&
        pyopencv_to(pyobj_camSize, camSize, ArgInfo("camSize", 0)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 0)) )
    {
        ERRWRAP2(_self_->unwrapPhaseMap(wrappedPhaseMap, unwrappedPhaseMap, camSize, shadowMask));
        return pyopencv_from(unwrappedPhaseMap);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_wrappedPhaseMap = NULL;
    vector_Mat wrappedPhaseMap;
    PyObject* pyobj_unwrappedPhaseMap = NULL;
    UMat unwrappedPhaseMap;
    PyObject* pyobj_camSize = NULL;
    Size camSize;
    PyObject* pyobj_shadowMask = NULL;
    UMat shadowMask;

    const char* keywords[] = { "wrappedPhaseMap", "camSize", "unwrappedPhaseMap", "shadowMask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OO:structured_light_SinusoidalPattern.unwrapPhaseMap", (char**)keywords, &pyobj_wrappedPhaseMap, &pyobj_camSize, &pyobj_unwrappedPhaseMap, &pyobj_shadowMask) &&
        pyopencv_to(pyobj_wrappedPhaseMap, wrappedPhaseMap, ArgInfo("wrappedPhaseMap", 0)) &&
        pyopencv_to(pyobj_unwrappedPhaseMap, unwrappedPhaseMap, ArgInfo("unwrappedPhaseMap", 1)) &&
        pyopencv_to(pyobj_camSize, camSize, ArgInfo("camSize", 0)) &&
        pyopencv_to(pyobj_shadowMask, shadowMask, ArgInfo("shadowMask", 0)) )
    {
        ERRWRAP2(_self_->unwrapPhaseMap(wrappedPhaseMap, unwrappedPhaseMap, camSize, shadowMask));
        return pyopencv_from(unwrappedPhaseMap);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_structured_light_SinusoidalPattern_methods[] =
{
    {"computeDataModulationTerm", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_SinusoidalPattern_computeDataModulationTerm, 0), "computeDataModulationTerm(patternImages, shadowMask[, dataModulationTerm]) -> dataModulationTerm\n.   * @brief compute the data modulation term.\n.   * @param patternImages captured images with projected patterns.\n.   * @param dataModulationTerm Mat where the data modulation term is saved.\n.   * @param shadowMask Mask used to discard shadow regions."},
    {"computePhaseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_SinusoidalPattern_computePhaseMap, 0), "computePhaseMap(patternImages[, wrappedPhaseMap[, shadowMask[, fundamental]]]) -> wrappedPhaseMap, shadowMask\n.   * @brief Compute a wrapped phase map from sinusoidal patterns.\n.   * @param patternImages Input data to compute the wrapped phase map.\n.   * @param wrappedPhaseMap Wrapped phase map obtained through one of the three methods.\n.   * @param shadowMask Mask used to discard shadow regions.\n.   * @param fundamental Fundamental matrix used to compute epipolar lines and ease the matching step."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_SinusoidalPattern_create_cls, METH_CLASS), "create([, parameters]) -> retval\n.   * @brief Constructor.\n.   * @param parameters SinusoidalPattern parameters SinusoidalPattern::Params: width, height of the projector and patterns parameters.\n.   *"},
    {"findProCamMatches", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_SinusoidalPattern_findProCamMatches, 0), "findProCamMatches(projUnwrappedPhaseMap, camUnwrappedPhaseMap[, matches]) -> matches\n.   * @brief Find correspondences between the two devices thanks to unwrapped phase maps.\n.   * @param projUnwrappedPhaseMap Projector's unwrapped phase map.\n.   * @param camUnwrappedPhaseMap Camera's unwrapped phase map.\n.   * @param matches Images used to display correspondences map."},
    {"unwrapPhaseMap", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_SinusoidalPattern_unwrapPhaseMap, 0), "unwrapPhaseMap(wrappedPhaseMap, camSize[, unwrappedPhaseMap[, shadowMask]]) -> unwrappedPhaseMap\n.   * @brief Unwrap the wrapped phase map to remove phase ambiguities.\n.   * @param wrappedPhaseMap The wrapped phase map computed from the pattern.\n.   * @param unwrappedPhaseMap The unwrapped phase map used to find correspondences between the two devices.\n.   * @param camSize Resolution of the camera.\n.   * @param shadowMask Mask used to discard shadow regions."},

    {NULL,          NULL}
};

static void pyopencv_structured_light_SinusoidalPattern_specials(void)
{
    pyopencv_structured_light_SinusoidalPattern_Type.tp_base = &pyopencv_structured_light_StructuredLightPattern_Type;
    pyopencv_structured_light_SinusoidalPattern_Type.tp_dealloc = pyopencv_structured_light_SinusoidalPattern_dealloc;
    pyopencv_structured_light_SinusoidalPattern_Type.tp_repr = pyopencv_structured_light_SinusoidalPattern_repr;
    pyopencv_structured_light_SinusoidalPattern_Type.tp_getset = pyopencv_structured_light_SinusoidalPattern_getseters;
    pyopencv_structured_light_SinusoidalPattern_Type.tp_init = (initproc)0;
    pyopencv_structured_light_SinusoidalPattern_Type.tp_methods = pyopencv_structured_light_SinusoidalPattern_methods;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<structured_light_SinusoidalPattern_Params %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_height(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->height);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_height(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the height attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->height) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_horizontal(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->horizontal);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_horizontal(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the horizontal attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->horizontal) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_methodId(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->methodId);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_methodId(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the methodId attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->methodId) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_nbrOfPeriods(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->nbrOfPeriods);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_nbrOfPeriods(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the nbrOfPeriods attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->nbrOfPeriods) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_nbrOfPixelsBetweenMarkers(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->nbrOfPixelsBetweenMarkers);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_nbrOfPixelsBetweenMarkers(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the nbrOfPixelsBetweenMarkers attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->nbrOfPixelsBetweenMarkers) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_setMarkers(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->setMarkers);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_setMarkers(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the setMarkers attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->setMarkers) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_shiftValue(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->shiftValue);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_shiftValue(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the shiftValue attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->shiftValue) ? 0 : -1;
}

static PyObject* pyopencv_structured_light_SinusoidalPattern_Params_get_width(pyopencv_structured_light_SinusoidalPattern_Params_t* p, void *closure)
{
    return pyopencv_from(p->v->width);
}

static int pyopencv_structured_light_SinusoidalPattern_Params_set_width(pyopencv_structured_light_SinusoidalPattern_Params_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the width attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->width) ? 0 : -1;
}


static PyGetSetDef pyopencv_structured_light_SinusoidalPattern_Params_getseters[] =
{
    {(char*)"height", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_height, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_height, (char*)"height", NULL},
    {(char*)"horizontal", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_horizontal, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_horizontal, (char*)"horizontal", NULL},
    {(char*)"methodId", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_methodId, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_methodId, (char*)"methodId", NULL},
    {(char*)"nbrOfPeriods", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_nbrOfPeriods, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_nbrOfPeriods, (char*)"nbrOfPeriods", NULL},
    {(char*)"nbrOfPixelsBetweenMarkers", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_nbrOfPixelsBetweenMarkers, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_nbrOfPixelsBetweenMarkers, (char*)"nbrOfPixelsBetweenMarkers", NULL},
    {(char*)"setMarkers", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_setMarkers, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_setMarkers, (char*)"setMarkers", NULL},
    {(char*)"shiftValue", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_shiftValue, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_shiftValue, (char*)"shiftValue", NULL},
    {(char*)"width", (getter)pyopencv_structured_light_SinusoidalPattern_Params_get_width, (setter)pyopencv_structured_light_SinusoidalPattern_Params_set_width, (char*)"width", NULL},
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_structured_light_structured_light_SinusoidalPattern_Params_SinusoidalPattern_Params(pyopencv_structured_light_SinusoidalPattern_Params_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;


    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        new (&(self->v)) Ptr<cv::structured_light::SinusoidalPattern::Params>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::structured_light::SinusoidalPattern::Params()));
        return 0;
    }

    return -1;
}



static PyMethodDef pyopencv_structured_light_SinusoidalPattern_Params_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_structured_light_SinusoidalPattern_Params_specials(void)
{
    pyopencv_structured_light_SinusoidalPattern_Params_Type.tp_base = NULL;
    pyopencv_structured_light_SinusoidalPattern_Params_Type.tp_dealloc = pyopencv_structured_light_SinusoidalPattern_Params_dealloc;
    pyopencv_structured_light_SinusoidalPattern_Params_Type.tp_repr = pyopencv_structured_light_SinusoidalPattern_Params_repr;
    pyopencv_structured_light_SinusoidalPattern_Params_Type.tp_getset = pyopencv_structured_light_SinusoidalPattern_Params_getseters;
    pyopencv_structured_light_SinusoidalPattern_Params_Type.tp_init = (initproc)pyopencv_cv_structured_light_structured_light_SinusoidalPattern_Params_SinusoidalPattern_Params;
    pyopencv_structured_light_SinusoidalPattern_Params_Type.tp_methods = pyopencv_structured_light_SinusoidalPattern_Params_methods;
}

static PyObject* pyopencv_structured_light_StructuredLightPattern_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<structured_light_StructuredLightPattern %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_structured_light_StructuredLightPattern_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_structured_light_structured_light_StructuredLightPattern_decode(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::StructuredLightPattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_StructuredLightPattern_Type))
        _self_ = dynamic_cast<cv::structured_light::StructuredLightPattern*>(((pyopencv_structured_light_StructuredLightPattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_StructuredLightPattern' or its derivative)");
    {
    PyObject* pyobj_patternImages = NULL;
    vector_vector_Mat patternImages;
    PyObject* pyobj_disparityMap = NULL;
    Mat disparityMap;
    PyObject* pyobj_blackImages = NULL;
    vector_Mat blackImages;
    PyObject* pyobj_whiteImages = NULL;
    vector_Mat whiteImages;
    int flags=DECODE_3D_UNDERWORLD;
    bool retval;

    const char* keywords[] = { "patternImages", "disparityMap", "blackImages", "whiteImages", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOOi:structured_light_StructuredLightPattern.decode", (char**)keywords, &pyobj_patternImages, &pyobj_disparityMap, &pyobj_blackImages, &pyobj_whiteImages, &flags) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) &&
        pyopencv_to(pyobj_disparityMap, disparityMap, ArgInfo("disparityMap", 1)) &&
        pyopencv_to(pyobj_blackImages, blackImages, ArgInfo("blackImages", 0)) &&
        pyopencv_to(pyobj_whiteImages, whiteImages, ArgInfo("whiteImages", 0)) )
    {
        ERRWRAP2(retval = _self_->decode(patternImages, disparityMap, blackImages, whiteImages, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(disparityMap));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_patternImages = NULL;
    vector_vector_Mat patternImages;
    PyObject* pyobj_disparityMap = NULL;
    UMat disparityMap;
    PyObject* pyobj_blackImages = NULL;
    vector_Mat blackImages;
    PyObject* pyobj_whiteImages = NULL;
    vector_Mat whiteImages;
    int flags=DECODE_3D_UNDERWORLD;
    bool retval;

    const char* keywords[] = { "patternImages", "disparityMap", "blackImages", "whiteImages", "flags", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OOOi:structured_light_StructuredLightPattern.decode", (char**)keywords, &pyobj_patternImages, &pyobj_disparityMap, &pyobj_blackImages, &pyobj_whiteImages, &flags) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 0)) &&
        pyopencv_to(pyobj_disparityMap, disparityMap, ArgInfo("disparityMap", 1)) &&
        pyopencv_to(pyobj_blackImages, blackImages, ArgInfo("blackImages", 0)) &&
        pyopencv_to(pyobj_whiteImages, whiteImages, ArgInfo("whiteImages", 0)) )
    {
        ERRWRAP2(retval = _self_->decode(patternImages, disparityMap, blackImages, whiteImages, flags));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(disparityMap));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_structured_light_structured_light_StructuredLightPattern_generate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::structured_light;

    cv::structured_light::StructuredLightPattern* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_structured_light_StructuredLightPattern_Type))
        _self_ = dynamic_cast<cv::structured_light::StructuredLightPattern*>(((pyopencv_structured_light_StructuredLightPattern_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'structured_light_StructuredLightPattern' or its derivative)");
    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    bool retval;

    const char* keywords[] = { "patternImages", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:structured_light_StructuredLightPattern.generate", (char**)keywords, &pyobj_patternImages) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 1)) )
    {
        ERRWRAP2(retval = _self_->generate(patternImages));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(patternImages));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_patternImages = NULL;
    vector_Mat patternImages;
    bool retval;

    const char* keywords[] = { "patternImages", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:structured_light_StructuredLightPattern.generate", (char**)keywords, &pyobj_patternImages) &&
        pyopencv_to(pyobj_patternImages, patternImages, ArgInfo("patternImages", 1)) )
    {
        ERRWRAP2(retval = _self_->generate(patternImages));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(patternImages));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_structured_light_StructuredLightPattern_methods[] =
{
    {"decode", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_StructuredLightPattern_decode, 0), "decode(patternImages[, disparityMap[, blackImages[, whiteImages[, flags]]]]) -> retval, disparityMap\n.   @brief Decodes the structured light pattern, generating a disparity map\n.   \n.   @param patternImages The acquired pattern images to decode (vector<vector<Mat>>), loaded as grayscale and previously rectified.\n.   @param disparityMap The decoding result: a CV_64F Mat at image resolution, storing the computed disparity map.\n.   @param blackImages The all-black images needed for shadowMasks computation.\n.   @param whiteImages The all-white images needed for shadowMasks computation.\n.   @param flags Flags setting decoding algorithms. Default: DECODE_3D_UNDERWORLD.\n.   @note All the images must be at the same resolution."},
    {"generate", CV_PY_FN_WITH_KW_(pyopencv_cv_structured_light_structured_light_StructuredLightPattern_generate, 0), "generate([, patternImages]) -> retval, patternImages\n.   @brief Generates the structured light pattern to project.\n.   \n.   @param patternImages The generated pattern: a vector<Mat>, in which each image is a CV_8U Mat at projector's resolution."},

    {NULL,          NULL}
};

static void pyopencv_structured_light_StructuredLightPattern_specials(void)
{
    pyopencv_structured_light_StructuredLightPattern_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_structured_light_StructuredLightPattern_Type.tp_dealloc = pyopencv_structured_light_StructuredLightPattern_dealloc;
    pyopencv_structured_light_StructuredLightPattern_Type.tp_repr = pyopencv_structured_light_StructuredLightPattern_repr;
    pyopencv_structured_light_StructuredLightPattern_Type.tp_getset = pyopencv_structured_light_StructuredLightPattern_getseters;
    pyopencv_structured_light_StructuredLightPattern_Type.tp_init = (initproc)0;
    pyopencv_structured_light_StructuredLightPattern_Type.tp_methods = pyopencv_structured_light_StructuredLightPattern_methods;
}

static PyObject* pyopencv_xfeatures2d_FREAK_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_FREAK %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_FREAK_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    bool orientationNormalized=true;
    bool scaleNormalized=true;
    float patternScale=22.0f;
    int nOctaves=4;
    PyObject* pyobj_selectedPairs = NULL;
    vector_int selectedPairs=std::vector<int>();
    Ptr<FREAK> retval;

    const char* keywords[] = { "orientationNormalized", "scaleNormalized", "patternScale", "nOctaves", "selectedPairs", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|bbfiO:xfeatures2d_FREAK.create", (char**)keywords, &orientationNormalized, &scaleNormalized, &patternScale, &nOctaves, &pyobj_selectedPairs) &&
        pyopencv_to(pyobj_selectedPairs, selectedPairs, ArgInfo("selectedPairs", 0)) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::FREAK::create(orientationNormalized, scaleNormalized, patternScale, nOctaves, selectedPairs));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_FREAK_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_FREAK_create_cls, METH_CLASS), "create([, orientationNormalized[, scaleNormalized[, patternScale[, nOctaves[, selectedPairs]]]]]) -> retval\n.   @param orientationNormalized Enable orientation normalization.\n.   @param scaleNormalized Enable scale normalization.\n.   @param patternScale Scaling of the description pattern.\n.   @param nOctaves Number of octaves covered by the detected keypoints.\n.   @param selectedPairs (Optional) user defined selected pairs indexes,"},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_FREAK_specials(void)
{
    pyopencv_xfeatures2d_FREAK_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_FREAK_Type.tp_dealloc = pyopencv_xfeatures2d_FREAK_dealloc;
    pyopencv_xfeatures2d_FREAK_Type.tp_repr = pyopencv_xfeatures2d_FREAK_repr;
    pyopencv_xfeatures2d_FREAK_Type.tp_getset = pyopencv_xfeatures2d_FREAK_getseters;
    pyopencv_xfeatures2d_FREAK_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_FREAK_Type.tp_methods = pyopencv_xfeatures2d_FREAK_methods;
}

static PyObject* pyopencv_xfeatures2d_StarDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_StarDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_StarDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int maxSize=45;
    int responseThreshold=30;
    int lineThresholdProjected=10;
    int lineThresholdBinarized=8;
    int suppressNonmaxSize=5;
    Ptr<StarDetector> retval;

    const char* keywords[] = { "maxSize", "responseThreshold", "lineThresholdProjected", "lineThresholdBinarized", "suppressNonmaxSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiiii:xfeatures2d_StarDetector.create", (char**)keywords, &maxSize, &responseThreshold, &lineThresholdProjected, &lineThresholdBinarized, &suppressNonmaxSize) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::StarDetector::create(maxSize, responseThreshold, lineThresholdProjected, lineThresholdBinarized, suppressNonmaxSize));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_StarDetector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_StarDetector_create_cls, METH_CLASS), "create([, maxSize[, responseThreshold[, lineThresholdProjected[, lineThresholdBinarized[, suppressNonmaxSize]]]]]) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_StarDetector_specials(void)
{
    pyopencv_xfeatures2d_StarDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_StarDetector_Type.tp_dealloc = pyopencv_xfeatures2d_StarDetector_dealloc;
    pyopencv_xfeatures2d_StarDetector_Type.tp_repr = pyopencv_xfeatures2d_StarDetector_repr;
    pyopencv_xfeatures2d_StarDetector_Type.tp_getset = pyopencv_xfeatures2d_StarDetector_getseters;
    pyopencv_xfeatures2d_StarDetector_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_StarDetector_Type.tp_methods = pyopencv_xfeatures2d_StarDetector_methods;
}

static PyObject* pyopencv_xfeatures2d_BriefDescriptorExtractor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_BriefDescriptorExtractor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_BriefDescriptorExtractor_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int bytes=32;
    bool use_orientation=false;
    Ptr<BriefDescriptorExtractor> retval;

    const char* keywords[] = { "bytes", "use_orientation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ib:xfeatures2d_BriefDescriptorExtractor.create", (char**)keywords, &bytes, &use_orientation) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::BriefDescriptorExtractor::create(bytes, use_orientation));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_BriefDescriptorExtractor_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BriefDescriptorExtractor_create_cls, METH_CLASS), "create([, bytes[, use_orientation]]) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_BriefDescriptorExtractor_specials(void)
{
    pyopencv_xfeatures2d_BriefDescriptorExtractor_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_BriefDescriptorExtractor_Type.tp_dealloc = pyopencv_xfeatures2d_BriefDescriptorExtractor_dealloc;
    pyopencv_xfeatures2d_BriefDescriptorExtractor_Type.tp_repr = pyopencv_xfeatures2d_BriefDescriptorExtractor_repr;
    pyopencv_xfeatures2d_BriefDescriptorExtractor_Type.tp_getset = pyopencv_xfeatures2d_BriefDescriptorExtractor_getseters;
    pyopencv_xfeatures2d_BriefDescriptorExtractor_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_BriefDescriptorExtractor_Type.tp_methods = pyopencv_xfeatures2d_BriefDescriptorExtractor_methods;
}

static PyObject* pyopencv_xfeatures2d_LUCID_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_LUCID %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_LUCID_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int lucid_kernel=1;
    int blur_kernel=2;
    Ptr<LUCID> retval;

    const char* keywords[] = { "lucid_kernel", "blur_kernel", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ii:xfeatures2d_LUCID.create", (char**)keywords, &lucid_kernel, &blur_kernel) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::LUCID::create(lucid_kernel, blur_kernel));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_LUCID_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LUCID_create_cls, METH_CLASS), "create([, lucid_kernel[, blur_kernel]]) -> retval\n.   * @param lucid_kernel kernel for descriptor construction, where 1=3x3, 2=5x5, 3=7x7 and so forth\n.   * @param blur_kernel kernel for blurring image prior to descriptor construction, where 1=3x3, 2=5x5, 3=7x7 and so forth"},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_LUCID_specials(void)
{
    pyopencv_xfeatures2d_LUCID_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_LUCID_Type.tp_dealloc = pyopencv_xfeatures2d_LUCID_dealloc;
    pyopencv_xfeatures2d_LUCID_Type.tp_repr = pyopencv_xfeatures2d_LUCID_repr;
    pyopencv_xfeatures2d_LUCID_Type.tp_getset = pyopencv_xfeatures2d_LUCID_getseters;
    pyopencv_xfeatures2d_LUCID_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_LUCID_Type.tp_methods = pyopencv_xfeatures2d_LUCID_methods;
}

static PyObject* pyopencv_xfeatures2d_LATCH_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_LATCH %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_LATCH_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int bytes=32;
    bool rotationInvariance=true;
    int half_ssd_size=3;
    double sigma=2.0;
    Ptr<LATCH> retval;

    const char* keywords[] = { "bytes", "rotationInvariance", "half_ssd_size", "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ibid:xfeatures2d_LATCH.create", (char**)keywords, &bytes, &rotationInvariance, &half_ssd_size, &sigma) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::LATCH::create(bytes, rotationInvariance, half_ssd_size, sigma));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_LATCH_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_LATCH_create_cls, METH_CLASS), "create([, bytes[, rotationInvariance[, half_ssd_size[, sigma]]]]) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_LATCH_specials(void)
{
    pyopencv_xfeatures2d_LATCH_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_LATCH_Type.tp_dealloc = pyopencv_xfeatures2d_LATCH_dealloc;
    pyopencv_xfeatures2d_LATCH_Type.tp_repr = pyopencv_xfeatures2d_LATCH_repr;
    pyopencv_xfeatures2d_LATCH_Type.tp_getset = pyopencv_xfeatures2d_LATCH_getseters;
    pyopencv_xfeatures2d_LATCH_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_LATCH_Type.tp_methods = pyopencv_xfeatures2d_LATCH_methods;
}

static PyObject* pyopencv_xfeatures2d_DAISY_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_DAISY %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_DAISY_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    {
    float radius=15;
    int q_radius=3;
    int q_theta=8;
    int q_hist=8;
    int norm=DAISY::NRM_NONE;
    PyObject* pyobj_H = NULL;
    Mat H;
    bool interpolation=true;
    bool use_orientation=false;
    Ptr<DAISY> retval;

    const char* keywords[] = { "radius", "q_radius", "q_theta", "q_hist", "norm", "H", "interpolation", "use_orientation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|fiiiiObb:xfeatures2d_DAISY.create", (char**)keywords, &radius, &q_radius, &q_theta, &q_hist, &norm, &pyobj_H, &interpolation, &use_orientation) &&
        pyopencv_to(pyobj_H, H, ArgInfo("H", 0)) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::DAISY::create(radius, q_radius, q_theta, q_hist, norm, H, interpolation, use_orientation));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    float radius=15;
    int q_radius=3;
    int q_theta=8;
    int q_hist=8;
    int norm=DAISY::NRM_NONE;
    PyObject* pyobj_H = NULL;
    UMat H;
    bool interpolation=true;
    bool use_orientation=false;
    Ptr<DAISY> retval;

    const char* keywords[] = { "radius", "q_radius", "q_theta", "q_hist", "norm", "H", "interpolation", "use_orientation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|fiiiiObb:xfeatures2d_DAISY.create", (char**)keywords, &radius, &q_radius, &q_theta, &q_hist, &norm, &pyobj_H, &interpolation, &use_orientation) &&
        pyopencv_to(pyobj_H, H, ArgInfo("H", 0)) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::DAISY::create(radius, q_radius, q_theta, q_hist, norm, H, interpolation, use_orientation));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_DAISY_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_DAISY_create_cls, METH_CLASS), "create([, radius[, q_radius[, q_theta[, q_hist[, norm[, H[, interpolation[, use_orientation]]]]]]]]) -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_DAISY_specials(void)
{
    pyopencv_xfeatures2d_DAISY_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_DAISY_Type.tp_dealloc = pyopencv_xfeatures2d_DAISY_dealloc;
    pyopencv_xfeatures2d_DAISY_Type.tp_repr = pyopencv_xfeatures2d_DAISY_repr;
    pyopencv_xfeatures2d_DAISY_Type.tp_getset = pyopencv_xfeatures2d_DAISY_getseters;
    pyopencv_xfeatures2d_DAISY_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_DAISY_Type.tp_methods = pyopencv_xfeatures2d_DAISY_methods;
}

static PyObject* pyopencv_xfeatures2d_MSDDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_MSDDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_MSDDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_xfeatures2d_MSDDetector_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_MSDDetector_specials(void)
{
    pyopencv_xfeatures2d_MSDDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_MSDDetector_Type.tp_dealloc = pyopencv_xfeatures2d_MSDDetector_dealloc;
    pyopencv_xfeatures2d_MSDDetector_Type.tp_repr = pyopencv_xfeatures2d_MSDDetector_repr;
    pyopencv_xfeatures2d_MSDDetector_Type.tp_getset = pyopencv_xfeatures2d_MSDDetector_getseters;
    pyopencv_xfeatures2d_MSDDetector_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_MSDDetector_Type.tp_methods = pyopencv_xfeatures2d_MSDDetector_methods;
}

static PyObject* pyopencv_xfeatures2d_VGG_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_VGG %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_VGG_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int desc=VGG::VGG_120;
    float isigma=1.4f;
    bool img_normalize=true;
    bool use_scale_orientation=true;
    float scale_factor=6.25f;
    bool dsc_normalize=false;
    Ptr<VGG> retval;

    const char* keywords[] = { "desc", "isigma", "img_normalize", "use_scale_orientation", "scale_factor", "dsc_normalize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ifbbfb:xfeatures2d_VGG.create", (char**)keywords, &desc, &isigma, &img_normalize, &use_scale_orientation, &scale_factor, &dsc_normalize) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::VGG::create(desc, isigma, img_normalize, use_scale_orientation, scale_factor, dsc_normalize));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getScaleFactor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScaleFactor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeDescriptor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseNormalizeDescriptor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseNormalizeImage());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseScaleOrientation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseScaleOrientation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setScaleFactor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_VGG.setScaleFactor", (char**)keywords, &scale_factor) )
    {
        ERRWRAP2(_self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    float isigma=0.f;

    const char* keywords[] = { "isigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_VGG.setSigma", (char**)keywords, &isigma) )
    {
        ERRWRAP2(_self_->setSigma(isigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeDescriptor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    bool dsc_normalize=0;

    const char* keywords[] = { "dsc_normalize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:xfeatures2d_VGG.setUseNormalizeDescriptor", (char**)keywords, &dsc_normalize) )
    {
        ERRWRAP2(_self_->setUseNormalizeDescriptor(dsc_normalize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    bool img_normalize=0;

    const char* keywords[] = { "img_normalize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:xfeatures2d_VGG.setUseNormalizeImage", (char**)keywords, &img_normalize) )
    {
        ERRWRAP2(_self_->setUseNormalizeImage(img_normalize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseScaleOrientation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::VGG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_VGG_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::VGG*>(((pyopencv_xfeatures2d_VGG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_VGG' or its derivative)");
    bool use_scale_orientation=0;

    const char* keywords[] = { "use_scale_orientation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:xfeatures2d_VGG.setUseScaleOrientation", (char**)keywords, &use_scale_orientation) )
    {
        ERRWRAP2(_self_->setUseScaleOrientation(use_scale_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_VGG_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_create_cls, METH_CLASS), "create([, desc[, isigma[, img_normalize[, use_scale_orientation[, scale_factor[, dsc_normalize]]]]]]) -> retval\n."},
    {"getScaleFactor", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getSigma, 0), "getSigma() -> retval\n."},
    {"getUseNormalizeDescriptor", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeDescriptor, 0), "getUseNormalizeDescriptor() -> retval\n."},
    {"getUseNormalizeImage", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseNormalizeImage, 0), "getUseNormalizeImage() -> retval\n."},
    {"getUseScaleOrientation", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_getUseScaleOrientation, 0), "getUseScaleOrientation() -> retval\n."},
    {"setScaleFactor", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},
    {"setSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setSigma, 0), "setSigma(isigma) -> None\n."},
    {"setUseNormalizeDescriptor", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeDescriptor, 0), "setUseNormalizeDescriptor(dsc_normalize) -> None\n."},
    {"setUseNormalizeImage", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseNormalizeImage, 0), "setUseNormalizeImage(img_normalize) -> None\n."},
    {"setUseScaleOrientation", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_VGG_setUseScaleOrientation, 0), "setUseScaleOrientation(use_scale_orientation) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_VGG_specials(void)
{
    pyopencv_xfeatures2d_VGG_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_VGG_Type.tp_dealloc = pyopencv_xfeatures2d_VGG_dealloc;
    pyopencv_xfeatures2d_VGG_Type.tp_repr = pyopencv_xfeatures2d_VGG_repr;
    pyopencv_xfeatures2d_VGG_Type.tp_getset = pyopencv_xfeatures2d_VGG_getseters;
    pyopencv_xfeatures2d_VGG_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_VGG_Type.tp_methods = pyopencv_xfeatures2d_VGG_methods;
}

static PyObject* pyopencv_xfeatures2d_BoostDesc_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_BoostDesc %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_BoostDesc_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int desc=BoostDesc::BINBOOST_256;
    bool use_scale_orientation=true;
    float scale_factor=6.25f;
    Ptr<BoostDesc> retval;

    const char* keywords[] = { "desc", "use_scale_orientation", "scale_factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|ibf:xfeatures2d_BoostDesc.create", (char**)keywords, &desc, &use_scale_orientation, &scale_factor) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::BoostDesc::create(desc, use_scale_orientation, scale_factor));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getScaleFactor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::BoostDesc* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_BoostDesc_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::BoostDesc*>(((pyopencv_xfeatures2d_BoostDesc_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getScaleFactor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getUseScaleOrientation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::BoostDesc* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_BoostDesc_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::BoostDesc*>(((pyopencv_xfeatures2d_BoostDesc_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseScaleOrientation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setScaleFactor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::BoostDesc* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_BoostDesc_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::BoostDesc*>(((pyopencv_xfeatures2d_BoostDesc_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    float scale_factor=0.f;

    const char* keywords[] = { "scale_factor", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_BoostDesc.setScaleFactor", (char**)keywords, &scale_factor) )
    {
        ERRWRAP2(_self_->setScaleFactor(scale_factor));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setUseScaleOrientation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::BoostDesc* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_BoostDesc_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::BoostDesc*>(((pyopencv_xfeatures2d_BoostDesc_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_BoostDesc' or its derivative)");
    bool use_scale_orientation=0;

    const char* keywords[] = { "use_scale_orientation", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:xfeatures2d_BoostDesc.setUseScaleOrientation", (char**)keywords, &use_scale_orientation) )
    {
        ERRWRAP2(_self_->setUseScaleOrientation(use_scale_orientation));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_BoostDesc_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_create_cls, METH_CLASS), "create([, desc[, use_scale_orientation[, scale_factor]]]) -> retval\n."},
    {"getScaleFactor", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getScaleFactor, 0), "getScaleFactor() -> retval\n."},
    {"getUseScaleOrientation", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_getUseScaleOrientation, 0), "getUseScaleOrientation() -> retval\n."},
    {"setScaleFactor", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setScaleFactor, 0), "setScaleFactor(scale_factor) -> None\n."},
    {"setUseScaleOrientation", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_BoostDesc_setUseScaleOrientation, 0), "setUseScaleOrientation(use_scale_orientation) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_BoostDesc_specials(void)
{
    pyopencv_xfeatures2d_BoostDesc_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_BoostDesc_Type.tp_dealloc = pyopencv_xfeatures2d_BoostDesc_dealloc;
    pyopencv_xfeatures2d_BoostDesc_Type.tp_repr = pyopencv_xfeatures2d_BoostDesc_repr;
    pyopencv_xfeatures2d_BoostDesc_Type.tp_getset = pyopencv_xfeatures2d_BoostDesc_getseters;
    pyopencv_xfeatures2d_BoostDesc_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_BoostDesc_Type.tp_methods = pyopencv_xfeatures2d_BoostDesc_methods;
}

static PyObject* pyopencv_xfeatures2d_PCTSignatures_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_PCTSignatures %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_PCTSignatures_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignature(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_signature = NULL;
    Mat signature;

    const char* keywords[] = { "image", "signature", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:xfeatures2d_PCTSignatures.computeSignature", (char**)keywords, &pyobj_image, &pyobj_signature) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_signature, signature, ArgInfo("signature", 1)) )
    {
        ERRWRAP2(_self_->computeSignature(image, signature));
        return pyopencv_from(signature);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_signature = NULL;
    UMat signature;

    const char* keywords[] = { "image", "signature", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:xfeatures2d_PCTSignatures.computeSignature", (char**)keywords, &pyobj_image, &pyobj_signature) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_signature, signature, ArgInfo("signature", 1)) )
    {
        ERRWRAP2(_self_->computeSignature(image, signature));
        return pyopencv_from(signature);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_signatures = NULL;
    vector_Mat signatures;

    const char* keywords[] = { "images", "signatures", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:xfeatures2d_PCTSignatures.computeSignatures", (char**)keywords, &pyobj_images, &pyobj_signatures) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_signatures, signatures, ArgInfo("signatures", 0)) )
    {
        ERRWRAP2(_self_->computeSignatures(images, signatures));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_signatures = NULL;
    vector_Mat signatures;

    const char* keywords[] = { "images", "signatures", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:xfeatures2d_PCTSignatures.computeSignatures", (char**)keywords, &pyobj_images, &pyobj_signatures) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_signatures, signatures, ArgInfo("signatures", 0)) )
    {
        ERRWRAP2(_self_->computeSignatures(images, signatures));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    {
    int initSampleCount=2000;
    int initSeedCount=400;
    int pointDistribution=0;
    Ptr<PCTSignatures> retval;

    const char* keywords[] = { "initSampleCount", "initSeedCount", "pointDistribution", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iii:xfeatures2d_PCTSignatures.create", (char**)keywords, &initSampleCount, &initSeedCount, &pointDistribution) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::PCTSignatures::create(initSampleCount, initSeedCount, pointDistribution));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_initSamplingPoints = NULL;
    vector_Point2f initSamplingPoints;
    int initSeedCount=0;
    Ptr<PCTSignatures> retval;

    const char* keywords[] = { "initSamplingPoints", "initSeedCount", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:xfeatures2d_PCTSignatures.create", (char**)keywords, &pyobj_initSamplingPoints, &initSeedCount) &&
        pyopencv_to(pyobj_initSamplingPoints, initSamplingPoints, ArgInfo("initSamplingPoints", 0)) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::PCTSignatures::create(initSamplingPoints, initSeedCount));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_initSamplingPoints = NULL;
    vector_Point2f initSamplingPoints;
    PyObject* pyobj_initClusterSeedIndexes = NULL;
    vector_int initClusterSeedIndexes;
    Ptr<PCTSignatures> retval;

    const char* keywords[] = { "initSamplingPoints", "initClusterSeedIndexes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:xfeatures2d_PCTSignatures.create", (char**)keywords, &pyobj_initSamplingPoints, &pyobj_initClusterSeedIndexes) &&
        pyopencv_to(pyobj_initSamplingPoints, initSamplingPoints, ArgInfo("initSamplingPoints", 0)) &&
        pyopencv_to(pyobj_initClusterSeedIndexes, initClusterSeedIndexes, ArgInfo("initClusterSeedIndexes", 0)) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::PCTSignatures::create(initSamplingPoints, initClusterSeedIndexes));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_drawSignature_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    {
    PyObject* pyobj_source = NULL;
    Mat source;
    PyObject* pyobj_signature = NULL;
    Mat signature;
    PyObject* pyobj_result = NULL;
    Mat result;
    float radiusToShorterSideRatio=1.0 / 8;
    int borderThickness=1;

    const char* keywords[] = { "source", "signature", "result", "radiusToShorterSideRatio", "borderThickness", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Ofi:xfeatures2d_PCTSignatures.drawSignature", (char**)keywords, &pyobj_source, &pyobj_signature, &pyobj_result, &radiusToShorterSideRatio, &borderThickness) &&
        pyopencv_to(pyobj_source, source, ArgInfo("source", 0)) &&
        pyopencv_to(pyobj_signature, signature, ArgInfo("signature", 0)) &&
        pyopencv_to(pyobj_result, result, ArgInfo("result", 1)) )
    {
        ERRWRAP2(cv::xfeatures2d::PCTSignatures::drawSignature(source, signature, result, radiusToShorterSideRatio, borderThickness));
        return pyopencv_from(result);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_source = NULL;
    UMat source;
    PyObject* pyobj_signature = NULL;
    UMat signature;
    PyObject* pyobj_result = NULL;
    UMat result;
    float radiusToShorterSideRatio=1.0 / 8;
    int borderThickness=1;

    const char* keywords[] = { "source", "signature", "result", "radiusToShorterSideRatio", "borderThickness", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Ofi:xfeatures2d_PCTSignatures.drawSignature", (char**)keywords, &pyobj_source, &pyobj_signature, &pyobj_result, &radiusToShorterSideRatio, &borderThickness) &&
        pyopencv_to(pyobj_source, source, ArgInfo("source", 0)) &&
        pyopencv_to(pyobj_signature, signature, ArgInfo("signature", 0)) &&
        pyopencv_to(pyobj_result, result, ArgInfo("result", 1)) )
    {
        ERRWRAP2(cv::xfeatures2d::PCTSignatures::drawSignature(source, signature, result, radiusToShorterSideRatio, borderThickness));
        return pyopencv_from(result);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_generateInitPoints_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    PyObject* pyobj_initPoints = NULL;
    vector_Point2f initPoints;
    int count=0;
    int pointDistribution=0;

    const char* keywords[] = { "initPoints", "count", "pointDistribution", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oii:xfeatures2d_PCTSignatures.generateInitPoints", (char**)keywords, &pyobj_initPoints, &count, &pointDistribution) &&
        pyopencv_to(pyobj_initPoints, initPoints, ArgInfo("initPoints", 0)) )
    {
        ERRWRAP2(cv::xfeatures2d::PCTSignatures::generateInitPoints(initPoints, count, pointDistribution));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getClusterMinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getClusterMinSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDistanceFunction(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDistanceFunction());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDropThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDropThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getGrayscaleBits(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGrayscaleBits());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInitSeedCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedIndexes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    std::vector<int> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getInitSeedIndexes());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getIterationCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIterationCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getJoiningDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getJoiningDistance());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getMaxClustersCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxClustersCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSampleCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSampleCount());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSamplingPoints(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    std::vector<Point2f> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSamplingPoints());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightA(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightA());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightB(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightB());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightContrast(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightContrast());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightEntropy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightEntropy());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightL(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightL());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightX(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightX());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightY(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWeightY());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWindowRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getWindowRadius());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setClusterMinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int clusterMinSize=0;

    const char* keywords[] = { "clusterMinSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_PCTSignatures.setClusterMinSize", (char**)keywords, &clusterMinSize) )
    {
        ERRWRAP2(_self_->setClusterMinSize(clusterMinSize));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDistanceFunction(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int distanceFunction=0;

    const char* keywords[] = { "distanceFunction", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_PCTSignatures.setDistanceFunction", (char**)keywords, &distanceFunction) )
    {
        ERRWRAP2(_self_->setDistanceFunction(distanceFunction));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDropThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float dropThreshold=0.f;

    const char* keywords[] = { "dropThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setDropThreshold", (char**)keywords, &dropThreshold) )
    {
        ERRWRAP2(_self_->setDropThreshold(dropThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setGrayscaleBits(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int grayscaleBits=0;

    const char* keywords[] = { "grayscaleBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_PCTSignatures.setGrayscaleBits", (char**)keywords, &grayscaleBits) )
    {
        ERRWRAP2(_self_->setGrayscaleBits(grayscaleBits));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setInitSeedIndexes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    PyObject* pyobj_initSeedIndexes = NULL;
    vector_int initSeedIndexes;

    const char* keywords[] = { "initSeedIndexes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:xfeatures2d_PCTSignatures.setInitSeedIndexes", (char**)keywords, &pyobj_initSeedIndexes) &&
        pyopencv_to(pyobj_initSeedIndexes, initSeedIndexes, ArgInfo("initSeedIndexes", 0)) )
    {
        ERRWRAP2(_self_->setInitSeedIndexes(initSeedIndexes));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setIterationCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int iterationCount=0;

    const char* keywords[] = { "iterationCount", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_PCTSignatures.setIterationCount", (char**)keywords, &iterationCount) )
    {
        ERRWRAP2(_self_->setIterationCount(iterationCount));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setJoiningDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float joiningDistance=0.f;

    const char* keywords[] = { "joiningDistance", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setJoiningDistance", (char**)keywords, &joiningDistance) )
    {
        ERRWRAP2(_self_->setJoiningDistance(joiningDistance));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setMaxClustersCount(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int maxClustersCount=0;

    const char* keywords[] = { "maxClustersCount", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_PCTSignatures.setMaxClustersCount", (char**)keywords, &maxClustersCount) )
    {
        ERRWRAP2(_self_->setMaxClustersCount(maxClustersCount));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setSamplingPoints(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    PyObject* pyobj_samplingPoints = NULL;
    vector_Point2f samplingPoints;

    const char* keywords[] = { "samplingPoints", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:xfeatures2d_PCTSignatures.setSamplingPoints", (char**)keywords, &pyobj_samplingPoints) &&
        pyopencv_to(pyobj_samplingPoints, samplingPoints, ArgInfo("samplingPoints", 0)) )
    {
        ERRWRAP2(_self_->setSamplingPoints(samplingPoints));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int idx=0;
    float value=0.f;

    const char* keywords[] = { "idx", "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "if:xfeatures2d_PCTSignatures.setTranslation", (char**)keywords, &idx, &value) )
    {
        ERRWRAP2(_self_->setTranslation(idx, value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    PyObject* pyobj_translations = NULL;
    vector_float translations;

    const char* keywords[] = { "translations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:xfeatures2d_PCTSignatures.setTranslations", (char**)keywords, &pyobj_translations) &&
        pyopencv_to(pyobj_translations, translations, ArgInfo("translations", 0)) )
    {
        ERRWRAP2(_self_->setTranslations(translations));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeight(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int idx=0;
    float value=0.f;

    const char* keywords[] = { "idx", "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "if:xfeatures2d_PCTSignatures.setWeight", (char**)keywords, &idx, &value) )
    {
        ERRWRAP2(_self_->setWeight(idx, value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightA(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightA", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightA(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightB(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightB", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightB(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightContrast(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightContrast", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightContrast(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightEntropy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightEntropy", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightEntropy(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightL(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightL", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightL(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightX(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightX", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightX(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightY(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    float weight=0.f;

    const char* keywords[] = { "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:xfeatures2d_PCTSignatures.setWeightY", (char**)keywords, &weight) )
    {
        ERRWRAP2(_self_->setWeightY(weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeights(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    PyObject* pyobj_weights = NULL;
    vector_float weights;

    const char* keywords[] = { "weights", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:xfeatures2d_PCTSignatures.setWeights", (char**)keywords, &pyobj_weights) &&
        pyopencv_to(pyobj_weights, weights, ArgInfo("weights", 0)) )
    {
        ERRWRAP2(_self_->setWeights(weights));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWindowRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignatures* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignatures_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignatures*>(((pyopencv_xfeatures2d_PCTSignatures_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignatures' or its derivative)");
    int radius=0;

    const char* keywords[] = { "radius", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_PCTSignatures.setWindowRadius", (char**)keywords, &radius) )
    {
        ERRWRAP2(_self_->setWindowRadius(radius));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_PCTSignatures_methods[] =
{
    {"computeSignature", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignature, 0), "computeSignature(image[, signature]) -> signature\n.   * @brief Computes signature of given image.\n.   * @param image Input image of CV_8U type.\n.   * @param signature Output computed signature."},
    {"computeSignatures", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_computeSignatures, 0), "computeSignatures(images, signatures) -> None\n.   * @brief Computes signatures for multiple images in parallel.\n.   * @param images Vector of input images of CV_8U type.\n.   * @param signatures Vector of computed signatures."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_create_cls, METH_CLASS), "create([, initSampleCount[, initSeedCount[, pointDistribution]]]) -> retval\n.   * @brief Creates PCTSignatures algorithm using sample and seed count.\n.   *       It generates its own sets of sampling points and clusterization seed indexes.\n.   * @param initSampleCount Number of points used for image sampling.\n.   * @param initSeedCount Number of initial clusterization seeds.\n.   *       Must be lower or equal to initSampleCount\n.   * @param pointDistribution Distribution of generated points. Default: UNIFORM.\n.   *       Available: UNIFORM, REGULAR, NORMAL.\n.   * @return Created algorithm.\n\n\n\ncreate(initSamplingPoints, initSeedCount) -> retval\n.   * @brief Creates PCTSignatures algorithm using pre-generated sampling points\n.   *       and number of clusterization seeds. It uses the provided\n.   *       sampling points and generates its own clusterization seed indexes.\n.   * @param initSamplingPoints Sampling points used in image sampling.\n.   * @param initSeedCount Number of initial clusterization seeds.\n.   *       Must be lower or equal to initSamplingPoints.size().\n.   * @return Created algorithm.\n\n\n\ncreate(initSamplingPoints, initClusterSeedIndexes) -> retval\n.   * @brief Creates PCTSignatures algorithm using pre-generated sampling points\n.   *       and clusterization seeds indexes.\n.   * @param initSamplingPoints Sampling points used in image sampling.\n.   * @param initClusterSeedIndexes Indexes of initial clusterization seeds.\n.   *       Its size must be lower or equal to initSamplingPoints.size().\n.   * @return Created algorithm."},
    {"drawSignature", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_drawSignature_cls, METH_CLASS), "drawSignature(source, signature[, result[, radiusToShorterSideRatio[, borderThickness]]]) -> result\n.   * @brief Draws signature in the source image and outputs the result.\n.   *       Signatures are visualized as a circle\n.   *       with radius based on signature weight\n.   *       and color based on signature color.\n.   *       Contrast and entropy are not visualized.\n.   * @param source Source image.\n.   * @param signature Image signature.\n.   * @param result Output result.\n.   * @param radiusToShorterSideRatio Determines maximal radius of signature in the output image.\n.   * @param borderThickness Border thickness of the visualized signature."},
    {"generateInitPoints", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_generateInitPoints_cls, METH_CLASS), "generateInitPoints(initPoints, count, pointDistribution) -> None\n.   * @brief Generates initial sampling points according to selected point distribution.\n.   * @param initPoints Output vector where the generated points will be saved.\n.   * @param count Number of points to generate.\n.   * @param pointDistribution Point distribution selector.\n.   *       Available: UNIFORM, REGULAR, NORMAL.\n.   * @note Generated coordinates are in range [0..1)"},
    {"getClusterMinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getClusterMinSize, 0), "getClusterMinSize() -> retval\n.   * @brief This parameter multiplied by the index of iteration gives lower limit for cluster size.\n.   *       Clusters containing fewer points than specified by the limit have their centroid dismissed\n.   *       and points are reassigned."},
    {"getDistanceFunction", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDistanceFunction, 0), "getDistanceFunction() -> retval\n.   * @brief Distance function selector used for measuring distance between two points in k-means."},
    {"getDropThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getDropThreshold, 0), "getDropThreshold() -> retval\n.   * @brief Remove centroids in k-means whose weight is lesser or equal to given threshold."},
    {"getGrayscaleBits", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getGrayscaleBits, 0), "getGrayscaleBits() -> retval\n.   * @brief Color resolution of the greyscale bitmap represented in allocated bits\n.   *       (i.e., value 4 means that 16 shades of grey are used).\n.   *       The greyscale bitmap is used for computing contrast and entropy values."},
    {"getInitSeedCount", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedCount, 0), "getInitSeedCount() -> retval\n.   * @brief Number of initial seeds (initial number of clusters) for the k-means algorithm."},
    {"getInitSeedIndexes", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getInitSeedIndexes, 0), "getInitSeedIndexes() -> retval\n.   * @brief Initial seeds (initial number of clusters) for the k-means algorithm."},
    {"getIterationCount", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getIterationCount, 0), "getIterationCount() -> retval\n.   * @brief Number of iterations of the k-means clustering.\n.   *       We use fixed number of iterations, since the modified clustering is pruning clusters\n.   *       (not iteratively refining k clusters)."},
    {"getJoiningDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getJoiningDistance, 0), "getJoiningDistance() -> retval\n.   * @brief Threshold euclidean distance between two centroids.\n.   *       If two cluster centers are closer than this distance,\n.   *       one of the centroid is dismissed and points are reassigned."},
    {"getMaxClustersCount", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getMaxClustersCount, 0), "getMaxClustersCount() -> retval\n.   * @brief Maximal number of generated clusters. If the number is exceeded,\n.   *       the clusters are sorted by their weights and the smallest clusters are cropped."},
    {"getSampleCount", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSampleCount, 0), "getSampleCount() -> retval\n.   * @brief Number of initial samples taken from the image."},
    {"getSamplingPoints", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getSamplingPoints, 0), "getSamplingPoints() -> retval\n.   * @brief Initial samples taken from the image.\n.   *       These sampled features become the input for clustering."},
    {"getWeightA", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightA, 0), "getWeightA() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightB", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightB, 0), "getWeightB() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightContrast", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightContrast, 0), "getWeightContrast() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightEntropy", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightEntropy, 0), "getWeightEntropy() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightL", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightL, 0), "getWeightL() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightX", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightX, 0), "getWeightX() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWeightY", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWeightY, 0), "getWeightY() -> retval\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"getWindowRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_getWindowRadius, 0), "getWindowRadius() -> retval\n.   * @brief Size of the texture sampling window used to compute contrast and entropy\n.   *       (center of the window is always in the pixel selected by x,y coordinates\n.   *       of the corresponding feature sample)."},
    {"setClusterMinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setClusterMinSize, 0), "setClusterMinSize(clusterMinSize) -> None\n.   * @brief This parameter multiplied by the index of iteration gives lower limit for cluster size.\n.   *       Clusters containing fewer points than specified by the limit have their centroid dismissed\n.   *       and points are reassigned."},
    {"setDistanceFunction", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDistanceFunction, 0), "setDistanceFunction(distanceFunction) -> None\n.   * @brief Distance function selector used for measuring distance between two points in k-means.\n.   *       Available: L0_25, L0_5, L1, L2, L2SQUARED, L5, L_INFINITY."},
    {"setDropThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setDropThreshold, 0), "setDropThreshold(dropThreshold) -> None\n.   * @brief Remove centroids in k-means whose weight is lesser or equal to given threshold."},
    {"setGrayscaleBits", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setGrayscaleBits, 0), "setGrayscaleBits(grayscaleBits) -> None\n.   * @brief Color resolution of the greyscale bitmap represented in allocated bits\n.   *       (i.e., value 4 means that 16 shades of grey are used).\n.   *       The greyscale bitmap is used for computing contrast and entropy values."},
    {"setInitSeedIndexes", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setInitSeedIndexes, 0), "setInitSeedIndexes(initSeedIndexes) -> None\n.   * @brief Initial seed indexes for the k-means algorithm."},
    {"setIterationCount", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setIterationCount, 0), "setIterationCount(iterationCount) -> None\n.   * @brief Number of iterations of the k-means clustering.\n.   *       We use fixed number of iterations, since the modified clustering is pruning clusters\n.   *       (not iteratively refining k clusters)."},
    {"setJoiningDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setJoiningDistance, 0), "setJoiningDistance(joiningDistance) -> None\n.   * @brief Threshold euclidean distance between two centroids.\n.   *       If two cluster centers are closer than this distance,\n.   *       one of the centroid is dismissed and points are reassigned."},
    {"setMaxClustersCount", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setMaxClustersCount, 0), "setMaxClustersCount(maxClustersCount) -> None\n.   * @brief Maximal number of generated clusters. If the number is exceeded,\n.   *       the clusters are sorted by their weights and the smallest clusters are cropped."},
    {"setSamplingPoints", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setSamplingPoints, 0), "setSamplingPoints(samplingPoints) -> None\n.   * @brief Sets sampling points used to sample the input image.\n.   * @param samplingPoints Vector of sampling points in range [0..1)\n.   * @note Number of sampling points must be greater or equal to clusterization seed count."},
    {"setTranslation", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslation, 0), "setTranslation(idx, value) -> None\n.   * @brief Translations of the individual axes of the feature space.\n.   * @param idx ID of the translation\n.   * @param value Value of the translation\n.   * @note\n.   *       WEIGHT_IDX = 0;\n.   *       X_IDX = 1;\n.   *       Y_IDX = 2;\n.   *       L_IDX = 3;\n.   *       A_IDX = 4;\n.   *       B_IDX = 5;\n.   *       CONTRAST_IDX = 6;\n.   *       ENTROPY_IDX = 7;"},
    {"setTranslations", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setTranslations, 0), "setTranslations(translations) -> None\n.   * @brief Translations of the individual axes of the feature space.\n.   * @param translations Values of all translations.\n.   * @note\n.   *       WEIGHT_IDX = 0;\n.   *       X_IDX = 1;\n.   *       Y_IDX = 2;\n.   *       L_IDX = 3;\n.   *       A_IDX = 4;\n.   *       B_IDX = 5;\n.   *       CONTRAST_IDX = 6;\n.   *       ENTROPY_IDX = 7;"},
    {"setWeight", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeight, 0), "setWeight(idx, value) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space.\n.   * @param idx ID of the weight\n.   * @param value Value of the weight\n.   * @note\n.   *       WEIGHT_IDX = 0;\n.   *       X_IDX = 1;\n.   *       Y_IDX = 2;\n.   *       L_IDX = 3;\n.   *       A_IDX = 4;\n.   *       B_IDX = 5;\n.   *       CONTRAST_IDX = 6;\n.   *       ENTROPY_IDX = 7;"},
    {"setWeightA", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightA, 0), "setWeightA(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightB", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightB, 0), "setWeightB(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightContrast", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightContrast, 0), "setWeightContrast(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightEntropy", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightEntropy, 0), "setWeightEntropy(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightL", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightL, 0), "setWeightL(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightX", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightX, 0), "setWeightX(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeightY", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeightY, 0), "setWeightY(weight) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space\n.   *       (x,y = position; L,a,b = color in CIE Lab space; c = contrast. e = entropy)"},
    {"setWeights", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWeights, 0), "setWeights(weights) -> None\n.   * @brief Weights (multiplicative constants) that linearly stretch individual axes of the feature space.\n.   * @param weights Values of all weights.\n.   * @note\n.   *       WEIGHT_IDX = 0;\n.   *       X_IDX = 1;\n.   *       Y_IDX = 2;\n.   *       L_IDX = 3;\n.   *       A_IDX = 4;\n.   *       B_IDX = 5;\n.   *       CONTRAST_IDX = 6;\n.   *       ENTROPY_IDX = 7;"},
    {"setWindowRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignatures_setWindowRadius, 0), "setWindowRadius(radius) -> None\n.   * @brief Size of the texture sampling window used to compute contrast and entropy\n.   *       (center of the window is always in the pixel selected by x,y coordinates\n.   *       of the corresponding feature sample)."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_PCTSignatures_specials(void)
{
    pyopencv_xfeatures2d_PCTSignatures_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_xfeatures2d_PCTSignatures_Type.tp_dealloc = pyopencv_xfeatures2d_PCTSignatures_dealloc;
    pyopencv_xfeatures2d_PCTSignatures_Type.tp_repr = pyopencv_xfeatures2d_PCTSignatures_repr;
    pyopencv_xfeatures2d_PCTSignatures_Type.tp_getset = pyopencv_xfeatures2d_PCTSignatures_getseters;
    pyopencv_xfeatures2d_PCTSignatures_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_PCTSignatures_Type.tp_methods = pyopencv_xfeatures2d_PCTSignatures_methods;
}

static PyObject* pyopencv_xfeatures2d_PCTSignaturesSQFD_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_PCTSignaturesSQFD %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_PCTSignaturesSQFD_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistance(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignaturesSQFD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignaturesSQFD_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignaturesSQFD*>(((pyopencv_xfeatures2d_PCTSignaturesSQFD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignaturesSQFD' or its derivative)");
    {
    PyObject* pyobj__signature0 = NULL;
    Mat _signature0;
    PyObject* pyobj__signature1 = NULL;
    Mat _signature1;
    float retval;

    const char* keywords[] = { "_signature0", "_signature1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistance", (char**)keywords, &pyobj__signature0, &pyobj__signature1) &&
        pyopencv_to(pyobj__signature0, _signature0, ArgInfo("_signature0", 0)) &&
        pyopencv_to(pyobj__signature1, _signature1, ArgInfo("_signature1", 0)) )
    {
        ERRWRAP2(retval = _self_->computeQuadraticFormDistance(_signature0, _signature1));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__signature0 = NULL;
    UMat _signature0;
    PyObject* pyobj__signature1 = NULL;
    UMat _signature1;
    float retval;

    const char* keywords[] = { "_signature0", "_signature1", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistance", (char**)keywords, &pyobj__signature0, &pyobj__signature1) &&
        pyopencv_to(pyobj__signature0, _signature0, ArgInfo("_signature0", 0)) &&
        pyopencv_to(pyobj__signature1, _signature1, ArgInfo("_signature1", 0)) )
    {
        ERRWRAP2(retval = _self_->computeQuadraticFormDistance(_signature0, _signature1));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistances(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::PCTSignaturesSQFD* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_PCTSignaturesSQFD_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::PCTSignaturesSQFD*>(((pyopencv_xfeatures2d_PCTSignaturesSQFD_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_PCTSignaturesSQFD' or its derivative)");
    {
    PyObject* pyobj_sourceSignature = NULL;
    Mat sourceSignature;
    PyObject* pyobj_imageSignatures = NULL;
    vector_Mat imageSignatures;
    PyObject* pyobj_distances = NULL;
    vector_float distances;

    const char* keywords[] = { "sourceSignature", "imageSignatures", "distances", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistances", (char**)keywords, &pyobj_sourceSignature, &pyobj_imageSignatures, &pyobj_distances) &&
        pyopencv_to(pyobj_sourceSignature, sourceSignature, ArgInfo("sourceSignature", 0)) &&
        pyopencv_to(pyobj_imageSignatures, imageSignatures, ArgInfo("imageSignatures", 0)) &&
        pyopencv_to(pyobj_distances, distances, ArgInfo("distances", 0)) )
    {
        ERRWRAP2(_self_->computeQuadraticFormDistances(sourceSignature, imageSignatures, distances));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_sourceSignature = NULL;
    Mat sourceSignature;
    PyObject* pyobj_imageSignatures = NULL;
    vector_Mat imageSignatures;
    PyObject* pyobj_distances = NULL;
    vector_float distances;

    const char* keywords[] = { "sourceSignature", "imageSignatures", "distances", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:xfeatures2d_PCTSignaturesSQFD.computeQuadraticFormDistances", (char**)keywords, &pyobj_sourceSignature, &pyobj_imageSignatures, &pyobj_distances) &&
        pyopencv_to(pyobj_sourceSignature, sourceSignature, ArgInfo("sourceSignature", 0)) &&
        pyopencv_to(pyobj_imageSignatures, imageSignatures, ArgInfo("imageSignatures", 0)) &&
        pyopencv_to(pyobj_distances, distances, ArgInfo("distances", 0)) )
    {
        ERRWRAP2(_self_->computeQuadraticFormDistances(sourceSignature, imageSignatures, distances));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int distanceFunction=3;
    int similarityFunction=2;
    float similarityParameter=1.0f;
    Ptr<PCTSignaturesSQFD> retval;

    const char* keywords[] = { "distanceFunction", "similarityFunction", "similarityParameter", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iif:xfeatures2d_PCTSignaturesSQFD.create", (char**)keywords, &distanceFunction, &similarityFunction, &similarityParameter) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::PCTSignaturesSQFD::create(distanceFunction, similarityFunction, similarityParameter));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_PCTSignaturesSQFD_methods[] =
{
    {"computeQuadraticFormDistance", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistance, 0), "computeQuadraticFormDistance(_signature0, _signature1) -> retval\n.   * @brief Computes Signature Quadratic Form Distance of two signatures.\n.   * @param _signature0 The first signature.\n.   * @param _signature1 The second signature."},
    {"computeQuadraticFormDistances", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_computeQuadraticFormDistances, 0), "computeQuadraticFormDistances(sourceSignature, imageSignatures, distances) -> None\n.   * @brief Computes Signature Quadratic Form Distance between the reference signature\n.   *       and each of the other image signatures.\n.   * @param sourceSignature The signature to measure distance of other signatures from.\n.   * @param imageSignatures Vector of signatures to measure distance from the source signature.\n.   * @param distances Output vector of measured distances."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_PCTSignaturesSQFD_create_cls, METH_CLASS), "create([, distanceFunction[, similarityFunction[, similarityParameter]]]) -> retval\n.   * @brief Creates the algorithm instance using selected distance function,\n.   *       similarity function and similarity function parameter.\n.   * @param distanceFunction Distance function selector. Default: L2\n.   *       Available: L0_25, L0_5, L1, L2, L2SQUARED, L5, L_INFINITY\n.   * @param similarityFunction Similarity function selector. Default: HEURISTIC\n.   *       Available: MINUS, GAUSSIAN, HEURISTIC\n.   * @param similarityParameter Parameter of the similarity function."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_PCTSignaturesSQFD_specials(void)
{
    pyopencv_xfeatures2d_PCTSignaturesSQFD_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_xfeatures2d_PCTSignaturesSQFD_Type.tp_dealloc = pyopencv_xfeatures2d_PCTSignaturesSQFD_dealloc;
    pyopencv_xfeatures2d_PCTSignaturesSQFD_Type.tp_repr = pyopencv_xfeatures2d_PCTSignaturesSQFD_repr;
    pyopencv_xfeatures2d_PCTSignaturesSQFD_Type.tp_getset = pyopencv_xfeatures2d_PCTSignaturesSQFD_getseters;
    pyopencv_xfeatures2d_PCTSignaturesSQFD_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_PCTSignaturesSQFD_Type.tp_methods = pyopencv_xfeatures2d_PCTSignaturesSQFD_methods;
}

static PyObject* pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_HarrisLaplaceFeatureDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int numOctaves=6;
    float corn_thresh=0.01f;
    float DOG_thresh=0.01f;
    int maxCorners=5000;
    int num_layers=4;
    Ptr<HarrisLaplaceFeatureDetector> retval;

    const char* keywords[] = { "numOctaves", "corn_thresh", "DOG_thresh", "maxCorners", "num_layers", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iffii:xfeatures2d_HarrisLaplaceFeatureDetector.create", (char**)keywords, &numOctaves, &corn_thresh, &DOG_thresh, &maxCorners, &num_layers) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::HarrisLaplaceFeatureDetector::create(numOctaves, corn_thresh, DOG_thresh, maxCorners, num_layers));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_HarrisLaplaceFeatureDetector_create_cls, METH_CLASS), "create([, numOctaves[, corn_thresh[, DOG_thresh[, maxCorners[, num_layers]]]]]) -> retval\n.   * @brief Creates a new implementation instance.\n.   *\n.   * @param numOctaves the number of octaves in the scale-space pyramid\n.   * @param corn_thresh the threshold for the Harris cornerness measure\n.   * @param DOG_thresh the threshold for the Difference-of-Gaussians scale selection\n.   * @param maxCorners the maximum number of corners to consider\n.   * @param num_layers the number of intermediate scales per octave"},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_specials(void)
{
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type.tp_dealloc = pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_dealloc;
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type.tp_repr = pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_repr;
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type.tp_getset = pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_getseters;
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_Type.tp_methods = pyopencv_xfeatures2d_HarrisLaplaceFeatureDetector_methods;
}

static PyObject* pyopencv_xfeatures2d_SIFT_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_SIFT %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_SIFT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SIFT_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    int nfeatures=0;
    int nOctaveLayers=3;
    double contrastThreshold=0.04;
    double edgeThreshold=10;
    double sigma=1.6;
    Ptr<SIFT> retval;

    const char* keywords[] = { "nfeatures", "nOctaveLayers", "contrastThreshold", "edgeThreshold", "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiddd:xfeatures2d_SIFT.create", (char**)keywords, &nfeatures, &nOctaveLayers, &contrastThreshold, &edgeThreshold, &sigma) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::SIFT::create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma));
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_SIFT_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SIFT_create_cls, METH_CLASS), "create([, nfeatures[, nOctaveLayers[, contrastThreshold[, edgeThreshold[, sigma]]]]]) -> retval\n.   @param nfeatures The number of best features to retain. The features are ranked by their scores\n.   (measured in SIFT algorithm as the local contrast)\n.   \n.   @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The\n.   number of octaves is computed automatically from the image resolution.\n.   \n.   @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform\n.   (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\n.   \n.   @param edgeThreshold The threshold used to filter out edge-like features. Note that the its meaning\n.   is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are\n.   filtered out (more features are retained).\n.   \n.   @param sigma The sigma of the Gaussian applied to the input image at the octave \\#0. If your image\n.   is captured with a weak camera with soft lenses, you might want to reduce the number."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_SIFT_specials(void)
{
    pyopencv_xfeatures2d_SIFT_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_SIFT_Type.tp_dealloc = pyopencv_xfeatures2d_SIFT_dealloc;
    pyopencv_xfeatures2d_SIFT_Type.tp_repr = pyopencv_xfeatures2d_SIFT_repr;
    pyopencv_xfeatures2d_SIFT_Type.tp_getset = pyopencv_xfeatures2d_SIFT_getseters;
    pyopencv_xfeatures2d_SIFT_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_SIFT_Type.tp_methods = pyopencv_xfeatures2d_SIFT_methods;
}

static PyObject* pyopencv_xfeatures2d_SURF_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<xfeatures2d_SURF %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_xfeatures2d_SURF_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    double hessianThreshold=100;
    int nOctaves=4;
    int nOctaveLayers=3;
    bool extended=false;
    bool upright=false;
    Ptr<SURF> retval;

    const char* keywords[] = { "hessianThreshold", "nOctaves", "nOctaveLayers", "extended", "upright", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|diibb:xfeatures2d_SURF.create", (char**)keywords, &hessianThreshold, &nOctaves, &nOctaveLayers, &extended, &upright) )
    {
        ERRWRAP2(retval = cv::xfeatures2d::SURF::create(hessianThreshold, nOctaves, nOctaveLayers, extended, upright));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getExtended(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getExtended());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getHessianThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHessianThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaveLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNOctaveLayers());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaves(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNOctaves());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getUpright(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUpright());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setExtended(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    bool extended=0;

    const char* keywords[] = { "extended", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:xfeatures2d_SURF.setExtended", (char**)keywords, &extended) )
    {
        ERRWRAP2(_self_->setExtended(extended));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setHessianThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    double hessianThreshold=0;

    const char* keywords[] = { "hessianThreshold", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:xfeatures2d_SURF.setHessianThreshold", (char**)keywords, &hessianThreshold) )
    {
        ERRWRAP2(_self_->setHessianThreshold(hessianThreshold));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaveLayers(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    int nOctaveLayers=0;

    const char* keywords[] = { "nOctaveLayers", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_SURF.setNOctaveLayers", (char**)keywords, &nOctaveLayers) )
    {
        ERRWRAP2(_self_->setNOctaveLayers(nOctaveLayers));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaves(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    int nOctaves=0;

    const char* keywords[] = { "nOctaves", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:xfeatures2d_SURF.setNOctaves", (char**)keywords, &nOctaves) )
    {
        ERRWRAP2(_self_->setNOctaves(nOctaves));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setUpright(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::xfeatures2d;

    cv::xfeatures2d::SURF* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_xfeatures2d_SURF_Type))
        _self_ = dynamic_cast<cv::xfeatures2d::SURF*>(((pyopencv_xfeatures2d_SURF_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'xfeatures2d_SURF' or its derivative)");
    bool upright=0;

    const char* keywords[] = { "upright", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:xfeatures2d_SURF.setUpright", (char**)keywords, &upright) )
    {
        ERRWRAP2(_self_->setUpright(upright));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_xfeatures2d_SURF_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_create_cls, METH_CLASS), "create([, hessianThreshold[, nOctaves[, nOctaveLayers[, extended[, upright]]]]]) -> retval\n.   @param hessianThreshold Threshold for hessian keypoint detector used in SURF.\n.   @param nOctaves Number of pyramid octaves the keypoint detector will use.\n.   @param nOctaveLayers Number of octave layers within each octave.\n.   @param extended Extended descriptor flag (true - use extended 128-element descriptors; false - use\n.   64-element descriptors).\n.   @param upright Up-right or rotated features flag (true - do not compute orientation of features;\n.   false - compute orientation)."},
    {"getExtended", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getExtended, 0), "getExtended() -> retval\n."},
    {"getHessianThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getHessianThreshold, 0), "getHessianThreshold() -> retval\n."},
    {"getNOctaveLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaveLayers, 0), "getNOctaveLayers() -> retval\n."},
    {"getNOctaves", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getNOctaves, 0), "getNOctaves() -> retval\n."},
    {"getUpright", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_getUpright, 0), "getUpright() -> retval\n."},
    {"setExtended", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setExtended, 0), "setExtended(extended) -> None\n."},
    {"setHessianThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setHessianThreshold, 0), "setHessianThreshold(hessianThreshold) -> None\n."},
    {"setNOctaveLayers", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaveLayers, 0), "setNOctaveLayers(nOctaveLayers) -> None\n."},
    {"setNOctaves", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setNOctaves, 0), "setNOctaves(nOctaves) -> None\n."},
    {"setUpright", CV_PY_FN_WITH_KW_(pyopencv_cv_xfeatures2d_xfeatures2d_SURF_setUpright, 0), "setUpright(upright) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_xfeatures2d_SURF_specials(void)
{
    pyopencv_xfeatures2d_SURF_Type.tp_base = &pyopencv_Feature2D_Type;
    pyopencv_xfeatures2d_SURF_Type.tp_dealloc = pyopencv_xfeatures2d_SURF_dealloc;
    pyopencv_xfeatures2d_SURF_Type.tp_repr = pyopencv_xfeatures2d_SURF_repr;
    pyopencv_xfeatures2d_SURF_Type.tp_getset = pyopencv_xfeatures2d_SURF_getseters;
    pyopencv_xfeatures2d_SURF_Type.tp_init = (initproc)0;
    pyopencv_xfeatures2d_SURF_Type.tp_methods = pyopencv_xfeatures2d_SURF_methods;
}

static PyObject* pyopencv_ximgproc_DisparityFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_DisparityFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_DisparityFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityFilter_filter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityFilter*>(((pyopencv_ximgproc_DisparityFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityFilter' or its derivative)");
    {
    PyObject* pyobj_disparity_map_left = NULL;
    Mat disparity_map_left;
    PyObject* pyobj_left_view = NULL;
    Mat left_view;
    PyObject* pyobj_filtered_disparity_map = NULL;
    Mat filtered_disparity_map;
    PyObject* pyobj_disparity_map_right = NULL;
    Mat disparity_map_right;
    PyObject* pyobj_ROI = NULL;
    Rect ROI;
    PyObject* pyobj_right_view = NULL;
    Mat right_view;

    const char* keywords[] = { "disparity_map_left", "left_view", "filtered_disparity_map", "disparity_map_right", "ROI", "right_view", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OOOO:ximgproc_DisparityFilter.filter", (char**)keywords, &pyobj_disparity_map_left, &pyobj_left_view, &pyobj_filtered_disparity_map, &pyobj_disparity_map_right, &pyobj_ROI, &pyobj_right_view) &&
        pyopencv_to(pyobj_disparity_map_left, disparity_map_left, ArgInfo("disparity_map_left", 0)) &&
        pyopencv_to(pyobj_left_view, left_view, ArgInfo("left_view", 0)) &&
        pyopencv_to(pyobj_filtered_disparity_map, filtered_disparity_map, ArgInfo("filtered_disparity_map", 1)) &&
        pyopencv_to(pyobj_disparity_map_right, disparity_map_right, ArgInfo("disparity_map_right", 0)) &&
        pyopencv_to(pyobj_ROI, ROI, ArgInfo("ROI", 0)) &&
        pyopencv_to(pyobj_right_view, right_view, ArgInfo("right_view", 0)) )
    {
        ERRWRAP2(_self_->filter(disparity_map_left, left_view, filtered_disparity_map, disparity_map_right, ROI, right_view));
        return pyopencv_from(filtered_disparity_map);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_disparity_map_left = NULL;
    UMat disparity_map_left;
    PyObject* pyobj_left_view = NULL;
    UMat left_view;
    PyObject* pyobj_filtered_disparity_map = NULL;
    UMat filtered_disparity_map;
    PyObject* pyobj_disparity_map_right = NULL;
    UMat disparity_map_right;
    PyObject* pyobj_ROI = NULL;
    Rect ROI;
    PyObject* pyobj_right_view = NULL;
    UMat right_view;

    const char* keywords[] = { "disparity_map_left", "left_view", "filtered_disparity_map", "disparity_map_right", "ROI", "right_view", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|OOOO:ximgproc_DisparityFilter.filter", (char**)keywords, &pyobj_disparity_map_left, &pyobj_left_view, &pyobj_filtered_disparity_map, &pyobj_disparity_map_right, &pyobj_ROI, &pyobj_right_view) &&
        pyopencv_to(pyobj_disparity_map_left, disparity_map_left, ArgInfo("disparity_map_left", 0)) &&
        pyopencv_to(pyobj_left_view, left_view, ArgInfo("left_view", 0)) &&
        pyopencv_to(pyobj_filtered_disparity_map, filtered_disparity_map, ArgInfo("filtered_disparity_map", 1)) &&
        pyopencv_to(pyobj_disparity_map_right, disparity_map_right, ArgInfo("disparity_map_right", 0)) &&
        pyopencv_to(pyobj_ROI, ROI, ArgInfo("ROI", 0)) &&
        pyopencv_to(pyobj_right_view, right_view, ArgInfo("right_view", 0)) )
    {
        ERRWRAP2(_self_->filter(disparity_map_left, left_view, filtered_disparity_map, disparity_map_right, ROI, right_view));
        return pyopencv_from(filtered_disparity_map);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_DisparityFilter_methods[] =
{
    {"filter", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityFilter_filter, 0), "filter(disparity_map_left, left_view[, filtered_disparity_map[, disparity_map_right[, ROI[, right_view]]]]) -> filtered_disparity_map\n.   @brief Apply filtering to the disparity map.\n.   \n.   @param disparity_map_left disparity map of the left view, 1 channel, CV_16S type. Implicitly assumes that disparity\n.   values are scaled by 16 (one-pixel disparity corresponds to the value of 16 in the disparity map). Disparity map\n.   can have any resolution, it will be automatically resized to fit left_view resolution.\n.   \n.   @param left_view left view of the original stereo-pair to guide the filtering process, 8-bit single-channel\n.   or three-channel image.\n.   \n.   @param filtered_disparity_map output disparity map.\n.   \n.   @param disparity_map_right optional argument, some implementations might also use the disparity map\n.   of the right view to compute confidence maps, for instance.\n.   \n.   @param ROI region of the disparity map to filter. Optional, usually it should be set automatically.\n.   \n.   @param right_view optional argument, some implementations might also use the right view of the original\n.   stereo-pair."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_DisparityFilter_specials(void)
{
    pyopencv_ximgproc_DisparityFilter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_DisparityFilter_Type.tp_dealloc = pyopencv_ximgproc_DisparityFilter_dealloc;
    pyopencv_ximgproc_DisparityFilter_Type.tp_repr = pyopencv_ximgproc_DisparityFilter_repr;
    pyopencv_ximgproc_DisparityFilter_Type.tp_getset = pyopencv_ximgproc_DisparityFilter_getseters;
    pyopencv_ximgproc_DisparityFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_DisparityFilter_Type.tp_methods = pyopencv_ximgproc_DisparityFilter_methods;
}

static PyObject* pyopencv_ximgproc_DisparityWLSFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_DisparityWLSFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_DisparityWLSFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getConfidenceMap(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Mat retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getConfidenceMap());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getDepthDiscontinuityRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDepthDiscontinuityRadius());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLRCthresh(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLRCthresh());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLambda());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getROI(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    Rect retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getROI());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getSigmaColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigmaColor());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setDepthDiscontinuityRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    int _disc_radius=0;

    const char* keywords[] = { "_disc_radius", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_DisparityWLSFilter.setDepthDiscontinuityRadius", (char**)keywords, &_disc_radius) )
    {
        ERRWRAP2(_self_->setDepthDiscontinuityRadius(_disc_radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLRCthresh(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    int _LRC_thresh=0;

    const char* keywords[] = { "_LRC_thresh", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_DisparityWLSFilter.setLRCthresh", (char**)keywords, &_LRC_thresh) )
    {
        ERRWRAP2(_self_->setLRCthresh(_LRC_thresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    double _lambda=0;

    const char* keywords[] = { "_lambda", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ximgproc_DisparityWLSFilter.setLambda", (char**)keywords, &_lambda) )
    {
        ERRWRAP2(_self_->setLambda(_lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setSigmaColor(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DisparityWLSFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DisparityWLSFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DisparityWLSFilter*>(((pyopencv_ximgproc_DisparityWLSFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DisparityWLSFilter' or its derivative)");
    double _sigma_color=0;

    const char* keywords[] = { "_sigma_color", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ximgproc_DisparityWLSFilter.setSigmaColor", (char**)keywords, &_sigma_color) )
    {
        ERRWRAP2(_self_->setSigmaColor(_sigma_color));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_DisparityWLSFilter_methods[] =
{
    {"getConfidenceMap", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getConfidenceMap, 0), "getConfidenceMap() -> retval\n.   @brief Get the confidence map that was used in the last filter call. It is a CV_32F one-channel image\n.   with values ranging from 0.0 (totally untrusted regions of the raw disparity map) to 255.0 (regions containing\n.   correct disparity values with a high degree of confidence)."},
    {"getDepthDiscontinuityRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getDepthDiscontinuityRadius, 0), "getDepthDiscontinuityRadius() -> retval\n.   @brief DepthDiscontinuityRadius is a parameter used in confidence computation. It defines the size of\n.   low-confidence regions around depth discontinuities."},
    {"getLRCthresh", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLRCthresh, 0), "getLRCthresh() -> retval\n.   @brief LRCthresh is a threshold of disparity difference used in left-right-consistency check during\n.   confidence map computation. The default value of 24 (1.5 pixels) is virtually always good enough."},
    {"getLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getLambda, 0), "getLambda() -> retval\n.   @brief Lambda is a parameter defining the amount of regularization during filtering. Larger values force\n.   filtered disparity map edges to adhere more to source image edges. Typical value is 8000."},
    {"getROI", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getROI, 0), "getROI() -> retval\n.   @brief Get the ROI used in the last filter call"},
    {"getSigmaColor", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_getSigmaColor, 0), "getSigmaColor() -> retval\n.   @brief SigmaColor is a parameter defining how sensitive the filtering process is to source image edges.\n.   Large values can lead to disparity leakage through low-contrast edges. Small values can make the filter too\n.   sensitive to noise and textures in the source image. Typical values range from 0.8 to 2.0."},
    {"setDepthDiscontinuityRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setDepthDiscontinuityRadius, 0), "setDepthDiscontinuityRadius(_disc_radius) -> None\n.   @see getDepthDiscontinuityRadius"},
    {"setLRCthresh", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLRCthresh, 0), "setLRCthresh(_LRC_thresh) -> None\n.   @see getLRCthresh"},
    {"setLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setLambda, 0), "setLambda(_lambda) -> None\n.   @see getLambda"},
    {"setSigmaColor", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DisparityWLSFilter_setSigmaColor, 0), "setSigmaColor(_sigma_color) -> None\n.   @see getSigmaColor"},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_DisparityWLSFilter_specials(void)
{
    pyopencv_ximgproc_DisparityWLSFilter_Type.tp_base = &pyopencv_ximgproc_DisparityFilter_Type;
    pyopencv_ximgproc_DisparityWLSFilter_Type.tp_dealloc = pyopencv_ximgproc_DisparityWLSFilter_dealloc;
    pyopencv_ximgproc_DisparityWLSFilter_Type.tp_repr = pyopencv_ximgproc_DisparityWLSFilter_repr;
    pyopencv_ximgproc_DisparityWLSFilter_Type.tp_getset = pyopencv_ximgproc_DisparityWLSFilter_getseters;
    pyopencv_ximgproc_DisparityWLSFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_DisparityWLSFilter_Type.tp_methods = pyopencv_ximgproc_DisparityWLSFilter_methods;
}

static PyObject* pyopencv_ximgproc_DTFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_DTFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_DTFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_DTFilter_filter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::DTFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_DTFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::DTFilter*>(((pyopencv_ximgproc_DTFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_DTFilter' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ximgproc_DTFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &dDepth) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->filter(src, dst, dDepth));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ximgproc_DTFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &dDepth) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->filter(src, dst, dDepth));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_DTFilter_methods[] =
{
    {"filter", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_DTFilter_filter, 0), "filter(src[, dst[, dDepth]]) -> dst\n.   @brief Produce domain transform filtering operation on source image.\n.   \n.   @param src filtering image with unsigned 8-bit or floating-point 32-bit depth and up to 4 channels.\n.   \n.   @param dst destination image.\n.   \n.   @param dDepth optional depth of the output image. dDepth can be set to -1, which will be equivalent\n.   to src.depth()."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_DTFilter_specials(void)
{
    pyopencv_ximgproc_DTFilter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_DTFilter_Type.tp_dealloc = pyopencv_ximgproc_DTFilter_dealloc;
    pyopencv_ximgproc_DTFilter_Type.tp_repr = pyopencv_ximgproc_DTFilter_repr;
    pyopencv_ximgproc_DTFilter_Type.tp_getset = pyopencv_ximgproc_DTFilter_getseters;
    pyopencv_ximgproc_DTFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_DTFilter_Type.tp_methods = pyopencv_ximgproc_DTFilter_methods;
}

static PyObject* pyopencv_ximgproc_GuidedFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_GuidedFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_GuidedFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_GuidedFilter_filter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::GuidedFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_GuidedFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::GuidedFilter*>(((pyopencv_ximgproc_GuidedFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_GuidedFilter' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ximgproc_GuidedFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &dDepth) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->filter(src, dst, dDepth));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    int dDepth=-1;

    const char* keywords[] = { "src", "dst", "dDepth", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oi:ximgproc_GuidedFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &dDepth) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->filter(src, dst, dDepth));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_GuidedFilter_methods[] =
{
    {"filter", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_GuidedFilter_filter, 0), "filter(src[, dst[, dDepth]]) -> dst\n.   @brief Apply Guided Filter to the filtering image.\n.   \n.   @param src filtering image with any numbers of channels.\n.   \n.   @param dst output image.\n.   \n.   @param dDepth optional depth of the output image. dDepth can be set to -1, which will be equivalent\n.   to src.depth()."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_GuidedFilter_specials(void)
{
    pyopencv_ximgproc_GuidedFilter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_GuidedFilter_Type.tp_dealloc = pyopencv_ximgproc_GuidedFilter_dealloc;
    pyopencv_ximgproc_GuidedFilter_Type.tp_repr = pyopencv_ximgproc_GuidedFilter_repr;
    pyopencv_ximgproc_GuidedFilter_Type.tp_getset = pyopencv_ximgproc_GuidedFilter_getseters;
    pyopencv_ximgproc_GuidedFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_GuidedFilter_Type.tp_methods = pyopencv_ximgproc_GuidedFilter_methods;
}

static PyObject* pyopencv_ximgproc_AdaptiveManifoldFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_AdaptiveManifoldFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_AdaptiveManifoldFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_collectGarbage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::AdaptiveManifoldFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_AdaptiveManifoldFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::AdaptiveManifoldFilter*>(((pyopencv_ximgproc_AdaptiveManifoldFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_AdaptiveManifoldFilter' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->collectGarbage());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    Ptr<AdaptiveManifoldFilter> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::ximgproc::AdaptiveManifoldFilter::create());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_filter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::AdaptiveManifoldFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_AdaptiveManifoldFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::AdaptiveManifoldFilter*>(((pyopencv_ximgproc_AdaptiveManifoldFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_AdaptiveManifoldFilter' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_joint = NULL;
    Mat joint;

    const char* keywords[] = { "src", "dst", "joint", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OO:ximgproc_AdaptiveManifoldFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_joint) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_joint, joint, ArgInfo("joint", 0)) )
    {
        ERRWRAP2(_self_->filter(src, dst, joint));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_joint = NULL;
    UMat joint;

    const char* keywords[] = { "src", "dst", "joint", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|OO:ximgproc_AdaptiveManifoldFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_joint) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) &&
        pyopencv_to(pyobj_joint, joint, ArgInfo("joint", 0)) )
    {
        ERRWRAP2(_self_->filter(src, dst, joint));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_AdaptiveManifoldFilter_methods[] =
{
    {"collectGarbage", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_collectGarbage, 0), "collectGarbage() -> None\n."},
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_create_cls, METH_CLASS), "create() -> retval\n."},
    {"filter", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_AdaptiveManifoldFilter_filter, 0), "filter(src[, dst[, joint]]) -> dst\n.   @brief Apply high-dimensional filtering using adaptive manifolds.\n.   \n.   @param src filtering image with any numbers of channels.\n.   \n.   @param dst output image.\n.   \n.   @param joint optional joint (also called as guided) image with any numbers of channels."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_AdaptiveManifoldFilter_specials(void)
{
    pyopencv_ximgproc_AdaptiveManifoldFilter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_AdaptiveManifoldFilter_Type.tp_dealloc = pyopencv_ximgproc_AdaptiveManifoldFilter_dealloc;
    pyopencv_ximgproc_AdaptiveManifoldFilter_Type.tp_repr = pyopencv_ximgproc_AdaptiveManifoldFilter_repr;
    pyopencv_ximgproc_AdaptiveManifoldFilter_Type.tp_getset = pyopencv_ximgproc_AdaptiveManifoldFilter_getseters;
    pyopencv_ximgproc_AdaptiveManifoldFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_AdaptiveManifoldFilter_Type.tp_methods = pyopencv_ximgproc_AdaptiveManifoldFilter_methods;
}

static PyObject* pyopencv_ximgproc_FastGlobalSmootherFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_FastGlobalSmootherFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_FastGlobalSmootherFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_FastGlobalSmootherFilter_filter(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::FastGlobalSmootherFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_FastGlobalSmootherFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::FastGlobalSmootherFilter*>(((pyopencv_ximgproc_FastGlobalSmootherFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastGlobalSmootherFilter' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_FastGlobalSmootherFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->filter(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_FastGlobalSmootherFilter.filter", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->filter(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_FastGlobalSmootherFilter_methods[] =
{
    {"filter", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastGlobalSmootherFilter_filter, 0), "filter(src[, dst]) -> dst\n.   @brief Apply smoothing operation to the source image.\n.   \n.   @param src source image for filtering with unsigned 8-bit or signed 16-bit or floating-point 32-bit depth and up to 4 channels.\n.   \n.   @param dst destination image."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_FastGlobalSmootherFilter_specials(void)
{
    pyopencv_ximgproc_FastGlobalSmootherFilter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_FastGlobalSmootherFilter_Type.tp_dealloc = pyopencv_ximgproc_FastGlobalSmootherFilter_dealloc;
    pyopencv_ximgproc_FastGlobalSmootherFilter_Type.tp_repr = pyopencv_ximgproc_FastGlobalSmootherFilter_repr;
    pyopencv_ximgproc_FastGlobalSmootherFilter_Type.tp_getset = pyopencv_ximgproc_FastGlobalSmootherFilter_getseters;
    pyopencv_ximgproc_FastGlobalSmootherFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_FastGlobalSmootherFilter_Type.tp_methods = pyopencv_ximgproc_FastGlobalSmootherFilter_methods;
}

static PyObject* pyopencv_ximgproc_EdgeBoxes_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_EdgeBoxes %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_EdgeBoxes_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAlpha());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBeta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBeta());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBoundingBoxes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    {
    PyObject* pyobj_edge_map = NULL;
    Mat edge_map;
    PyObject* pyobj_orientation_map = NULL;
    Mat orientation_map;
    vector_Rect boxes;

    const char* keywords[] = { "edge_map", "orientation_map", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ximgproc_EdgeBoxes.getBoundingBoxes", (char**)keywords, &pyobj_edge_map, &pyobj_orientation_map) &&
        pyopencv_to(pyobj_edge_map, edge_map, ArgInfo("edge_map", 0)) &&
        pyopencv_to(pyobj_orientation_map, orientation_map, ArgInfo("orientation_map", 0)) )
    {
        ERRWRAP2(_self_->getBoundingBoxes(edge_map, orientation_map, boxes));
        return pyopencv_from(boxes);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_edge_map = NULL;
    UMat edge_map;
    PyObject* pyobj_orientation_map = NULL;
    UMat orientation_map;
    vector_Rect boxes;

    const char* keywords[] = { "edge_map", "orientation_map", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO:ximgproc_EdgeBoxes.getBoundingBoxes", (char**)keywords, &pyobj_edge_map, &pyobj_orientation_map) &&
        pyopencv_to(pyobj_edge_map, edge_map, ArgInfo("edge_map", 0)) &&
        pyopencv_to(pyobj_orientation_map, orientation_map, ArgInfo("orientation_map", 0)) )
    {
        ERRWRAP2(_self_->getBoundingBoxes(edge_map, orientation_map, boxes));
        return pyopencv_from(boxes);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getClusterMinMag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getClusterMinMag());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMergeThr(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEdgeMergeThr());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMinMag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEdgeMinMag());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getEta());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGamma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getKappa(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getKappa());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxAspectRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxAspectRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxBoxes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxBoxes());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinBoxArea(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinBoxArea());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinScore(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinScore());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setAlpha", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setAlpha(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setBeta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setBeta", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setBeta(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setClusterMinMag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setClusterMinMag", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setClusterMinMag(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMergeThr(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setEdgeMergeThr", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setEdgeMergeThr(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMinMag(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setEdgeMinMag", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setEdgeMinMag(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setEta", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setEta(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setGamma", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setGamma(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setKappa(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setKappa", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setKappa(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxAspectRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setMaxAspectRatio", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setMaxAspectRatio(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxBoxes(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    int value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_EdgeBoxes.setMaxBoxes", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setMaxBoxes(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinBoxArea(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setMinBoxArea", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setMinBoxArea(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinScore(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeBoxes* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeBoxes_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeBoxes*>(((pyopencv_ximgproc_EdgeBoxes_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeBoxes' or its derivative)");
    float value=0.f;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeBoxes.setMinScore", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setMinScore(value));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_EdgeBoxes_methods[] =
{
    {"getAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getAlpha, 0), "getAlpha() -> retval\n.   @brief Returns the step size of sliding window search."},
    {"getBeta", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBeta, 0), "getBeta() -> retval\n.   @brief Returns the nms threshold for object proposals."},
    {"getBoundingBoxes", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getBoundingBoxes, 0), "getBoundingBoxes(edge_map, orientation_map) -> boxes\n.   @brief Returns array containing proposal boxes.\n.   \n.   @param edge_map edge image.\n.   @param orientation_map orientation map.\n.   @param boxes proposal boxes."},
    {"getClusterMinMag", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getClusterMinMag, 0), "getClusterMinMag() -> retval\n.   @brief Returns the cluster min magnitude."},
    {"getEdgeMergeThr", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMergeThr, 0), "getEdgeMergeThr() -> retval\n.   @brief Returns the edge merge threshold."},
    {"getEdgeMinMag", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEdgeMinMag, 0), "getEdgeMinMag() -> retval\n.   @brief Returns the edge min magnitude."},
    {"getEta", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getEta, 0), "getEta() -> retval\n.   @brief Returns adaptation rate for nms threshold."},
    {"getGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getGamma, 0), "getGamma() -> retval\n.   @brief Returns the affinity sensitivity."},
    {"getKappa", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getKappa, 0), "getKappa() -> retval\n.   @brief Returns the scale sensitivity."},
    {"getMaxAspectRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxAspectRatio, 0), "getMaxAspectRatio() -> retval\n.   @brief Returns the max aspect ratio of boxes."},
    {"getMaxBoxes", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMaxBoxes, 0), "getMaxBoxes() -> retval\n.   @brief Returns the max number of boxes to detect."},
    {"getMinBoxArea", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinBoxArea, 0), "getMinBoxArea() -> retval\n.   @brief Returns the minimum area of boxes."},
    {"getMinScore", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_getMinScore, 0), "getMinScore() -> retval\n.   @brief Returns the min score of boxes to detect."},
    {"setAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setAlpha, 0), "setAlpha(value) -> None\n.   @brief Sets the step size of sliding window search."},
    {"setBeta", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setBeta, 0), "setBeta(value) -> None\n.   @brief Sets the nms threshold for object proposals."},
    {"setClusterMinMag", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setClusterMinMag, 0), "setClusterMinMag(value) -> None\n.   @brief Sets the cluster min magnitude."},
    {"setEdgeMergeThr", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMergeThr, 0), "setEdgeMergeThr(value) -> None\n.   @brief Sets the edge merge threshold."},
    {"setEdgeMinMag", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEdgeMinMag, 0), "setEdgeMinMag(value) -> None\n.   @brief Sets the edge min magnitude."},
    {"setEta", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setEta, 0), "setEta(value) -> None\n.   @brief Sets the adaptation rate for nms threshold."},
    {"setGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setGamma, 0), "setGamma(value) -> None\n.   @brief Sets the affinity sensitivity"},
    {"setKappa", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setKappa, 0), "setKappa(value) -> None\n.   @brief Sets the scale sensitivity."},
    {"setMaxAspectRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxAspectRatio, 0), "setMaxAspectRatio(value) -> None\n.   @brief Sets the max aspect ratio of boxes."},
    {"setMaxBoxes", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMaxBoxes, 0), "setMaxBoxes(value) -> None\n.   @brief Sets max number of boxes to detect."},
    {"setMinBoxArea", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinBoxArea, 0), "setMinBoxArea(value) -> None\n.   @brief Sets the minimum area of boxes."},
    {"setMinScore", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeBoxes_setMinScore, 0), "setMinScore(value) -> None\n.   @brief Sets the min score of boxes to detect."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_EdgeBoxes_specials(void)
{
    pyopencv_ximgproc_EdgeBoxes_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_EdgeBoxes_Type.tp_dealloc = pyopencv_ximgproc_EdgeBoxes_dealloc;
    pyopencv_ximgproc_EdgeBoxes_Type.tp_repr = pyopencv_ximgproc_EdgeBoxes_repr;
    pyopencv_ximgproc_EdgeBoxes_Type.tp_getset = pyopencv_ximgproc_EdgeBoxes_getseters;
    pyopencv_ximgproc_EdgeBoxes_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_EdgeBoxes_Type.tp_methods = pyopencv_ximgproc_EdgeBoxes_methods;
}

static PyObject* pyopencv_ximgproc_FastLineDetector_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_FastLineDetector %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_FastLineDetector_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_FastLineDetector_detect(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::FastLineDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_FastLineDetector_Type))
        _self_ = dynamic_cast<cv::ximgproc::FastLineDetector*>(((pyopencv_ximgproc_FastLineDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastLineDetector' or its derivative)");
    {
    PyObject* pyobj__image = NULL;
    Mat _image;
    PyObject* pyobj__lines = NULL;
    Mat _lines;

    const char* keywords[] = { "_image", "_lines", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_FastLineDetector.detect", (char**)keywords, &pyobj__image, &pyobj__lines) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 0)) &&
        pyopencv_to(pyobj__lines, _lines, ArgInfo("_lines", 1)) )
    {
        ERRWRAP2(_self_->detect(_image, _lines));
        return pyopencv_from(_lines);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__image = NULL;
    UMat _image;
    PyObject* pyobj__lines = NULL;
    UMat _lines;

    const char* keywords[] = { "_image", "_lines", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_FastLineDetector.detect", (char**)keywords, &pyobj__image, &pyobj__lines) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 0)) &&
        pyopencv_to(pyobj__lines, _lines, ArgInfo("_lines", 1)) )
    {
        ERRWRAP2(_self_->detect(_image, _lines));
        return pyopencv_from(_lines);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_FastLineDetector_drawSegments(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::FastLineDetector* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_FastLineDetector_Type))
        _self_ = dynamic_cast<cv::ximgproc::FastLineDetector*>(((pyopencv_ximgproc_FastLineDetector_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_FastLineDetector' or its derivative)");
    {
    PyObject* pyobj__image = NULL;
    Mat _image;
    PyObject* pyobj_lines = NULL;
    Mat lines;
    bool draw_arrow=false;

    const char* keywords[] = { "_image", "lines", "draw_arrow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|b:ximgproc_FastLineDetector.drawSegments", (char**)keywords, &pyobj__image, &pyobj_lines, &draw_arrow) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 1)) &&
        pyopencv_to(pyobj_lines, lines, ArgInfo("lines", 0)) )
    {
        ERRWRAP2(_self_->drawSegments(_image, lines, draw_arrow));
        return pyopencv_from(_image);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__image = NULL;
    UMat _image;
    PyObject* pyobj_lines = NULL;
    UMat lines;
    bool draw_arrow=false;

    const char* keywords[] = { "_image", "lines", "draw_arrow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|b:ximgproc_FastLineDetector.drawSegments", (char**)keywords, &pyobj__image, &pyobj_lines, &draw_arrow) &&
        pyopencv_to(pyobj__image, _image, ArgInfo("_image", 1)) &&
        pyopencv_to(pyobj_lines, lines, ArgInfo("lines", 0)) )
    {
        ERRWRAP2(_self_->drawSegments(_image, lines, draw_arrow));
        return pyopencv_from(_image);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_FastLineDetector_methods[] =
{
    {"detect", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastLineDetector_detect, 0), "detect(_image[, _lines]) -> _lines\n.   @brief Finds lines in the input image.\n.   This is the output of the default parameters of the algorithm on the above\n.   shown image.\n.   \n.   ![image](pics/corridor_fld.jpg)\n.   \n.   @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be\n.   selected, use: `fld_ptr-\\>detect(image(roi), lines, ...);\n.   lines += Scalar(roi.x, roi.y, roi.x, roi.y);`\n.   @param _lines A vector of Vec4f elements specifying the beginning\n.   and ending point of a line.  Where Vec4f is (x1, y1, x2, y2), point\n.   1 is the start, point 2 - end. Returned lines are directed so that the\n.   brighter side is on their left."},
    {"drawSegments", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_FastLineDetector_drawSegments, 0), "drawSegments(_image, lines[, draw_arrow]) -> _image\n.   @brief Draws the line segments on a given image.\n.   @param _image The image, where the lines will be drawn. Should be bigger\n.   or equal to the image, where the lines were found.\n.   @param lines A vector of the lines that needed to be drawn.\n.   @param draw_arrow If true, arrow heads will be drawn."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_FastLineDetector_specials(void)
{
    pyopencv_ximgproc_FastLineDetector_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_FastLineDetector_Type.tp_dealloc = pyopencv_ximgproc_FastLineDetector_dealloc;
    pyopencv_ximgproc_FastLineDetector_Type.tp_repr = pyopencv_ximgproc_FastLineDetector_repr;
    pyopencv_ximgproc_FastLineDetector_Type.tp_getset = pyopencv_ximgproc_FastLineDetector_getseters;
    pyopencv_ximgproc_FastLineDetector_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_FastLineDetector_Type.tp_methods = pyopencv_ximgproc_FastLineDetector_methods;
}

static PyObject* pyopencv_ximgproc_ContourFitting_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_ContourFitting %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_ContourFitting_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_ContourFitting_estimateTransformation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::ContourFitting* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_ContourFitting_Type))
        _self_ = dynamic_cast<cv::ximgproc::ContourFitting*>(((pyopencv_ximgproc_ContourFitting_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;
    PyObject* pyobj_alphaPhiST = NULL;
    Mat alphaPhiST;
    double dist;
    bool fdContour=false;

    const char* keywords[] = { "src", "dst", "alphaPhiST", "fdContour", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Ob:ximgproc_ContourFitting.estimateTransformation", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_alphaPhiST, &fdContour) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) &&
        pyopencv_to(pyobj_alphaPhiST, alphaPhiST, ArgInfo("alphaPhiST", 1)) )
    {
        ERRWRAP2(_self_->estimateTransformation(src, dst, alphaPhiST, dist, fdContour));
        return Py_BuildValue("(NN)", pyopencv_from(alphaPhiST), pyopencv_from(dist));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;
    PyObject* pyobj_alphaPhiST = NULL;
    UMat alphaPhiST;
    double dist;
    bool fdContour=false;

    const char* keywords[] = { "src", "dst", "alphaPhiST", "fdContour", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Ob:ximgproc_ContourFitting.estimateTransformation", (char**)keywords, &pyobj_src, &pyobj_dst, &pyobj_alphaPhiST, &fdContour) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 0)) &&
        pyopencv_to(pyobj_alphaPhiST, alphaPhiST, ArgInfo("alphaPhiST", 1)) )
    {
        ERRWRAP2(_self_->estimateTransformation(src, dst, alphaPhiST, dist, fdContour));
        return Py_BuildValue("(NN)", pyopencv_from(alphaPhiST), pyopencv_from(dist));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_ContourFitting_getCtrSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::ContourFitting* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_ContourFitting_Type))
        _self_ = dynamic_cast<cv::ximgproc::ContourFitting*>(((pyopencv_ximgproc_ContourFitting_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getCtrSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_ContourFitting_getFDSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::ContourFitting* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_ContourFitting_Type))
        _self_ = dynamic_cast<cv::ximgproc::ContourFitting*>(((pyopencv_ximgproc_ContourFitting_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFDSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_ContourFitting_setCtrSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::ContourFitting* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_ContourFitting_Type))
        _self_ = dynamic_cast<cv::ximgproc::ContourFitting*>(((pyopencv_ximgproc_ContourFitting_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    int n=0;

    const char* keywords[] = { "n", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_ContourFitting.setCtrSize", (char**)keywords, &n) )
    {
        ERRWRAP2(_self_->setCtrSize(n));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_ContourFitting_setFDSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::ContourFitting* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_ContourFitting_Type))
        _self_ = dynamic_cast<cv::ximgproc::ContourFitting*>(((pyopencv_ximgproc_ContourFitting_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_ContourFitting' or its derivative)");
    int n=0;

    const char* keywords[] = { "n", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_ContourFitting.setFDSize", (char**)keywords, &n) )
    {
        ERRWRAP2(_self_->setFDSize(n));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_ContourFitting_methods[] =
{
    {"estimateTransformation", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_estimateTransformation, 0), "estimateTransformation(src, dst[, alphaPhiST[, fdContour]]) -> alphaPhiST, dist\n.   @brief Fit two closed curves using fourier descriptors. More details in @cite PersoonFu1977 and @cite BergerRaghunathan1998\n.   \n.   @param src Contour defining first shape.\n.   @param dst Contour defining second shape (Target).\n.   @param alphaPhiST : \\f$ \\alpha \\f$=alphaPhiST(0,0), \\f$ \\phi \\f$=alphaPhiST(0,1) (in radian), s=alphaPhiST(0,2), Tx=alphaPhiST(0,3), Ty=alphaPhiST(0,4) rotation center\n.   @param dist distance between src and dst after matching.\n.   @param fdContour false then src and dst are contours and true src and dst are fourier descriptors."},
    {"getCtrSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_getCtrSize, 0), "getCtrSize() -> retval\n.   @returns number of fourier descriptors"},
    {"getFDSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_getFDSize, 0), "getFDSize() -> retval\n.   @returns number of fourier descriptors used for optimal curve matching"},
    {"setCtrSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_setCtrSize, 0), "setCtrSize(n) -> None\n.   @brief set number of Fourier descriptors used in estimateTransformation\n.   \n.   @param n number of Fourier descriptors equal to number of contour points after resampling."},
    {"setFDSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_ContourFitting_setFDSize, 0), "setFDSize(n) -> None\n.   @brief set number of Fourier descriptors when estimateTransformation used vector<Point>\n.   \n.   @param n number of fourier descriptors used for optimal curve matching."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_ContourFitting_specials(void)
{
    pyopencv_ximgproc_ContourFitting_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_ContourFitting_Type.tp_dealloc = pyopencv_ximgproc_ContourFitting_dealloc;
    pyopencv_ximgproc_ContourFitting_Type.tp_repr = pyopencv_ximgproc_ContourFitting_repr;
    pyopencv_ximgproc_ContourFitting_Type.tp_getset = pyopencv_ximgproc_ContourFitting_getseters;
    pyopencv_ximgproc_ContourFitting_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_ContourFitting_Type.tp_methods = pyopencv_ximgproc_ContourFitting_methods;
}

static PyObject* pyopencv_ximgproc_SuperpixelLSC_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_SuperpixelLSC %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_SuperpixelLSC_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_enforceLabelConnectivity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelLSC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelLSC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelLSC*>(((pyopencv_ximgproc_SuperpixelLSC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    int min_element_size=20;

    const char* keywords[] = { "min_element_size", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:ximgproc_SuperpixelLSC.enforceLabelConnectivity", (char**)keywords, &min_element_size) )
    {
        ERRWRAP2(_self_->enforceLabelConnectivity(min_element_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabelContourMask(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelLSC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelLSC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelLSC*>(((pyopencv_ximgproc_SuperpixelLSC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:ximgproc_SuperpixelLSC.getLabelContourMask", (char**)keywords, &pyobj_image, &thick_line) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(_self_->getLabelContourMask(image, thick_line));
        return pyopencv_from(image);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:ximgproc_SuperpixelLSC.getLabelContourMask", (char**)keywords, &pyobj_image, &thick_line) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(_self_->getLabelContourMask(image, thick_line));
        return pyopencv_from(image);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelLSC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelLSC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelLSC*>(((pyopencv_ximgproc_SuperpixelLSC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    {
    PyObject* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ximgproc_SuperpixelLSC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        pyopencv_to(pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)) )
    {
        ERRWRAP2(_self_->getLabels(labels_out));
        return pyopencv_from(labels_out);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ximgproc_SuperpixelLSC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        pyopencv_to(pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)) )
    {
        ERRWRAP2(_self_->getLabels(labels_out));
        return pyopencv_from(labels_out);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getNumberOfSuperpixels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelLSC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelLSC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelLSC*>(((pyopencv_ximgproc_SuperpixelLSC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumberOfSuperpixels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_iterate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelLSC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelLSC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelLSC*>(((pyopencv_ximgproc_SuperpixelLSC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelLSC' or its derivative)");
    int num_iterations=10;

    const char* keywords[] = { "num_iterations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:ximgproc_SuperpixelLSC.iterate", (char**)keywords, &num_iterations) )
    {
        ERRWRAP2(_self_->iterate(num_iterations));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_SuperpixelLSC_methods[] =
{
    {"enforceLabelConnectivity", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_enforceLabelConnectivity, 0), "enforceLabelConnectivity([, min_element_size]) -> None\n.   @brief Enforce label connectivity.\n.   \n.   @param min_element_size The minimum element size in percents that should be absorbed into a bigger\n.   superpixel. Given resulted average superpixel size valid value should be in 0-100 range, 25 means\n.   that less then a quarter sized superpixel should be absorbed, this is default.\n.   \n.   The function merge component that is too small, assigning the previously found adjacent label\n.   to this component. Calling this function may change the final number of superpixels."},
    {"getLabelContourMask", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in SuperpixelLSC object.\n.   \n.   @param image Return: CV_8U1 image mask where -1 indicates that the pixel is a superpixel border,\n.   and 0 otherwise.\n.   \n.   @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border\n.   are masked.\n.   \n.   The function return the boundaries of the superpixel segmentation."},
    {"getLabels", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.   Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.   @param labels_out Return: A CV_32SC1 integer array containing the labels of the superpixel\n.   segmentation. The labels are in the range [0, getNumberOfSuperpixels()].\n.   \n.   The function returns an image with the labels of the superpixel segmentation. The labels are in\n.   the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Calculates the actual amount of superpixels on a given segmentation computed\n.   and stored in SuperpixelLSC object."},
    {"iterate", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelLSC_iterate, 0), "iterate([, num_iterations]) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.   parameters in the SuperpixelLSC object.\n.   \n.   This function can be called again without the need of initializing the algorithm with\n.   createSuperpixelLSC(). This save the computational cost of allocating memory for all the\n.   structures of the algorithm.\n.   \n.   @param num_iterations Number of iterations. Higher number improves the result.\n.   \n.   The function computes the superpixels segmentation of an image with the parameters initialized\n.   with the function createSuperpixelLSC(). The algorithms starts from a grid of superpixels and\n.   then refines the boundaries by proposing updates of edges boundaries."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_SuperpixelLSC_specials(void)
{
    pyopencv_ximgproc_SuperpixelLSC_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_SuperpixelLSC_Type.tp_dealloc = pyopencv_ximgproc_SuperpixelLSC_dealloc;
    pyopencv_ximgproc_SuperpixelLSC_Type.tp_repr = pyopencv_ximgproc_SuperpixelLSC_repr;
    pyopencv_ximgproc_SuperpixelLSC_Type.tp_getset = pyopencv_ximgproc_SuperpixelLSC_getseters;
    pyopencv_ximgproc_SuperpixelLSC_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_SuperpixelLSC_Type.tp_methods = pyopencv_ximgproc_SuperpixelLSC_methods;
}

static PyObject* pyopencv_ximgproc_RidgeDetectionFilter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_RidgeDetectionFilter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_RidgeDetectionFilter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    int ddepth=CV_32FC1;
    int dx=1;
    int dy=1;
    int ksize=3;
    int out_dtype=CV_8UC1;
    double scale=1;
    double delta=0;
    int borderType=BORDER_DEFAULT;
    Ptr<RidgeDetectionFilter> retval;

    const char* keywords[] = { "ddepth", "dx", "dy", "ksize", "out_dtype", "scale", "delta", "borderType", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iiiiiddi:ximgproc_RidgeDetectionFilter.create", (char**)keywords, &ddepth, &dx, &dy, &ksize, &out_dtype, &scale, &delta, &borderType) )
    {
        ERRWRAP2(retval = cv::ximgproc::RidgeDetectionFilter::create(ddepth, dx, dy, ksize, out_dtype, scale, delta, borderType));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_getRidgeFilteredImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::RidgeDetectionFilter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_RidgeDetectionFilter_Type))
        _self_ = dynamic_cast<cv::ximgproc::RidgeDetectionFilter*>(((pyopencv_ximgproc_RidgeDetectionFilter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_RidgeDetectionFilter' or its derivative)");
    {
    PyObject* pyobj__img = NULL;
    Mat _img;
    PyObject* pyobj_out = NULL;
    Mat out;

    const char* keywords[] = { "_img", "out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_RidgeDetectionFilter.getRidgeFilteredImage", (char**)keywords, &pyobj__img, &pyobj_out) &&
        pyopencv_to(pyobj__img, _img, ArgInfo("_img", 0)) &&
        pyopencv_to(pyobj_out, out, ArgInfo("out", 1)) )
    {
        ERRWRAP2(_self_->getRidgeFilteredImage(_img, out));
        return pyopencv_from(out);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__img = NULL;
    UMat _img;
    PyObject* pyobj_out = NULL;
    UMat out;

    const char* keywords[] = { "_img", "out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_RidgeDetectionFilter.getRidgeFilteredImage", (char**)keywords, &pyobj__img, &pyobj_out) &&
        pyopencv_to(pyobj__img, _img, ArgInfo("_img", 0)) &&
        pyopencv_to(pyobj_out, out, ArgInfo("out", 1)) )
    {
        ERRWRAP2(_self_->getRidgeFilteredImage(_img, out));
        return pyopencv_from(out);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_RidgeDetectionFilter_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_create_cls, METH_CLASS), "create([, ddepth[, dx[, dy[, ksize[, out_dtype[, scale[, delta[, borderType]]]]]]]]) -> retval\n.   @brief Create pointer to the Ridge detection filter.\n.   @param ddepth  Specifies output image depth. Defualt is CV_32FC1\n.   @param dx Order of derivative x, default is 1\n.   @param dy  Order of derivative y, default is 1\n.   @param ksize Sobel kernel size , default is 3\n.   @param out_dtype Converted format for output, default is CV_8UC1\n.   @param scale Optional scale value for derivative values, default is 1\n.   @param delta  Optional bias added to output, default is 0\n.   @param borderType Pixel extrapolation method, default is BORDER_DEFAULT\n.   @see Sobel, threshold, getStructuringElement, morphologyEx.( for additional refinement)"},
    {"getRidgeFilteredImage", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RidgeDetectionFilter_getRidgeFilteredImage, 0), "getRidgeFilteredImage(_img[, out]) -> out\n.   @brief Apply Ridge detection filter on input image.\n.   @param _img InputArray as supported by Sobel. img can be 1-Channel or 3-Channels.\n.   @param out OutputAray of structure as RidgeDetectionFilter::ddepth. Output image with ridges."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_RidgeDetectionFilter_specials(void)
{
    pyopencv_ximgproc_RidgeDetectionFilter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_RidgeDetectionFilter_Type.tp_dealloc = pyopencv_ximgproc_RidgeDetectionFilter_dealloc;
    pyopencv_ximgproc_RidgeDetectionFilter_Type.tp_repr = pyopencv_ximgproc_RidgeDetectionFilter_repr;
    pyopencv_ximgproc_RidgeDetectionFilter_Type.tp_getset = pyopencv_ximgproc_RidgeDetectionFilter_getseters;
    pyopencv_ximgproc_RidgeDetectionFilter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_RidgeDetectionFilter_Type.tp_methods = pyopencv_ximgproc_RidgeDetectionFilter_methods;
}

static PyObject* pyopencv_ximgproc_SuperpixelSEEDS_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_SuperpixelSEEDS %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_SuperpixelSEEDS_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabelContourMask(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSEEDS* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSEEDS_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSEEDS*>(((pyopencv_ximgproc_SuperpixelSEEDS_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    bool thick_line=false;

    const char* keywords[] = { "image", "thick_line", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:ximgproc_SuperpixelSEEDS.getLabelContourMask", (char**)keywords, &pyobj_image, &thick_line) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(_self_->getLabelContourMask(image, thick_line));
        return pyopencv_from(image);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    bool thick_line=false;

    const char* keywords[] = { "image", "thick_line", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:ximgproc_SuperpixelSEEDS.getLabelContourMask", (char**)keywords, &pyobj_image, &thick_line) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(_self_->getLabelContourMask(image, thick_line));
        return pyopencv_from(image);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSEEDS* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSEEDS_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSEEDS*>(((pyopencv_ximgproc_SuperpixelSEEDS_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    {
    PyObject* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ximgproc_SuperpixelSEEDS.getLabels", (char**)keywords, &pyobj_labels_out) &&
        pyopencv_to(pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)) )
    {
        ERRWRAP2(_self_->getLabels(labels_out));
        return pyopencv_from(labels_out);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ximgproc_SuperpixelSEEDS.getLabels", (char**)keywords, &pyobj_labels_out) &&
        pyopencv_to(pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)) )
    {
        ERRWRAP2(_self_->getLabels(labels_out));
        return pyopencv_from(labels_out);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getNumberOfSuperpixels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSEEDS* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSEEDS_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSEEDS*>(((pyopencv_ximgproc_SuperpixelSEEDS_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumberOfSuperpixels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_iterate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSEEDS* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSEEDS_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSEEDS*>(((pyopencv_ximgproc_SuperpixelSEEDS_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSEEDS' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    int num_iterations=4;

    const char* keywords[] = { "img", "num_iterations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|i:ximgproc_SuperpixelSEEDS.iterate", (char**)keywords, &pyobj_img, &num_iterations) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) )
    {
        ERRWRAP2(_self_->iterate(img, num_iterations));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;
    int num_iterations=4;

    const char* keywords[] = { "img", "num_iterations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|i:ximgproc_SuperpixelSEEDS.iterate", (char**)keywords, &pyobj_img, &num_iterations) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) )
    {
        ERRWRAP2(_self_->iterate(img, num_iterations));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_SuperpixelSEEDS_methods[] =
{
    {"getLabelContourMask", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in SuperpixelSEEDS object.\n.   \n.   @param image Return: CV_8UC1 image mask where -1 indicates that the pixel is a superpixel border,\n.   and 0 otherwise.\n.   \n.   @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border\n.   are masked.\n.   \n.   The function return the boundaries of the superpixel segmentation.\n.   \n.   @note\n.   -   (Python) A demo on how to generate superpixels in images from the webcam can be found at\n.   opencv_source_code/samples/python2/seeds.py\n.   -   (cpp) A demo on how to generate superpixels in images from the webcam can be found at\n.   opencv_source_code/modules/ximgproc/samples/seeds.cpp. By adding a file image as a command\n.   line argument, the static image will be used instead of the webcam.\n.   -   It will show a window with the video from the webcam with the superpixel boundaries marked\n.   in red (see below). Use Space to switch between different output modes. At the top of the\n.   window there are 4 sliders, from which the user can change on-the-fly the number of\n.   superpixels, the number of block levels, the strength of the boundary prior term to modify\n.   the shape, and the number of iterations at pixel level. This is useful to play with the\n.   parameters and set them to the user convenience. In the console the frame-rate of the\n.   algorithm is indicated.\n.   \n.   ![image](pics/superpixels_demo.png)"},
    {"getLabels", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.   Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.   @param labels_out Return: A CV_32UC1 integer array containing the labels of the superpixel\n.   segmentation. The labels are in the range [0, getNumberOfSuperpixels()].\n.   \n.   The function returns an image with ssthe labels of the superpixel segmentation. The labels are in\n.   the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Calculates the superpixel segmentation on a given image stored in SuperpixelSEEDS object.\n.   \n.   The function computes the superpixels segmentation of an image with the parameters initialized\n.   with the function createSuperpixelSEEDS()."},
    {"iterate", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSEEDS_iterate, 0), "iterate(img[, num_iterations]) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.   parameters in the SuperpixelSEEDS object.\n.   \n.   This function can be called again for other images without the need of initializing the\n.   algorithm with createSuperpixelSEEDS(). This save the computational cost of allocating memory\n.   for all the structures of the algorithm.\n.   \n.   @param img Input image. Supported formats: CV_8U, CV_16U, CV_32F. Image size & number of\n.   channels must match with the initialized image size & channels with the function\n.   createSuperpixelSEEDS(). It should be in HSV or Lab color space. Lab is a bit better, but also\n.   slower.\n.   \n.   @param num_iterations Number of pixel level iterations. Higher number improves the result.\n.   \n.   The function computes the superpixels segmentation of an image with the parameters initialized\n.   with the function createSuperpixelSEEDS(). The algorithms starts from a grid of superpixels and\n.   then refines the boundaries by proposing updates of blocks of pixels that lie at the boundaries\n.   from large to smaller size, finalizing with proposing pixel updates. An illustrative example\n.   can be seen below.\n.   \n.   ![image](pics/superpixels_blocks2.png)"},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_SuperpixelSEEDS_specials(void)
{
    pyopencv_ximgproc_SuperpixelSEEDS_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_SuperpixelSEEDS_Type.tp_dealloc = pyopencv_ximgproc_SuperpixelSEEDS_dealloc;
    pyopencv_ximgproc_SuperpixelSEEDS_Type.tp_repr = pyopencv_ximgproc_SuperpixelSEEDS_repr;
    pyopencv_ximgproc_SuperpixelSEEDS_Type.tp_getset = pyopencv_ximgproc_SuperpixelSEEDS_getseters;
    pyopencv_ximgproc_SuperpixelSEEDS_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_SuperpixelSEEDS_Type.tp_methods = pyopencv_ximgproc_SuperpixelSEEDS_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_GraphSegmentation_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_GraphSegmentation %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_GraphSegmentation_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getK());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getMinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_processImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_dst = NULL;
    Mat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_segmentation_GraphSegmentation.processImage", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->processImage(src, dst));
        return pyopencv_from(dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    UMat src;
    PyObject* pyobj_dst = NULL;
    UMat dst;

    const char* keywords[] = { "src", "dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_segmentation_GraphSegmentation.processImage", (char**)keywords, &pyobj_src, &pyobj_dst) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_dst, dst, ArgInfo("dst", 1)) )
    {
        ERRWRAP2(_self_->processImage(src, dst));
        return pyopencv_from(dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    float k=0.f;

    const char* keywords[] = { "k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_segmentation_GraphSegmentation.setK", (char**)keywords, &k) )
    {
        ERRWRAP2(_self_->setK(k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setMinSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    int min_size=0;

    const char* keywords[] = { "min_size", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_segmentation_GraphSegmentation.setMinSize", (char**)keywords, &min_size) )
    {
        ERRWRAP2(_self_->setMinSize(min_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::GraphSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_GraphSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::GraphSegmentation*>(((pyopencv_ximgproc_segmentation_GraphSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_GraphSegmentation' or its derivative)");
    double sigma=0;

    const char* keywords[] = { "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:ximgproc_segmentation_GraphSegmentation.setSigma", (char**)keywords, &sigma) )
    {
        ERRWRAP2(_self_->setSigma(sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_segmentation_GraphSegmentation_methods[] =
{
    {"getK", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getK, 0), "getK() -> retval\n."},
    {"getMinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getMinSize, 0), "getMinSize() -> retval\n."},
    {"getSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_getSigma, 0), "getSigma() -> retval\n."},
    {"processImage", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_processImage, 0), "processImage(src[, dst]) -> dst\n.   @brief Segment an image and store output in dst\n.   @param src The input image. Any number of channel (1 (Eg: Gray), 3 (Eg: RGB), 4 (Eg: RGB-D)) can be provided\n.   @param dst The output segmentation. It's a CV_32SC1 Mat with the same number of cols and rows as input image, with an unique, sequential, id for each pixel."},
    {"setK", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setK, 0), "setK(k) -> None\n."},
    {"setMinSize", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setMinSize, 0), "setMinSize(min_size) -> None\n."},
    {"setSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_GraphSegmentation_setSigma, 0), "setSigma(sigma) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_GraphSegmentation_specials(void)
{
    pyopencv_ximgproc_segmentation_GraphSegmentation_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_segmentation_GraphSegmentation_Type.tp_dealloc = pyopencv_ximgproc_segmentation_GraphSegmentation_dealloc;
    pyopencv_ximgproc_segmentation_GraphSegmentation_Type.tp_repr = pyopencv_ximgproc_segmentation_GraphSegmentation_repr;
    pyopencv_ximgproc_segmentation_GraphSegmentation_Type.tp_getset = pyopencv_ximgproc_segmentation_GraphSegmentation_getseters;
    pyopencv_ximgproc_segmentation_GraphSegmentation_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_GraphSegmentation_Type.tp_methods = pyopencv_ximgproc_segmentation_GraphSegmentation_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentationStrategy %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_get(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategy' or its derivative)");
    int r1=0;
    int r2=0;
    float retval;

    const char* keywords[] = { "r1", "r2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.get", (char**)keywords, &r1, &r2) )
    {
        ERRWRAP2(retval = _self_->get(r1, r2));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_merge(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategy' or its derivative)");
    int r1=0;
    int r2=0;

    const char* keywords[] = { "r1", "r2", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.merge", (char**)keywords, &r1, &r2) )
    {
        ERRWRAP2(_self_->merge(r1, r2));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_setImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategy' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;
    PyObject* pyobj_regions = NULL;
    Mat regions;
    PyObject* pyobj_sizes = NULL;
    Mat sizes;
    int image_id=-1;

    const char* keywords[] = { "img", "regions", "sizes", "image_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|i:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.setImage", (char**)keywords, &pyobj_img, &pyobj_regions, &pyobj_sizes, &image_id) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_regions, regions, ArgInfo("regions", 0)) &&
        pyopencv_to(pyobj_sizes, sizes, ArgInfo("sizes", 0)) )
    {
        ERRWRAP2(_self_->setImage(img, regions, sizes, image_id));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;
    PyObject* pyobj_regions = NULL;
    UMat regions;
    PyObject* pyobj_sizes = NULL;
    UMat sizes;
    int image_id=-1;

    const char* keywords[] = { "img", "regions", "sizes", "image_id", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO|i:ximgproc_segmentation_SelectiveSearchSegmentationStrategy.setImage", (char**)keywords, &pyobj_img, &pyobj_regions, &pyobj_sizes, &image_id) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) &&
        pyopencv_to(pyobj_regions, regions, ArgInfo("regions", 0)) &&
        pyopencv_to(pyobj_sizes, sizes, ArgInfo("sizes", 0)) )
    {
        ERRWRAP2(_self_->setImage(img, regions, sizes, image_id));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_methods[] =
{
    {"get", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_get, 0), "get(r1, r2) -> retval\n.   @brief Return the score between two regions (between 0 and 1)\n.   @param r1 The first region\n.   @param r2 The second region"},
    {"merge", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_merge, 0), "merge(r1, r2) -> None\n.   @brief Inform the strategy that two regions will be merged\n.   @param r1 The first region\n.   @param r2 The second region"},
    {"setImage", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_setImage, 0), "setImage(img, regions, sizes[, image_id]) -> None\n.   @brief Set a initial image, with a segementation.\n.   @param img The input image. Any number of channel can be provided\n.   @param regions A segementation of the image. The parameter must be the same size of img.\n.   @param sizes The sizes of different regions\n.   @param image_id If not set to -1, try to cache pre-computations. If the same set og (img, regions, size) is used, the image_id need to be the same."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type.tp_base = &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyColor_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentationStrategySize %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type.tp_base = &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategySize_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type.tp_base = &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyTexture_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type.tp_base = &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyFill_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_addStrategy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple' or its derivative)");
    PyObject* pyobj_g = NULL;
    Ptr<SelectiveSearchSegmentationStrategy> g;
    float weight=0.f;

    const char* keywords[] = { "g", "weight", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Of:ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple.addStrategy", (char**)keywords, &pyobj_g, &weight) &&
        pyopencv_to(pyobj_g, g, ArgInfo("g", 0)) )
    {
        ERRWRAP2(_self_->addStrategy(g, weight));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_clearStrategies(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategyMultiple*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clearStrategies());
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_methods[] =
{
    {"addStrategy", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_addStrategy, 0), "addStrategy(g, weight) -> None\n.   @brief Add a new sub-strategy\n.   @param g The strategy\n.   @param weight The weight of the strategy"},
    {"clearStrategies", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_clearStrategies, 0), "clearStrategies() -> None\n.   @brief Remove all sub-strategies"},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type.tp_base = &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategy_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentationStrategyMultiple_methods;
}

static PyObject* pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_segmentation_SelectiveSearchSegmentation %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addGraphSegmentation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    PyObject* pyobj_g = NULL;
    Ptr<GraphSegmentation> g;

    const char* keywords[] = { "g", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addGraphSegmentation", (char**)keywords, &pyobj_g) &&
        pyopencv_to(pyobj_g, g, ArgInfo("g", 0)) )
    {
        ERRWRAP2(_self_->addGraphSegmentation(g));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;

    const char* keywords[] = { "img", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addImage", (char**)keywords, &pyobj_img) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) )
    {
        ERRWRAP2(_self_->addImage(img));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;

    const char* keywords[] = { "img", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addImage", (char**)keywords, &pyobj_img) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) )
    {
        ERRWRAP2(_self_->addImage(img));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addStrategy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    PyObject* pyobj_s = NULL;
    Ptr<SelectiveSearchSegmentationStrategy> s;

    const char* keywords[] = { "s", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ximgproc_segmentation_SelectiveSearchSegmentation.addStrategy", (char**)keywords, &pyobj_s) &&
        pyopencv_to(pyobj_s, s, ArgInfo("s", 0)) )
    {
        ERRWRAP2(_self_->addStrategy(s));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearGraphSegmentations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clearGraphSegmentations());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearImages(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clearImages());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearStrategies(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->clearStrategies());
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_process(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    vector_Rect rects;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(_self_->process(rects));
        return pyopencv_from(rects);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_setBaseImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    {
    PyObject* pyobj_img = NULL;
    Mat img;

    const char* keywords[] = { "img", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ximgproc_segmentation_SelectiveSearchSegmentation.setBaseImage", (char**)keywords, &pyobj_img) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) )
    {
        ERRWRAP2(_self_->setBaseImage(img));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_img = NULL;
    UMat img;

    const char* keywords[] = { "img", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:ximgproc_segmentation_SelectiveSearchSegmentation.setBaseImage", (char**)keywords, &pyobj_img) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 0)) )
    {
        ERRWRAP2(_self_->setBaseImage(img));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchFast(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    int base_k=150;
    int inc_k=150;
    float sigma=0.8f;

    const char* keywords[] = { "base_k", "inc_k", "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iif:ximgproc_segmentation_SelectiveSearchSegmentation.switchToSelectiveSearchFast", (char**)keywords, &base_k, &inc_k, &sigma) )
    {
        ERRWRAP2(_self_->switchToSelectiveSearchFast(base_k, inc_k, sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchQuality(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    int base_k=150;
    int inc_k=150;
    float sigma=0.8f;

    const char* keywords[] = { "base_k", "inc_k", "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|iif:ximgproc_segmentation_SelectiveSearchSegmentation.switchToSelectiveSearchQuality", (char**)keywords, &base_k, &inc_k, &sigma) )
    {
        ERRWRAP2(_self_->switchToSelectiveSearchQuality(base_k, inc_k, sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSingleStrategy(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc::segmentation;

    cv::ximgproc::segmentation::SelectiveSearchSegmentation* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type))
        _self_ = dynamic_cast<cv::ximgproc::segmentation::SelectiveSearchSegmentation*>(((pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_segmentation_SelectiveSearchSegmentation' or its derivative)");
    int k=200;
    float sigma=0.8f;

    const char* keywords[] = { "k", "sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|if:ximgproc_segmentation_SelectiveSearchSegmentation.switchToSingleStrategy", (char**)keywords, &k, &sigma) )
    {
        ERRWRAP2(_self_->switchToSingleStrategy(k, sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_methods[] =
{
    {"addGraphSegmentation", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addGraphSegmentation, 0), "addGraphSegmentation(g) -> None\n.   @brief Add a new graph segmentation in the list of graph segementations to process.\n.   @param g The graph segmentation"},
    {"addImage", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addImage, 0), "addImage(img) -> None\n.   @brief Add a new image in the list of images to process.\n.   @param img The image"},
    {"addStrategy", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_addStrategy, 0), "addStrategy(s) -> None\n.   @brief Add a new strategy in the list of strategy to process.\n.   @param s The strategy"},
    {"clearGraphSegmentations", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearGraphSegmentations, 0), "clearGraphSegmentations() -> None\n.   @brief Clear the list of graph segmentations to process;"},
    {"clearImages", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearImages, 0), "clearImages() -> None\n.   @brief Clear the list of images to process"},
    {"clearStrategies", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_clearStrategies, 0), "clearStrategies() -> None\n.   @brief Clear the list of strategy to process;"},
    {"process", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_process, 0), "process() -> rects\n.   @brief Based on all images, graph segmentations and stragies, computes all possible rects and return them\n.   @param rects The list of rects. The first ones are more relevents than the lasts ones."},
    {"setBaseImage", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_setBaseImage, 0), "setBaseImage(img) -> None\n.   @brief Set a image used by switch* functions to initialize the class\n.   @param img The image"},
    {"switchToSelectiveSearchFast", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchFast, 0), "switchToSelectiveSearchFast([, base_k[, inc_k[, sigma]]]) -> None\n.   @brief Initialize the class with the 'Selective search fast' parameters describled in @cite uijlings2013selective.\n.   @param base_k The k parameter for the first graph segmentation\n.   @param inc_k The increment of the k parameter for all graph segmentations\n.   @param sigma The sigma parameter for the graph segmentation"},
    {"switchToSelectiveSearchQuality", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSelectiveSearchQuality, 0), "switchToSelectiveSearchQuality([, base_k[, inc_k[, sigma]]]) -> None\n.   @brief Initialize the class with the 'Selective search fast' parameters describled in @cite uijlings2013selective.\n.   @param base_k The k parameter for the first graph segmentation\n.   @param inc_k The increment of the k parameter for all graph segmentations\n.   @param sigma The sigma parameter for the graph segmentation"},
    {"switchToSingleStrategy", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_segmentation_ximgproc_segmentation_SelectiveSearchSegmentation_switchToSingleStrategy, 0), "switchToSingleStrategy([, k[, sigma]]) -> None\n.   @brief Initialize the class with the 'Single stragegy' parameters describled in @cite uijlings2013selective.\n.   @param k The k parameter for the graph segmentation\n.   @param sigma The sigma parameter for the graph segmentation"},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_specials(void)
{
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type.tp_dealloc = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_dealloc;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type.tp_repr = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_repr;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type.tp_getset = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_getseters;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_Type.tp_methods = pyopencv_ximgproc_segmentation_SelectiveSearchSegmentation_methods;
}

static PyObject* pyopencv_ximgproc_SuperpixelSLIC_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_SuperpixelSLIC %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_SuperpixelSLIC_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_enforceLabelConnectivity(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSLIC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSLIC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSLIC*>(((pyopencv_ximgproc_SuperpixelSLIC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    int min_element_size=25;

    const char* keywords[] = { "min_element_size", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:ximgproc_SuperpixelSLIC.enforceLabelConnectivity", (char**)keywords, &min_element_size) )
    {
        ERRWRAP2(_self_->enforceLabelConnectivity(min_element_size));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabelContourMask(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSLIC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSLIC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSLIC*>(((pyopencv_ximgproc_SuperpixelSLIC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:ximgproc_SuperpixelSLIC.getLabelContourMask", (char**)keywords, &pyobj_image, &thick_line) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(_self_->getLabelContourMask(image, thick_line));
        return pyopencv_from(image);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    bool thick_line=true;

    const char* keywords[] = { "image", "thick_line", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|Ob:ximgproc_SuperpixelSLIC.getLabelContourMask", (char**)keywords, &pyobj_image, &thick_line) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 1)) )
    {
        ERRWRAP2(_self_->getLabelContourMask(image, thick_line));
        return pyopencv_from(image);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSLIC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSLIC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSLIC*>(((pyopencv_ximgproc_SuperpixelSLIC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    {
    PyObject* pyobj_labels_out = NULL;
    Mat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ximgproc_SuperpixelSLIC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        pyopencv_to(pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)) )
    {
        ERRWRAP2(_self_->getLabels(labels_out));
        return pyopencv_from(labels_out);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_labels_out = NULL;
    UMat labels_out;

    const char* keywords[] = { "labels_out", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:ximgproc_SuperpixelSLIC.getLabels", (char**)keywords, &pyobj_labels_out) &&
        pyopencv_to(pyobj_labels_out, labels_out, ArgInfo("labels_out", 1)) )
    {
        ERRWRAP2(_self_->getLabels(labels_out));
        return pyopencv_from(labels_out);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getNumberOfSuperpixels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSLIC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSLIC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSLIC*>(((pyopencv_ximgproc_SuperpixelSLIC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumberOfSuperpixels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_iterate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SuperpixelSLIC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SuperpixelSLIC_Type))
        _self_ = dynamic_cast<cv::ximgproc::SuperpixelSLIC*>(((pyopencv_ximgproc_SuperpixelSLIC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SuperpixelSLIC' or its derivative)");
    int num_iterations=10;

    const char* keywords[] = { "num_iterations", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|i:ximgproc_SuperpixelSLIC.iterate", (char**)keywords, &num_iterations) )
    {
        ERRWRAP2(_self_->iterate(num_iterations));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_SuperpixelSLIC_methods[] =
{
    {"enforceLabelConnectivity", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_enforceLabelConnectivity, 0), "enforceLabelConnectivity([, min_element_size]) -> None\n.   @brief Enforce label connectivity.\n.   \n.   @param min_element_size The minimum element size in percents that should be absorbed into a bigger\n.   superpixel. Given resulted average superpixel size valid value should be in 0-100 range, 25 means\n.   that less then a quarter sized superpixel should be absorbed, this is default.\n.   \n.   The function merge component that is too small, assigning the previously found adjacent label\n.   to this component. Calling this function may change the final number of superpixels."},
    {"getLabelContourMask", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabelContourMask, 0), "getLabelContourMask([, image[, thick_line]]) -> image\n.   @brief Returns the mask of the superpixel segmentation stored in SuperpixelSLIC object.\n.   \n.   @param image Return: CV_8U1 image mask where -1 indicates that the pixel is a superpixel border,\n.   and 0 otherwise.\n.   \n.   @param thick_line If false, the border is only one pixel wide, otherwise all pixels at the border\n.   are masked.\n.   \n.   The function return the boundaries of the superpixel segmentation."},
    {"getLabels", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getLabels, 0), "getLabels([, labels_out]) -> labels_out\n.   @brief Returns the segmentation labeling of the image.\n.   \n.   Each label represents a superpixel, and each pixel is assigned to one superpixel label.\n.   \n.   @param labels_out Return: A CV_32SC1 integer array containing the labels of the superpixel\n.   segmentation. The labels are in the range [0, getNumberOfSuperpixels()].\n.   \n.   The function returns an image with the labels of the superpixel segmentation. The labels are in\n.   the range [0, getNumberOfSuperpixels()]."},
    {"getNumberOfSuperpixels", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_getNumberOfSuperpixels, 0), "getNumberOfSuperpixels() -> retval\n.   @brief Calculates the actual amount of superpixels on a given segmentation computed\n.   and stored in SuperpixelSLIC object."},
    {"iterate", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SuperpixelSLIC_iterate, 0), "iterate([, num_iterations]) -> None\n.   @brief Calculates the superpixel segmentation on a given image with the initialized\n.   parameters in the SuperpixelSLIC object.\n.   \n.   This function can be called again without the need of initializing the algorithm with\n.   createSuperpixelSLIC(). This save the computational cost of allocating memory for all the\n.   structures of the algorithm.\n.   \n.   @param num_iterations Number of iterations. Higher number improves the result.\n.   \n.   The function computes the superpixels segmentation of an image with the parameters initialized\n.   with the function createSuperpixelSLIC(). The algorithms starts from a grid of superpixels and\n.   then refines the boundaries by proposing updates of edges boundaries."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_SuperpixelSLIC_specials(void)
{
    pyopencv_ximgproc_SuperpixelSLIC_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_SuperpixelSLIC_Type.tp_dealloc = pyopencv_ximgproc_SuperpixelSLIC_dealloc;
    pyopencv_ximgproc_SuperpixelSLIC_Type.tp_repr = pyopencv_ximgproc_SuperpixelSLIC_repr;
    pyopencv_ximgproc_SuperpixelSLIC_Type.tp_getset = pyopencv_ximgproc_SuperpixelSLIC_getseters;
    pyopencv_ximgproc_SuperpixelSLIC_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_SuperpixelSLIC_Type.tp_methods = pyopencv_ximgproc_SuperpixelSLIC_methods;
}

static PyObject* pyopencv_ximgproc_SparseMatchInterpolator_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_SparseMatchInterpolator %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_SparseMatchInterpolator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_SparseMatchInterpolator_interpolate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::SparseMatchInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_SparseMatchInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::SparseMatchInterpolator*>(((pyopencv_ximgproc_SparseMatchInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_SparseMatchInterpolator' or its derivative)");
    {
    PyObject* pyobj_from_image = NULL;
    Mat from_image;
    PyObject* pyobj_from_points = NULL;
    Mat from_points;
    PyObject* pyobj_to_image = NULL;
    Mat to_image;
    PyObject* pyobj_to_points = NULL;
    Mat to_points;
    PyObject* pyobj_dense_flow = NULL;
    Mat dense_flow;

    const char* keywords[] = { "from_image", "from_points", "to_image", "to_points", "dense_flow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO|O:ximgproc_SparseMatchInterpolator.interpolate", (char**)keywords, &pyobj_from_image, &pyobj_from_points, &pyobj_to_image, &pyobj_to_points, &pyobj_dense_flow) &&
        pyopencv_to(pyobj_from_image, from_image, ArgInfo("from_image", 0)) &&
        pyopencv_to(pyobj_from_points, from_points, ArgInfo("from_points", 0)) &&
        pyopencv_to(pyobj_to_image, to_image, ArgInfo("to_image", 0)) &&
        pyopencv_to(pyobj_to_points, to_points, ArgInfo("to_points", 0)) &&
        pyopencv_to(pyobj_dense_flow, dense_flow, ArgInfo("dense_flow", 1)) )
    {
        ERRWRAP2(_self_->interpolate(from_image, from_points, to_image, to_points, dense_flow));
        return pyopencv_from(dense_flow);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_from_image = NULL;
    UMat from_image;
    PyObject* pyobj_from_points = NULL;
    UMat from_points;
    PyObject* pyobj_to_image = NULL;
    UMat to_image;
    PyObject* pyobj_to_points = NULL;
    UMat to_points;
    PyObject* pyobj_dense_flow = NULL;
    UMat dense_flow;

    const char* keywords[] = { "from_image", "from_points", "to_image", "to_points", "dense_flow", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO|O:ximgproc_SparseMatchInterpolator.interpolate", (char**)keywords, &pyobj_from_image, &pyobj_from_points, &pyobj_to_image, &pyobj_to_points, &pyobj_dense_flow) &&
        pyopencv_to(pyobj_from_image, from_image, ArgInfo("from_image", 0)) &&
        pyopencv_to(pyobj_from_points, from_points, ArgInfo("from_points", 0)) &&
        pyopencv_to(pyobj_to_image, to_image, ArgInfo("to_image", 0)) &&
        pyopencv_to(pyobj_to_points, to_points, ArgInfo("to_points", 0)) &&
        pyopencv_to(pyobj_dense_flow, dense_flow, ArgInfo("dense_flow", 1)) )
    {
        ERRWRAP2(_self_->interpolate(from_image, from_points, to_image, to_points, dense_flow));
        return pyopencv_from(dense_flow);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_SparseMatchInterpolator_methods[] =
{
    {"interpolate", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_SparseMatchInterpolator_interpolate, 0), "interpolate(from_image, from_points, to_image, to_points[, dense_flow]) -> dense_flow\n.   @brief Interpolate input sparse matches.\n.   \n.   @param from_image first of the two matched images, 8-bit single-channel or three-channel.\n.   \n.   @param from_points points of the from_image for which there are correspondences in the\n.   to_image (Point2f vector, size shouldn't exceed 32767)\n.   \n.   @param to_image second of the two matched images, 8-bit single-channel or three-channel.\n.   \n.   @param to_points points in the to_image corresponding to from_points\n.   (Point2f vector, size shouldn't exceed 32767)\n.   \n.   @param dense_flow output dense matching (two-channel CV_32F image)"},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_SparseMatchInterpolator_specials(void)
{
    pyopencv_ximgproc_SparseMatchInterpolator_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_SparseMatchInterpolator_Type.tp_dealloc = pyopencv_ximgproc_SparseMatchInterpolator_dealloc;
    pyopencv_ximgproc_SparseMatchInterpolator_Type.tp_repr = pyopencv_ximgproc_SparseMatchInterpolator_repr;
    pyopencv_ximgproc_SparseMatchInterpolator_Type.tp_getset = pyopencv_ximgproc_SparseMatchInterpolator_getseters;
    pyopencv_ximgproc_SparseMatchInterpolator_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_SparseMatchInterpolator_Type.tp_methods = pyopencv_ximgproc_SparseMatchInterpolator_methods;
}

static PyObject* pyopencv_ximgproc_EdgeAwareInterpolator_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_EdgeAwareInterpolator %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_EdgeAwareInterpolator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFGSLambda());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFGSSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getK());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getLambda());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getUsePostProcessing(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUsePostProcessing());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float _lambda=0.f;

    const char* keywords[] = { "_lambda", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeAwareInterpolator.setFGSLambda", (char**)keywords, &_lambda) )
    {
        ERRWRAP2(_self_->setFGSLambda(_lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float _sigma=0.f;

    const char* keywords[] = { "_sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeAwareInterpolator.setFGSSigma", (char**)keywords, &_sigma) )
    {
        ERRWRAP2(_self_->setFGSSigma(_sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setK(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    int _k=0;

    const char* keywords[] = { "_k", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:ximgproc_EdgeAwareInterpolator.setK", (char**)keywords, &_k) )
    {
        ERRWRAP2(_self_->setK(_k));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setLambda(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float _lambda=0.f;

    const char* keywords[] = { "_lambda", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeAwareInterpolator.setLambda", (char**)keywords, &_lambda) )
    {
        ERRWRAP2(_self_->setLambda(_lambda));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    float _sigma=0.f;

    const char* keywords[] = { "_sigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:ximgproc_EdgeAwareInterpolator.setSigma", (char**)keywords, &_sigma) )
    {
        ERRWRAP2(_self_->setSigma(_sigma));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setUsePostProcessing(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::EdgeAwareInterpolator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_EdgeAwareInterpolator_Type))
        _self_ = dynamic_cast<cv::ximgproc::EdgeAwareInterpolator*>(((pyopencv_ximgproc_EdgeAwareInterpolator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_EdgeAwareInterpolator' or its derivative)");
    bool _use_post_proc=0;

    const char* keywords[] = { "_use_post_proc", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:ximgproc_EdgeAwareInterpolator.setUsePostProcessing", (char**)keywords, &_use_post_proc) )
    {
        ERRWRAP2(_self_->setUsePostProcessing(_use_post_proc));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_EdgeAwareInterpolator_methods[] =
{
    {"getFGSLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSLambda, 0), "getFGSLambda() -> retval\n.   @see setFGSLambda"},
    {"getFGSSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getFGSSigma, 0), "getFGSSigma() -> retval\n.   @see setFGSLambda"},
    {"getK", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getK, 0), "getK() -> retval\n.   @see setK"},
    {"getLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getLambda, 0), "getLambda() -> retval\n.   @see setLambda"},
    {"getSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getSigma, 0), "getSigma() -> retval\n.   @see setSigma"},
    {"getUsePostProcessing", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_getUsePostProcessing, 0), "getUsePostProcessing() -> retval\n.   @see setUsePostProcessing"},
    {"setFGSLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSLambda, 0), "setFGSLambda(_lambda) -> None\n.   @brief Sets the respective fastGlobalSmootherFilter() parameter."},
    {"setFGSSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setFGSSigma, 0), "setFGSSigma(_sigma) -> None\n.   @see setFGSLambda"},
    {"setK", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setK, 0), "setK(_k) -> None\n.   @brief K is a number of nearest-neighbor matches considered, when fitting a locally affine\n.   model. Usually it should be around 128. However, lower values would make the interpolation\n.   noticeably faster."},
    {"setLambda", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setLambda, 0), "setLambda(_lambda) -> None\n.   @brief Lambda is a parameter defining the weight of the edge-aware term in geodesic distance,\n.   should be in the range of 0 to 1000."},
    {"setSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setSigma, 0), "setSigma(_sigma) -> None\n.   @brief Sigma is a parameter defining how fast the weights decrease in the locally-weighted affine\n.   fitting. Higher values can help preserve fine details, lower values can help to get rid of noise in the\n.   output flow."},
    {"setUsePostProcessing", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_EdgeAwareInterpolator_setUsePostProcessing, 0), "setUsePostProcessing(_use_post_proc) -> None\n.   @brief Sets whether the fastGlobalSmootherFilter() post-processing is employed. It is turned on by\n.   default."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_EdgeAwareInterpolator_specials(void)
{
    pyopencv_ximgproc_EdgeAwareInterpolator_Type.tp_base = &pyopencv_ximgproc_SparseMatchInterpolator_Type;
    pyopencv_ximgproc_EdgeAwareInterpolator_Type.tp_dealloc = pyopencv_ximgproc_EdgeAwareInterpolator_dealloc;
    pyopencv_ximgproc_EdgeAwareInterpolator_Type.tp_repr = pyopencv_ximgproc_EdgeAwareInterpolator_repr;
    pyopencv_ximgproc_EdgeAwareInterpolator_Type.tp_getset = pyopencv_ximgproc_EdgeAwareInterpolator_getseters;
    pyopencv_ximgproc_EdgeAwareInterpolator_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_EdgeAwareInterpolator_Type.tp_methods = pyopencv_ximgproc_EdgeAwareInterpolator_methods;
}

static PyObject* pyopencv_ximgproc_RFFeatureGetter_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_RFFeatureGetter %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_RFFeatureGetter_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_RFFeatureGetter_getFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::RFFeatureGetter* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_RFFeatureGetter_Type))
        _self_ = dynamic_cast<cv::ximgproc::RFFeatureGetter*>(((pyopencv_ximgproc_RFFeatureGetter_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_RFFeatureGetter' or its derivative)");
    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_features = NULL;
    Mat features;
    int gnrmRad=0;
    int gsmthRad=0;
    int shrink=0;
    int outNum=0;
    int gradNum=0;

    const char* keywords[] = { "src", "features", "gnrmRad", "gsmthRad", "shrink", "outNum", "gradNum", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOiiiii:ximgproc_RFFeatureGetter.getFeatures", (char**)keywords, &pyobj_src, &pyobj_features, &gnrmRad, &gsmthRad, &shrink, &outNum, &gradNum) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) )
    {
        ERRWRAP2(_self_->getFeatures(src, features, gnrmRad, gsmthRad, shrink, outNum, gradNum));
        Py_RETURN_NONE;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_src = NULL;
    Mat src;
    PyObject* pyobj_features = NULL;
    Mat features;
    int gnrmRad=0;
    int gsmthRad=0;
    int shrink=0;
    int outNum=0;
    int gradNum=0;

    const char* keywords[] = { "src", "features", "gnrmRad", "gsmthRad", "shrink", "outNum", "gradNum", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOiiiii:ximgproc_RFFeatureGetter.getFeatures", (char**)keywords, &pyobj_src, &pyobj_features, &gnrmRad, &gsmthRad, &shrink, &outNum, &gradNum) &&
        pyopencv_to(pyobj_src, src, ArgInfo("src", 0)) &&
        pyopencv_to(pyobj_features, features, ArgInfo("features", 0)) )
    {
        ERRWRAP2(_self_->getFeatures(src, features, gnrmRad, gsmthRad, shrink, outNum, gradNum));
        Py_RETURN_NONE;
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_RFFeatureGetter_methods[] =
{
    {"getFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_RFFeatureGetter_getFeatures, 0), "getFeatures(src, features, gnrmRad, gsmthRad, shrink, outNum, gradNum) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_RFFeatureGetter_specials(void)
{
    pyopencv_ximgproc_RFFeatureGetter_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_RFFeatureGetter_Type.tp_dealloc = pyopencv_ximgproc_RFFeatureGetter_dealloc;
    pyopencv_ximgproc_RFFeatureGetter_Type.tp_repr = pyopencv_ximgproc_RFFeatureGetter_repr;
    pyopencv_ximgproc_RFFeatureGetter_Type.tp_getset = pyopencv_ximgproc_RFFeatureGetter_getseters;
    pyopencv_ximgproc_RFFeatureGetter_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_RFFeatureGetter_Type.tp_methods = pyopencv_ximgproc_RFFeatureGetter_methods;
}

static PyObject* pyopencv_ximgproc_StructuredEdgeDetection_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<ximgproc_StructuredEdgeDetection %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_ximgproc_StructuredEdgeDetection_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_computeOrientation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::StructuredEdgeDetection* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_StructuredEdgeDetection_Type))
        _self_ = dynamic_cast<cv::ximgproc::StructuredEdgeDetection*>(((pyopencv_ximgproc_StructuredEdgeDetection_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_StructuredEdgeDetection' or its derivative)");
    {
    PyObject* pyobj__src = NULL;
    Mat _src;
    PyObject* pyobj__dst = NULL;
    Mat _dst;

    const char* keywords[] = { "_src", "_dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_StructuredEdgeDetection.computeOrientation", (char**)keywords, &pyobj__src, &pyobj__dst) &&
        pyopencv_to(pyobj__src, _src, ArgInfo("_src", 0)) &&
        pyopencv_to(pyobj__dst, _dst, ArgInfo("_dst", 1)) )
    {
        ERRWRAP2(_self_->computeOrientation(_src, _dst));
        return pyopencv_from(_dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__src = NULL;
    UMat _src;
    PyObject* pyobj__dst = NULL;
    UMat _dst;

    const char* keywords[] = { "_src", "_dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_StructuredEdgeDetection.computeOrientation", (char**)keywords, &pyobj__src, &pyobj__dst) &&
        pyopencv_to(pyobj__src, _src, ArgInfo("_src", 0)) &&
        pyopencv_to(pyobj__dst, _dst, ArgInfo("_dst", 1)) )
    {
        ERRWRAP2(_self_->computeOrientation(_src, _dst));
        return pyopencv_from(_dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_detectEdges(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::StructuredEdgeDetection* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_StructuredEdgeDetection_Type))
        _self_ = dynamic_cast<cv::ximgproc::StructuredEdgeDetection*>(((pyopencv_ximgproc_StructuredEdgeDetection_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_StructuredEdgeDetection' or its derivative)");
    {
    PyObject* pyobj__src = NULL;
    Mat _src;
    PyObject* pyobj__dst = NULL;
    Mat _dst;

    const char* keywords[] = { "_src", "_dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_StructuredEdgeDetection.detectEdges", (char**)keywords, &pyobj__src, &pyobj__dst) &&
        pyopencv_to(pyobj__src, _src, ArgInfo("_src", 0)) &&
        pyopencv_to(pyobj__dst, _dst, ArgInfo("_dst", 1)) )
    {
        ERRWRAP2(_self_->detectEdges(_src, _dst));
        return pyopencv_from(_dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj__src = NULL;
    UMat _src;
    PyObject* pyobj__dst = NULL;
    UMat _dst;

    const char* keywords[] = { "_src", "_dst", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:ximgproc_StructuredEdgeDetection.detectEdges", (char**)keywords, &pyobj__src, &pyobj__dst) &&
        pyopencv_to(pyobj__src, _src, ArgInfo("_src", 0)) &&
        pyopencv_to(pyobj__dst, _dst, ArgInfo("_dst", 1)) )
    {
        ERRWRAP2(_self_->detectEdges(_src, _dst));
        return pyopencv_from(_dst);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_edgesNms(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::ximgproc;

    cv::ximgproc::StructuredEdgeDetection* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_ximgproc_StructuredEdgeDetection_Type))
        _self_ = dynamic_cast<cv::ximgproc::StructuredEdgeDetection*>(((pyopencv_ximgproc_StructuredEdgeDetection_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'ximgproc_StructuredEdgeDetection' or its derivative)");
    {
    PyObject* pyobj_edge_image = NULL;
    Mat edge_image;
    PyObject* pyobj_orientation_image = NULL;
    Mat orientation_image;
    PyObject* pyobj__dst = NULL;
    Mat _dst;
    int r=2;
    int s=0;
    float m=1;
    bool isParallel=true;

    const char* keywords[] = { "edge_image", "orientation_image", "_dst", "r", "s", "m", "isParallel", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Oiifb:ximgproc_StructuredEdgeDetection.edgesNms", (char**)keywords, &pyobj_edge_image, &pyobj_orientation_image, &pyobj__dst, &r, &s, &m, &isParallel) &&
        pyopencv_to(pyobj_edge_image, edge_image, ArgInfo("edge_image", 0)) &&
        pyopencv_to(pyobj_orientation_image, orientation_image, ArgInfo("orientation_image", 0)) &&
        pyopencv_to(pyobj__dst, _dst, ArgInfo("_dst", 1)) )
    {
        ERRWRAP2(_self_->edgesNms(edge_image, orientation_image, _dst, r, s, m, isParallel));
        return pyopencv_from(_dst);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_edge_image = NULL;
    UMat edge_image;
    PyObject* pyobj_orientation_image = NULL;
    UMat orientation_image;
    PyObject* pyobj__dst = NULL;
    UMat _dst;
    int r=2;
    int s=0;
    float m=1;
    bool isParallel=true;

    const char* keywords[] = { "edge_image", "orientation_image", "_dst", "r", "s", "m", "isParallel", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OO|Oiifb:ximgproc_StructuredEdgeDetection.edgesNms", (char**)keywords, &pyobj_edge_image, &pyobj_orientation_image, &pyobj__dst, &r, &s, &m, &isParallel) &&
        pyopencv_to(pyobj_edge_image, edge_image, ArgInfo("edge_image", 0)) &&
        pyopencv_to(pyobj_orientation_image, orientation_image, ArgInfo("orientation_image", 0)) &&
        pyopencv_to(pyobj__dst, _dst, ArgInfo("_dst", 1)) )
    {
        ERRWRAP2(_self_->edgesNms(edge_image, orientation_image, _dst, r, s, m, isParallel));
        return pyopencv_from(_dst);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_ximgproc_StructuredEdgeDetection_methods[] =
{
    {"computeOrientation", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_computeOrientation, 0), "computeOrientation(_src[, _dst]) -> _dst\n.   @brief The function computes orientation from edge image.\n.   \n.   @param _src edge image.\n.   @param _dst orientation image."},
    {"detectEdges", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_detectEdges, 0), "detectEdges(_src[, _dst]) -> _dst\n.   @brief The function detects edges in src and draw them to dst.\n.   \n.   The algorithm underlies this function is much more robust to texture presence, than common\n.   approaches, e.g. Sobel\n.   @param _src source image (RGB, float, in [0;1]) to detect edges\n.   @param _dst destination image (grayscale, float, in [0;1]) where edges are drawn\n.   @sa Sobel, Canny"},
    {"edgesNms", CV_PY_FN_WITH_KW_(pyopencv_cv_ximgproc_ximgproc_StructuredEdgeDetection_edgesNms, 0), "edgesNms(edge_image, orientation_image[, _dst[, r[, s[, m[, isParallel]]]]]) -> _dst\n.   @brief The function edgenms in edge image and suppress edges where edge is stronger in orthogonal direction.\n.   \n.   @param edge_image edge image from detectEdges function.\n.   @param orientation_image orientation image from computeOrientation function.\n.   @param _dst suppressed image (grayscale, float, in [0;1])\n.   @param r radius for NMS suppression.\n.   @param s radius for boundary suppression.\n.   @param m multiplier for conservative suppression.\n.   @param isParallel enables/disables parallel computing."},

    {NULL,          NULL}
};

static void pyopencv_ximgproc_StructuredEdgeDetection_specials(void)
{
    pyopencv_ximgproc_StructuredEdgeDetection_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_ximgproc_StructuredEdgeDetection_Type.tp_dealloc = pyopencv_ximgproc_StructuredEdgeDetection_dealloc;
    pyopencv_ximgproc_StructuredEdgeDetection_Type.tp_repr = pyopencv_ximgproc_StructuredEdgeDetection_repr;
    pyopencv_ximgproc_StructuredEdgeDetection_Type.tp_getset = pyopencv_ximgproc_StructuredEdgeDetection_getseters;
    pyopencv_ximgproc_StructuredEdgeDetection_Type.tp_init = (initproc)0;
    pyopencv_ximgproc_StructuredEdgeDetection_Type.tp_methods = pyopencv_ximgproc_StructuredEdgeDetection_methods;
}

static PyObject* pyopencv_aruco_DetectorParameters_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<aruco_DetectorParameters %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshConstant(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->adaptiveThreshConstant);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshConstant(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshConstant attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->adaptiveThreshConstant) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMax(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->adaptiveThreshWinSizeMax);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMax(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeMax attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->adaptiveThreshWinSizeMax) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMin(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->adaptiveThreshWinSizeMin);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMin(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeMin attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->adaptiveThreshWinSizeMin) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeStep(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->adaptiveThreshWinSizeStep);
}

static int pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeStep(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the adaptiveThreshWinSizeStep attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->adaptiveThreshWinSizeStep) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagCriticalRad(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagCriticalRad);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagCriticalRad(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagCriticalRad attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagCriticalRad) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagDeglitch(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagDeglitch);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagDeglitch(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagDeglitch attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagDeglitch) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMaxLineFitMse(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagMaxLineFitMse);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMaxLineFitMse(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMaxLineFitMse attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagMaxLineFitMse) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMaxNmaxima(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagMaxNmaxima);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMaxNmaxima(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMaxNmaxima attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagMaxNmaxima) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMinClusterPixels(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagMinClusterPixels);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMinClusterPixels(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMinClusterPixels attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagMinClusterPixels) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagMinWhiteBlackDiff(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagMinWhiteBlackDiff);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagMinWhiteBlackDiff(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagMinWhiteBlackDiff attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagMinWhiteBlackDiff) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagQuadDecimate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagQuadDecimate);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagQuadDecimate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagQuadDecimate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagQuadDecimate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_aprilTagQuadSigma(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->aprilTagQuadSigma);
}

static int pyopencv_aruco_DetectorParameters_set_aprilTagQuadSigma(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the aprilTagQuadSigma attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->aprilTagQuadSigma) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementMaxIterations(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->cornerRefinementMaxIterations);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementMaxIterations(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementMaxIterations attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->cornerRefinementMaxIterations) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementMethod(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->cornerRefinementMethod);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementMethod(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementMethod attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->cornerRefinementMethod) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementMinAccuracy(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->cornerRefinementMinAccuracy);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementMinAccuracy(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementMinAccuracy attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->cornerRefinementMinAccuracy) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_cornerRefinementWinSize(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->cornerRefinementWinSize);
}

static int pyopencv_aruco_DetectorParameters_set_cornerRefinementWinSize(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the cornerRefinementWinSize attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->cornerRefinementWinSize) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_errorCorrectionRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->errorCorrectionRate);
}

static int pyopencv_aruco_DetectorParameters_set_errorCorrectionRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the errorCorrectionRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->errorCorrectionRate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_markerBorderBits(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->markerBorderBits);
}

static int pyopencv_aruco_DetectorParameters_set_markerBorderBits(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the markerBorderBits attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->markerBorderBits) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_maxErroneousBitsInBorderRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->maxErroneousBitsInBorderRate);
}

static int pyopencv_aruco_DetectorParameters_set_maxErroneousBitsInBorderRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxErroneousBitsInBorderRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->maxErroneousBitsInBorderRate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_maxMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->maxMarkerPerimeterRate);
}

static int pyopencv_aruco_DetectorParameters_set_maxMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxMarkerPerimeterRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->maxMarkerPerimeterRate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minCornerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->minCornerDistanceRate);
}

static int pyopencv_aruco_DetectorParameters_set_minCornerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minCornerDistanceRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->minCornerDistanceRate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minDistanceToBorder(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->minDistanceToBorder);
}

static int pyopencv_aruco_DetectorParameters_set_minDistanceToBorder(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minDistanceToBorder attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->minDistanceToBorder) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minMarkerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->minMarkerDistanceRate);
}

static int pyopencv_aruco_DetectorParameters_set_minMarkerDistanceRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minMarkerDistanceRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->minMarkerDistanceRate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->minMarkerPerimeterRate);
}

static int pyopencv_aruco_DetectorParameters_set_minMarkerPerimeterRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minMarkerPerimeterRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->minMarkerPerimeterRate) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_minOtsuStdDev(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->minOtsuStdDev);
}

static int pyopencv_aruco_DetectorParameters_set_minOtsuStdDev(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the minOtsuStdDev attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->minOtsuStdDev) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_perspectiveRemoveIgnoredMarginPerCell(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->perspectiveRemoveIgnoredMarginPerCell);
}

static int pyopencv_aruco_DetectorParameters_set_perspectiveRemoveIgnoredMarginPerCell(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the perspectiveRemoveIgnoredMarginPerCell attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->perspectiveRemoveIgnoredMarginPerCell) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_perspectiveRemovePixelPerCell(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->perspectiveRemovePixelPerCell);
}

static int pyopencv_aruco_DetectorParameters_set_perspectiveRemovePixelPerCell(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the perspectiveRemovePixelPerCell attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->perspectiveRemovePixelPerCell) ? 0 : -1;
}

static PyObject* pyopencv_aruco_DetectorParameters_get_polygonalApproxAccuracyRate(pyopencv_aruco_DetectorParameters_t* p, void *closure)
{
    return pyopencv_from(p->v->polygonalApproxAccuracyRate);
}

static int pyopencv_aruco_DetectorParameters_set_polygonalApproxAccuracyRate(pyopencv_aruco_DetectorParameters_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the polygonalApproxAccuracyRate attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->polygonalApproxAccuracyRate) ? 0 : -1;
}


static PyGetSetDef pyopencv_aruco_DetectorParameters_getseters[] =
{
    {(char*)"adaptiveThreshConstant", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshConstant, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshConstant, (char*)"adaptiveThreshConstant", NULL},
    {(char*)"adaptiveThreshWinSizeMax", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMax, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMax, (char*)"adaptiveThreshWinSizeMax", NULL},
    {(char*)"adaptiveThreshWinSizeMin", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeMin, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeMin, (char*)"adaptiveThreshWinSizeMin", NULL},
    {(char*)"adaptiveThreshWinSizeStep", (getter)pyopencv_aruco_DetectorParameters_get_adaptiveThreshWinSizeStep, (setter)pyopencv_aruco_DetectorParameters_set_adaptiveThreshWinSizeStep, (char*)"adaptiveThreshWinSizeStep", NULL},
    {(char*)"aprilTagCriticalRad", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagCriticalRad, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagCriticalRad, (char*)"aprilTagCriticalRad", NULL},
    {(char*)"aprilTagDeglitch", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagDeglitch, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagDeglitch, (char*)"aprilTagDeglitch", NULL},
    {(char*)"aprilTagMaxLineFitMse", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMaxLineFitMse, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMaxLineFitMse, (char*)"aprilTagMaxLineFitMse", NULL},
    {(char*)"aprilTagMaxNmaxima", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMaxNmaxima, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMaxNmaxima, (char*)"aprilTagMaxNmaxima", NULL},
    {(char*)"aprilTagMinClusterPixels", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMinClusterPixels, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMinClusterPixels, (char*)"aprilTagMinClusterPixels", NULL},
    {(char*)"aprilTagMinWhiteBlackDiff", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagMinWhiteBlackDiff, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagMinWhiteBlackDiff, (char*)"aprilTagMinWhiteBlackDiff", NULL},
    {(char*)"aprilTagQuadDecimate", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagQuadDecimate, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagQuadDecimate, (char*)"aprilTagQuadDecimate", NULL},
    {(char*)"aprilTagQuadSigma", (getter)pyopencv_aruco_DetectorParameters_get_aprilTagQuadSigma, (setter)pyopencv_aruco_DetectorParameters_set_aprilTagQuadSigma, (char*)"aprilTagQuadSigma", NULL},
    {(char*)"cornerRefinementMaxIterations", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementMaxIterations, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementMaxIterations, (char*)"cornerRefinementMaxIterations", NULL},
    {(char*)"cornerRefinementMethod", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementMethod, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementMethod, (char*)"cornerRefinementMethod", NULL},
    {(char*)"cornerRefinementMinAccuracy", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementMinAccuracy, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementMinAccuracy, (char*)"cornerRefinementMinAccuracy", NULL},
    {(char*)"cornerRefinementWinSize", (getter)pyopencv_aruco_DetectorParameters_get_cornerRefinementWinSize, (setter)pyopencv_aruco_DetectorParameters_set_cornerRefinementWinSize, (char*)"cornerRefinementWinSize", NULL},
    {(char*)"errorCorrectionRate", (getter)pyopencv_aruco_DetectorParameters_get_errorCorrectionRate, (setter)pyopencv_aruco_DetectorParameters_set_errorCorrectionRate, (char*)"errorCorrectionRate", NULL},
    {(char*)"markerBorderBits", (getter)pyopencv_aruco_DetectorParameters_get_markerBorderBits, (setter)pyopencv_aruco_DetectorParameters_set_markerBorderBits, (char*)"markerBorderBits", NULL},
    {(char*)"maxErroneousBitsInBorderRate", (getter)pyopencv_aruco_DetectorParameters_get_maxErroneousBitsInBorderRate, (setter)pyopencv_aruco_DetectorParameters_set_maxErroneousBitsInBorderRate, (char*)"maxErroneousBitsInBorderRate", NULL},
    {(char*)"maxMarkerPerimeterRate", (getter)pyopencv_aruco_DetectorParameters_get_maxMarkerPerimeterRate, (setter)pyopencv_aruco_DetectorParameters_set_maxMarkerPerimeterRate, (char*)"maxMarkerPerimeterRate", NULL},
    {(char*)"minCornerDistanceRate", (getter)pyopencv_aruco_DetectorParameters_get_minCornerDistanceRate, (setter)pyopencv_aruco_DetectorParameters_set_minCornerDistanceRate, (char*)"minCornerDistanceRate", NULL},
    {(char*)"minDistanceToBorder", (getter)pyopencv_aruco_DetectorParameters_get_minDistanceToBorder, (setter)pyopencv_aruco_DetectorParameters_set_minDistanceToBorder, (char*)"minDistanceToBorder", NULL},
    {(char*)"minMarkerDistanceRate", (getter)pyopencv_aruco_DetectorParameters_get_minMarkerDistanceRate, (setter)pyopencv_aruco_DetectorParameters_set_minMarkerDistanceRate, (char*)"minMarkerDistanceRate", NULL},
    {(char*)"minMarkerPerimeterRate", (getter)pyopencv_aruco_DetectorParameters_get_minMarkerPerimeterRate, (setter)pyopencv_aruco_DetectorParameters_set_minMarkerPerimeterRate, (char*)"minMarkerPerimeterRate", NULL},
    {(char*)"minOtsuStdDev", (getter)pyopencv_aruco_DetectorParameters_get_minOtsuStdDev, (setter)pyopencv_aruco_DetectorParameters_set_minOtsuStdDev, (char*)"minOtsuStdDev", NULL},
    {(char*)"perspectiveRemoveIgnoredMarginPerCell", (getter)pyopencv_aruco_DetectorParameters_get_perspectiveRemoveIgnoredMarginPerCell, (setter)pyopencv_aruco_DetectorParameters_set_perspectiveRemoveIgnoredMarginPerCell, (char*)"perspectiveRemoveIgnoredMarginPerCell", NULL},
    {(char*)"perspectiveRemovePixelPerCell", (getter)pyopencv_aruco_DetectorParameters_get_perspectiveRemovePixelPerCell, (setter)pyopencv_aruco_DetectorParameters_set_perspectiveRemovePixelPerCell, (char*)"perspectiveRemovePixelPerCell", NULL},
    {(char*)"polygonalApproxAccuracyRate", (getter)pyopencv_aruco_DetectorParameters_get_polygonalApproxAccuracyRate, (setter)pyopencv_aruco_DetectorParameters_set_polygonalApproxAccuracyRate, (char*)"polygonalApproxAccuracyRate", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_aruco_aruco_DetectorParameters_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    Ptr<DetectorParameters> retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = cv::aruco::DetectorParameters::create());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_aruco_DetectorParameters_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_DetectorParameters_create_cls, METH_CLASS), "create() -> retval\n."},

    {NULL,          NULL}
};

static void pyopencv_aruco_DetectorParameters_specials(void)
{
    pyopencv_aruco_DetectorParameters_Type.tp_base = NULL;
    pyopencv_aruco_DetectorParameters_Type.tp_dealloc = pyopencv_aruco_DetectorParameters_dealloc;
    pyopencv_aruco_DetectorParameters_Type.tp_repr = pyopencv_aruco_DetectorParameters_repr;
    pyopencv_aruco_DetectorParameters_Type.tp_getset = pyopencv_aruco_DetectorParameters_getseters;
    pyopencv_aruco_DetectorParameters_Type.tp_init = (initproc)0;
    pyopencv_aruco_DetectorParameters_Type.tp_methods = pyopencv_aruco_DetectorParameters_methods;
}

static PyObject* pyopencv_aruco_Board_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<aruco_Board %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_aruco_Board_get_dictionary(pyopencv_aruco_Board_t* p, void *closure)
{
    return pyopencv_from(p->v->dictionary);
}

static PyObject* pyopencv_aruco_Board_get_ids(pyopencv_aruco_Board_t* p, void *closure)
{
    return pyopencv_from(p->v->ids);
}

static PyObject* pyopencv_aruco_Board_get_objPoints(pyopencv_aruco_Board_t* p, void *closure)
{
    return pyopencv_from(p->v->objPoints);
}


static PyGetSetDef pyopencv_aruco_Board_getseters[] =
{
    {(char*)"dictionary", (getter)pyopencv_aruco_Board_get_dictionary, NULL, (char*)"dictionary", NULL},
    {(char*)"ids", (getter)pyopencv_aruco_Board_get_ids, NULL, (char*)"ids", NULL},
    {(char*)"objPoints", (getter)pyopencv_aruco_Board_get_objPoints, NULL, (char*)"objPoints", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_aruco_aruco_Board_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    {
    PyObject* pyobj_objPoints = NULL;
    vector_Mat objPoints;
    PyObject* pyobj_dictionary = NULL;
    Ptr<Dictionary> dictionary;
    PyObject* pyobj_ids = NULL;
    Mat ids;
    Ptr<Board> retval;

    const char* keywords[] = { "objPoints", "dictionary", "ids", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:aruco_Board.create", (char**)keywords, &pyobj_objPoints, &pyobj_dictionary, &pyobj_ids) &&
        pyopencv_to(pyobj_objPoints, objPoints, ArgInfo("objPoints", 0)) &&
        pyopencv_to(pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        pyopencv_to(pyobj_ids, ids, ArgInfo("ids", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Board::create(objPoints, dictionary, ids));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_objPoints = NULL;
    vector_Mat objPoints;
    PyObject* pyobj_dictionary = NULL;
    Ptr<Dictionary> dictionary;
    PyObject* pyobj_ids = NULL;
    UMat ids;
    Ptr<Board> retval;

    const char* keywords[] = { "objPoints", "dictionary", "ids", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOO:aruco_Board.create", (char**)keywords, &pyobj_objPoints, &pyobj_dictionary, &pyobj_ids) &&
        pyopencv_to(pyobj_objPoints, objPoints, ArgInfo("objPoints", 0)) &&
        pyopencv_to(pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) &&
        pyopencv_to(pyobj_ids, ids, ArgInfo("ids", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Board::create(objPoints, dictionary, ids));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_aruco_Board_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Board_create_cls, METH_CLASS), "create(objPoints, dictionary, ids) -> retval\n.   * @brief Provide way to create Board by passing nessesary data. Specially needed in Python.\n.   *\n.   * @param objPoints array of object points of all the marker corners in the board\n.   * @param dictionary the dictionary of markers employed for this board\n.   * @param ids vector of the identifiers of the markers in the board\n.   *"},

    {NULL,          NULL}
};

static void pyopencv_aruco_Board_specials(void)
{
    pyopencv_aruco_Board_Type.tp_base = NULL;
    pyopencv_aruco_Board_Type.tp_dealloc = pyopencv_aruco_Board_dealloc;
    pyopencv_aruco_Board_Type.tp_repr = pyopencv_aruco_Board_repr;
    pyopencv_aruco_Board_Type.tp_getset = pyopencv_aruco_Board_getseters;
    pyopencv_aruco_Board_Type.tp_init = (initproc)0;
    pyopencv_aruco_Board_Type.tp_methods = pyopencv_aruco_Board_methods;
}

static PyObject* pyopencv_aruco_GridBoard_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<aruco_GridBoard %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_aruco_GridBoard_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_aruco_aruco_GridBoard_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    int markersX=0;
    int markersY=0;
    float markerLength=0.f;
    float markerSeparation=0.f;
    PyObject* pyobj_dictionary = NULL;
    Ptr<Dictionary> dictionary;
    int firstMarker=0;
    Ptr<GridBoard> retval;

    const char* keywords[] = { "markersX", "markersY", "markerLength", "markerSeparation", "dictionary", "firstMarker", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiffO|i:aruco_GridBoard.create", (char**)keywords, &markersX, &markersY, &markerLength, &markerSeparation, &pyobj_dictionary, &firstMarker) &&
        pyopencv_to(pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::GridBoard::create(markersX, markersY, markerLength, markerSeparation, dictionary, firstMarker));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_GridBoard_draw(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::GridBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_GridBoard_Type))
        _self_ = ((pyopencv_aruco_GridBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    {
    PyObject* pyobj_outSize = NULL;
    Size outSize;
    PyObject* pyobj_img = NULL;
    Mat img;
    int marginSize=0;
    int borderBits=1;

    const char* keywords[] = { "outSize", "img", "marginSize", "borderBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oii:aruco_GridBoard.draw", (char**)keywords, &pyobj_outSize, &pyobj_img, &marginSize, &borderBits) &&
        pyopencv_to(pyobj_outSize, outSize, ArgInfo("outSize", 0)) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 1)) )
    {
        ERRWRAP2(_self_->draw(outSize, img, marginSize, borderBits));
        return pyopencv_from(img);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_outSize = NULL;
    Size outSize;
    PyObject* pyobj_img = NULL;
    UMat img;
    int marginSize=0;
    int borderBits=1;

    const char* keywords[] = { "outSize", "img", "marginSize", "borderBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oii:aruco_GridBoard.draw", (char**)keywords, &pyobj_outSize, &pyobj_img, &marginSize, &borderBits) &&
        pyopencv_to(pyobj_outSize, outSize, ArgInfo("outSize", 0)) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 1)) )
    {
        ERRWRAP2(_self_->draw(outSize, img, marginSize, borderBits));
        return pyopencv_from(img);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_GridBoard_getGridSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::GridBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_GridBoard_Type))
        _self_ = ((pyopencv_aruco_GridBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGridSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_GridBoard_getMarkerLength(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::GridBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_GridBoard_Type))
        _self_ = ((pyopencv_aruco_GridBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMarkerLength());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_GridBoard_getMarkerSeparation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::GridBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_GridBoard_Type))
        _self_ = ((pyopencv_aruco_GridBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_GridBoard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMarkerSeparation());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_aruco_GridBoard_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_create_cls, METH_CLASS), "create(markersX, markersY, markerLength, markerSeparation, dictionary[, firstMarker]) -> retval\n.   * @brief Create a GridBoard object\n.   *\n.   * @param markersX number of markers in X direction\n.   * @param markersY number of markers in Y direction\n.   * @param markerLength marker side length (normally in meters)\n.   * @param markerSeparation separation between two markers (same unit as markerLength)\n.   * @param dictionary dictionary of markers indicating the type of markers\n.   * @param firstMarker id of first marker in dictionary to use on board.\n.   * @return the output GridBoard object\n.   *\n.   * This functions creates a GridBoard object given the number of markers in each direction and\n.   * the marker size and marker separation."},
    {"draw", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_draw, 0), "draw(outSize[, img[, marginSize[, borderBits]]]) -> img\n.   * @brief Draw a GridBoard\n.   *\n.   * @param outSize size of the output image in pixels.\n.   * @param img output image with the board. The size of this image will be outSize\n.   * and the board will be on the center, keeping the board proportions.\n.   * @param marginSize minimum margins (in pixels) of the board in the output image\n.   * @param borderBits width of the marker borders.\n.   *\n.   * This function return the image of the GridBoard, ready to be printed."},
    {"getGridSize", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_getGridSize, 0), "getGridSize() -> retval\n.   *"},
    {"getMarkerLength", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_getMarkerLength, 0), "getMarkerLength() -> retval\n.   *"},
    {"getMarkerSeparation", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_GridBoard_getMarkerSeparation, 0), "getMarkerSeparation() -> retval\n.   *"},

    {NULL,          NULL}
};

static void pyopencv_aruco_GridBoard_specials(void)
{
    pyopencv_aruco_GridBoard_Type.tp_base = &pyopencv_aruco_Board_Type;
    pyopencv_aruco_GridBoard_Type.tp_dealloc = pyopencv_aruco_GridBoard_dealloc;
    pyopencv_aruco_GridBoard_Type.tp_repr = pyopencv_aruco_GridBoard_repr;
    pyopencv_aruco_GridBoard_Type.tp_getset = pyopencv_aruco_GridBoard_getseters;
    pyopencv_aruco_GridBoard_Type.tp_init = (initproc)0;
    pyopencv_aruco_GridBoard_Type.tp_methods = pyopencv_aruco_GridBoard_methods;
}

static PyObject* pyopencv_aruco_CharucoBoard_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<aruco_CharucoBoard %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_aruco_CharucoBoard_get_chessboardCorners(pyopencv_aruco_CharucoBoard_t* p, void *closure)
{
    return pyopencv_from(p->v->chessboardCorners);
}

static PyObject* pyopencv_aruco_CharucoBoard_get_nearestMarkerCorners(pyopencv_aruco_CharucoBoard_t* p, void *closure)
{
    return pyopencv_from(p->v->nearestMarkerCorners);
}

static PyObject* pyopencv_aruco_CharucoBoard_get_nearestMarkerIdx(pyopencv_aruco_CharucoBoard_t* p, void *closure)
{
    return pyopencv_from(p->v->nearestMarkerIdx);
}


static PyGetSetDef pyopencv_aruco_CharucoBoard_getseters[] =
{
    {(char*)"chessboardCorners", (getter)pyopencv_aruco_CharucoBoard_get_chessboardCorners, NULL, (char*)"chessboardCorners", NULL},
    {(char*)"nearestMarkerCorners", (getter)pyopencv_aruco_CharucoBoard_get_nearestMarkerCorners, NULL, (char*)"nearestMarkerCorners", NULL},
    {(char*)"nearestMarkerIdx", (getter)pyopencv_aruco_CharucoBoard_get_nearestMarkerIdx, NULL, (char*)"nearestMarkerIdx", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_aruco_aruco_CharucoBoard_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    int squaresX=0;
    int squaresY=0;
    float squareLength=0.f;
    float markerLength=0.f;
    PyObject* pyobj_dictionary = NULL;
    Ptr<Dictionary> dictionary;
    Ptr<CharucoBoard> retval;

    const char* keywords[] = { "squaresX", "squaresY", "squareLength", "markerLength", "dictionary", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiffO:aruco_CharucoBoard.create", (char**)keywords, &squaresX, &squaresY, &squareLength, &markerLength, &pyobj_dictionary) &&
        pyopencv_to(pyobj_dictionary, dictionary, ArgInfo("dictionary", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::CharucoBoard::create(squaresX, squaresY, squareLength, markerLength, dictionary));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_CharucoBoard_draw(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::CharucoBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_CharucoBoard_Type))
        _self_ = ((pyopencv_aruco_CharucoBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    {
    PyObject* pyobj_outSize = NULL;
    Size outSize;
    PyObject* pyobj_img = NULL;
    Mat img;
    int marginSize=0;
    int borderBits=1;

    const char* keywords[] = { "outSize", "img", "marginSize", "borderBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oii:aruco_CharucoBoard.draw", (char**)keywords, &pyobj_outSize, &pyobj_img, &marginSize, &borderBits) &&
        pyopencv_to(pyobj_outSize, outSize, ArgInfo("outSize", 0)) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 1)) )
    {
        ERRWRAP2(_self_->draw(outSize, img, marginSize, borderBits));
        return pyopencv_from(img);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_outSize = NULL;
    Size outSize;
    PyObject* pyobj_img = NULL;
    UMat img;
    int marginSize=0;
    int borderBits=1;

    const char* keywords[] = { "outSize", "img", "marginSize", "borderBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Oii:aruco_CharucoBoard.draw", (char**)keywords, &pyobj_outSize, &pyobj_img, &marginSize, &borderBits) &&
        pyopencv_to(pyobj_outSize, outSize, ArgInfo("outSize", 0)) &&
        pyopencv_to(pyobj_img, img, ArgInfo("img", 1)) )
    {
        ERRWRAP2(_self_->draw(outSize, img, marginSize, borderBits));
        return pyopencv_from(img);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_CharucoBoard_getChessboardSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::CharucoBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_CharucoBoard_Type))
        _self_ = ((pyopencv_aruco_CharucoBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    Size retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getChessboardSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_CharucoBoard_getMarkerLength(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::CharucoBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_CharucoBoard_Type))
        _self_ = ((pyopencv_aruco_CharucoBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMarkerLength());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_CharucoBoard_getSquareLength(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::CharucoBoard* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_CharucoBoard_Type))
        _self_ = ((pyopencv_aruco_CharucoBoard_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_CharucoBoard' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSquareLength());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_aruco_CharucoBoard_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_create_cls, METH_CLASS), "create(squaresX, squaresY, squareLength, markerLength, dictionary) -> retval\n.   * @brief Create a CharucoBoard object\n.   *\n.   * @param squaresX number of chessboard squares in X direction\n.   * @param squaresY number of chessboard squares in Y direction\n.   * @param squareLength chessboard square side length (normally in meters)\n.   * @param markerLength marker side length (same unit than squareLength)\n.   * @param dictionary dictionary of markers indicating the type of markers.\n.   * The first markers in the dictionary are used to fill the white chessboard squares.\n.   * @return the output CharucoBoard object\n.   *\n.   * This functions creates a CharucoBoard object given the number of squares in each direction\n.   * and the size of the markers and chessboard squares."},
    {"draw", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_draw, 0), "draw(outSize[, img[, marginSize[, borderBits]]]) -> img\n.   * @brief Draw a ChArUco board\n.   *\n.   * @param outSize size of the output image in pixels.\n.   * @param img output image with the board. The size of this image will be outSize\n.   * and the board will be on the center, keeping the board proportions.\n.   * @param marginSize minimum margins (in pixels) of the board in the output image\n.   * @param borderBits width of the marker borders.\n.   *\n.   * This function return the image of the ChArUco board, ready to be printed."},
    {"getChessboardSize", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getChessboardSize, 0), "getChessboardSize() -> retval\n.   *"},
    {"getMarkerLength", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getMarkerLength, 0), "getMarkerLength() -> retval\n.   *"},
    {"getSquareLength", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_CharucoBoard_getSquareLength, 0), "getSquareLength() -> retval\n.   *"},

    {NULL,          NULL}
};

static void pyopencv_aruco_CharucoBoard_specials(void)
{
    pyopencv_aruco_CharucoBoard_Type.tp_base = &pyopencv_aruco_Board_Type;
    pyopencv_aruco_CharucoBoard_Type.tp_dealloc = pyopencv_aruco_CharucoBoard_dealloc;
    pyopencv_aruco_CharucoBoard_Type.tp_repr = pyopencv_aruco_CharucoBoard_repr;
    pyopencv_aruco_CharucoBoard_Type.tp_getset = pyopencv_aruco_CharucoBoard_getseters;
    pyopencv_aruco_CharucoBoard_Type.tp_init = (initproc)0;
    pyopencv_aruco_CharucoBoard_Type.tp_methods = pyopencv_aruco_CharucoBoard_methods;
}

static PyObject* pyopencv_aruco_Dictionary_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<aruco_Dictionary %p>", self);
    return PyString_FromString(str);
}


static PyObject* pyopencv_aruco_Dictionary_get_bytesList(pyopencv_aruco_Dictionary_t* p, void *closure)
{
    return pyopencv_from(p->v->bytesList);
}

static int pyopencv_aruco_Dictionary_set_bytesList(pyopencv_aruco_Dictionary_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the bytesList attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->bytesList) ? 0 : -1;
}

static PyObject* pyopencv_aruco_Dictionary_get_markerSize(pyopencv_aruco_Dictionary_t* p, void *closure)
{
    return pyopencv_from(p->v->markerSize);
}

static int pyopencv_aruco_Dictionary_set_markerSize(pyopencv_aruco_Dictionary_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the markerSize attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->markerSize) ? 0 : -1;
}

static PyObject* pyopencv_aruco_Dictionary_get_maxCorrectionBits(pyopencv_aruco_Dictionary_t* p, void *closure)
{
    return pyopencv_from(p->v->maxCorrectionBits);
}

static int pyopencv_aruco_Dictionary_set_maxCorrectionBits(pyopencv_aruco_Dictionary_t* p, PyObject *value, void *closure)
{
    if (value == NULL)
    {
        PyErr_SetString(PyExc_TypeError, "Cannot delete the maxCorrectionBits attribute");
        return -1;
    }
    return pyopencv_to(value, p->v->maxCorrectionBits) ? 0 : -1;
}


static PyGetSetDef pyopencv_aruco_Dictionary_getseters[] =
{
    {(char*)"bytesList", (getter)pyopencv_aruco_Dictionary_get_bytesList, (setter)pyopencv_aruco_Dictionary_set_bytesList, (char*)"bytesList", NULL},
    {(char*)"markerSize", (getter)pyopencv_aruco_Dictionary_get_markerSize, (setter)pyopencv_aruco_Dictionary_set_markerSize, (char*)"markerSize", NULL},
    {(char*)"maxCorrectionBits", (getter)pyopencv_aruco_Dictionary_get_maxCorrectionBits, (setter)pyopencv_aruco_Dictionary_set_maxCorrectionBits, (char*)"maxCorrectionBits", NULL},
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_aruco_aruco_Dictionary_create_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    int nMarkers=0;
    int markerSize=0;
    int randomSeed=0;
    Ptr<Dictionary> retval;

    const char* keywords[] = { "nMarkers", "markerSize", "randomSeed", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii|i:aruco_Dictionary.create", (char**)keywords, &nMarkers, &markerSize, &randomSeed) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::create(nMarkers, markerSize, randomSeed));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_Dictionary_create_from_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    int nMarkers=0;
    int markerSize=0;
    PyObject* pyobj_baseDictionary = NULL;
    Ptr<Dictionary> baseDictionary;
    int randomSeed=0;
    Ptr<Dictionary> retval;

    const char* keywords[] = { "nMarkers", "markerSize", "baseDictionary", "randomSeed", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "iiO|i:aruco_Dictionary.create_from", (char**)keywords, &nMarkers, &markerSize, &pyobj_baseDictionary, &randomSeed) &&
        pyopencv_to(pyobj_baseDictionary, baseDictionary, ArgInfo("baseDictionary", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::create(nMarkers, markerSize, baseDictionary, randomSeed));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_Dictionary_drawMarker(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    cv::aruco::Dictionary* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_aruco_Dictionary_Type))
        _self_ = ((pyopencv_aruco_Dictionary_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'aruco_Dictionary' or its derivative)");
    {
    int id=0;
    int sidePixels=0;
    PyObject* pyobj__img = NULL;
    Mat _img;
    int borderBits=1;

    const char* keywords[] = { "id", "sidePixels", "_img", "borderBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii|Oi:aruco_Dictionary.drawMarker", (char**)keywords, &id, &sidePixels, &pyobj__img, &borderBits) &&
        pyopencv_to(pyobj__img, _img, ArgInfo("_img", 1)) )
    {
        ERRWRAP2(_self_->drawMarker(id, sidePixels, _img, borderBits));
        return pyopencv_from(_img);
    }
    }
    PyErr_Clear();

    {
    int id=0;
    int sidePixels=0;
    PyObject* pyobj__img = NULL;
    UMat _img;
    int borderBits=1;

    const char* keywords[] = { "id", "sidePixels", "_img", "borderBits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "ii|Oi:aruco_Dictionary.drawMarker", (char**)keywords, &id, &sidePixels, &pyobj__img, &borderBits) &&
        pyopencv_to(pyobj__img, _img, ArgInfo("_img", 1)) )
    {
        ERRWRAP2(_self_->drawMarker(id, sidePixels, _img, borderBits));
        return pyopencv_from(_img);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_Dictionary_get_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    int dict=0;
    Ptr<Dictionary> retval;

    const char* keywords[] = { "dict", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:aruco_Dictionary.get", (char**)keywords, &dict) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::get(dict));
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_Dictionary_getBitsFromByteList_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    {
    PyObject* pyobj_byteList = NULL;
    Mat byteList;
    int markerSize=0;
    Mat retval;

    const char* keywords[] = { "byteList", "markerSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:aruco_Dictionary.getBitsFromByteList", (char**)keywords, &pyobj_byteList, &markerSize) &&
        pyopencv_to(pyobj_byteList, byteList, ArgInfo("byteList", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::getBitsFromByteList(byteList, markerSize));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_byteList = NULL;
    Mat byteList;
    int markerSize=0;
    Mat retval;

    const char* keywords[] = { "byteList", "markerSize", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "Oi:aruco_Dictionary.getBitsFromByteList", (char**)keywords, &pyobj_byteList, &markerSize) &&
        pyopencv_to(pyobj_byteList, byteList, ArgInfo("byteList", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::getBitsFromByteList(byteList, markerSize));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_aruco_aruco_Dictionary_getByteListFromBits_cls(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::aruco;

    {
    PyObject* pyobj_bits = NULL;
    Mat bits;
    Mat retval;

    const char* keywords[] = { "bits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:aruco_Dictionary.getByteListFromBits", (char**)keywords, &pyobj_bits) &&
        pyopencv_to(pyobj_bits, bits, ArgInfo("bits", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::getByteListFromBits(bits));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_bits = NULL;
    Mat bits;
    Mat retval;

    const char* keywords[] = { "bits", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:aruco_Dictionary.getByteListFromBits", (char**)keywords, &pyobj_bits) &&
        pyopencv_to(pyobj_bits, bits, ArgInfo("bits", 0)) )
    {
        ERRWRAP2(retval = cv::aruco::Dictionary::getByteListFromBits(bits));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_aruco_Dictionary_methods[] =
{
    {"create", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_create_cls, METH_CLASS), "create(nMarkers, markerSize[, randomSeed]) -> retval\n.   * @see generateCustomDictionary"},
    {"create_from", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_create_from_cls, METH_CLASS), "create_from(nMarkers, markerSize, baseDictionary[, randomSeed]) -> retval\n.   * @see generateCustomDictionary"},
    {"drawMarker", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_drawMarker, 0), "drawMarker(id, sidePixels[, _img[, borderBits]]) -> _img\n.   * @brief Draw a canonical marker image"},
    {"get", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_get_cls, METH_CLASS), "get(dict) -> retval\n.   * @see getPredefinedDictionary"},
    {"getBitsFromByteList", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_getBitsFromByteList_cls, METH_CLASS), "getBitsFromByteList(byteList, markerSize) -> retval\n.   * @brief Transform list of bytes to matrix of bits"},
    {"getByteListFromBits", CV_PY_FN_WITH_KW_(pyopencv_cv_aruco_aruco_Dictionary_getByteListFromBits_cls, METH_CLASS), "getByteListFromBits(bits) -> retval\n.   * @brief Transform matrix of bits to list of bytes in the 4 rotations"},

    {NULL,          NULL}
};

static void pyopencv_aruco_Dictionary_specials(void)
{
    pyopencv_aruco_Dictionary_Type.tp_base = NULL;
    pyopencv_aruco_Dictionary_Type.tp_dealloc = pyopencv_aruco_Dictionary_dealloc;
    pyopencv_aruco_Dictionary_Type.tp_repr = pyopencv_aruco_Dictionary_repr;
    pyopencv_aruco_Dictionary_Type.tp_getset = pyopencv_aruco_Dictionary_getseters;
    pyopencv_aruco_Dictionary_Type.tp_init = (initproc)0;
    pyopencv_aruco_Dictionary_Type.tp_methods = pyopencv_aruco_Dictionary_methods;
}

static PyObject* pyopencv_bgsegm_BackgroundSubtractorMOG_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_BackgroundSubtractorMOG %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_BackgroundSubtractorMOG_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getBackgroundRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBackgroundRatio());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getHistory());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getNMixtures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNMixtures());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getNoiseSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNoiseSigma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setBackgroundRatio(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    double backgroundRatio=0;

    const char* keywords[] = { "backgroundRatio", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorMOG.setBackgroundRatio", (char**)keywords, &backgroundRatio) )
    {
        ERRWRAP2(_self_->setBackgroundRatio(backgroundRatio));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    int nframes=0;

    const char* keywords[] = { "nframes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorMOG.setHistory", (char**)keywords, &nframes) )
    {
        ERRWRAP2(_self_->setHistory(nframes));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setNMixtures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    int nmix=0;

    const char* keywords[] = { "nmix", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorMOG.setNMixtures", (char**)keywords, &nmix) )
    {
        ERRWRAP2(_self_->setNMixtures(nmix));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setNoiseSigma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorMOG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorMOG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorMOG*>(((pyopencv_bgsegm_BackgroundSubtractorMOG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorMOG' or its derivative)");
    double noiseSigma=0;

    const char* keywords[] = { "noiseSigma", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorMOG.setNoiseSigma", (char**)keywords, &noiseSigma) )
    {
        ERRWRAP2(_self_->setNoiseSigma(noiseSigma));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_bgsegm_BackgroundSubtractorMOG_methods[] =
{
    {"getBackgroundRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getBackgroundRatio, 0), "getBackgroundRatio() -> retval\n."},
    {"getHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getHistory, 0), "getHistory() -> retval\n."},
    {"getNMixtures", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getNMixtures, 0), "getNMixtures() -> retval\n."},
    {"getNoiseSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_getNoiseSigma, 0), "getNoiseSigma() -> retval\n."},
    {"setBackgroundRatio", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setBackgroundRatio, 0), "setBackgroundRatio(backgroundRatio) -> None\n."},
    {"setHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setHistory, 0), "setHistory(nframes) -> None\n."},
    {"setNMixtures", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setNMixtures, 0), "setNMixtures(nmix) -> None\n."},
    {"setNoiseSigma", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorMOG_setNoiseSigma, 0), "setNoiseSigma(noiseSigma) -> None\n."},

    {NULL,          NULL}
};

static void pyopencv_bgsegm_BackgroundSubtractorMOG_specials(void)
{
    pyopencv_bgsegm_BackgroundSubtractorMOG_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_bgsegm_BackgroundSubtractorMOG_Type.tp_dealloc = pyopencv_bgsegm_BackgroundSubtractorMOG_dealloc;
    pyopencv_bgsegm_BackgroundSubtractorMOG_Type.tp_repr = pyopencv_bgsegm_BackgroundSubtractorMOG_repr;
    pyopencv_bgsegm_BackgroundSubtractorMOG_Type.tp_getset = pyopencv_bgsegm_BackgroundSubtractorMOG_getseters;
    pyopencv_bgsegm_BackgroundSubtractorMOG_Type.tp_init = (initproc)0;
    pyopencv_bgsegm_BackgroundSubtractorMOG_Type.tp_methods = pyopencv_bgsegm_BackgroundSubtractorMOG_methods;
}

static PyObject* pyopencv_bgsegm_BackgroundSubtractorGMG_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_BackgroundSubtractorGMG %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_BackgroundSubtractorGMG_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getBackgroundPrior(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getBackgroundPrior());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getDecisionThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDecisionThreshold());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getDefaultLearningRate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDefaultLearningRate());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getMaxFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxFeatures());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getMaxVal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxVal());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getMinVal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinVal());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getNumFrames(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getNumFrames());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getQuantizationLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getQuantizationLevels());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getSmoothingRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSmoothingRadius());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getUpdateBackgroundModel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUpdateBackgroundModel());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setBackgroundPrior(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double bgprior=0;

    const char* keywords[] = { "bgprior", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorGMG.setBackgroundPrior", (char**)keywords, &bgprior) )
    {
        ERRWRAP2(_self_->setBackgroundPrior(bgprior));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setDecisionThreshold(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double thresh=0;

    const char* keywords[] = { "thresh", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorGMG.setDecisionThreshold", (char**)keywords, &thresh) )
    {
        ERRWRAP2(_self_->setDecisionThreshold(thresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setDefaultLearningRate(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double lr=0;

    const char* keywords[] = { "lr", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorGMG.setDefaultLearningRate", (char**)keywords, &lr) )
    {
        ERRWRAP2(_self_->setDefaultLearningRate(lr));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setMaxFeatures(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int maxFeatures=0;

    const char* keywords[] = { "maxFeatures", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorGMG.setMaxFeatures", (char**)keywords, &maxFeatures) )
    {
        ERRWRAP2(_self_->setMaxFeatures(maxFeatures));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setMaxVal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorGMG.setMaxVal", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMaxVal(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setMinVal(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    double val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:bgsegm_BackgroundSubtractorGMG.setMinVal", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setMinVal(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setNumFrames(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int nframes=0;

    const char* keywords[] = { "nframes", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorGMG.setNumFrames", (char**)keywords, &nframes) )
    {
        ERRWRAP2(_self_->setNumFrames(nframes));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setQuantizationLevels(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int nlevels=0;

    const char* keywords[] = { "nlevels", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorGMG.setQuantizationLevels", (char**)keywords, &nlevels) )
    {
        ERRWRAP2(_self_->setQuantizationLevels(nlevels));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setSmoothingRadius(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    int radius=0;

    const char* keywords[] = { "radius", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorGMG.setSmoothingRadius", (char**)keywords, &radius) )
    {
        ERRWRAP2(_self_->setSmoothingRadius(radius));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setUpdateBackgroundModel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGMG* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGMG_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGMG*>(((pyopencv_bgsegm_BackgroundSubtractorGMG_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGMG' or its derivative)");
    bool update=0;

    const char* keywords[] = { "update", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:bgsegm_BackgroundSubtractorGMG.setUpdateBackgroundModel", (char**)keywords, &update) )
    {
        ERRWRAP2(_self_->setUpdateBackgroundModel(update));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_bgsegm_BackgroundSubtractorGMG_methods[] =
{
    {"getBackgroundPrior", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getBackgroundPrior, 0), "getBackgroundPrior() -> retval\n.   @brief Returns the prior probability that each individual pixel is a background pixel."},
    {"getDecisionThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getDecisionThreshold, 0), "getDecisionThreshold() -> retval\n.   @brief Returns the value of decision threshold.\n.   \n.   Decision value is the value above which pixel is determined to be FG."},
    {"getDefaultLearningRate", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getDefaultLearningRate, 0), "getDefaultLearningRate() -> retval\n.   @brief Returns the learning rate of the algorithm.\n.   \n.   It lies between 0.0 and 1.0. It determines how quickly features are \"forgotten\" from\n.   histograms."},
    {"getMaxFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getMaxFeatures, 0), "getMaxFeatures() -> retval\n.   @brief Returns total number of distinct colors to maintain in histogram."},
    {"getMaxVal", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getMaxVal, 0), "getMaxVal() -> retval\n.   @brief Returns the maximum value taken on by pixels in image sequence. e.g. 1.0 or 255."},
    {"getMinVal", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getMinVal, 0), "getMinVal() -> retval\n.   @brief Returns the minimum value taken on by pixels in image sequence. Usually 0."},
    {"getNumFrames", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getNumFrames, 0), "getNumFrames() -> retval\n.   @brief Returns the number of frames used to initialize background model."},
    {"getQuantizationLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getQuantizationLevels, 0), "getQuantizationLevels() -> retval\n.   @brief Returns the parameter used for quantization of color-space.\n.   \n.   It is the number of discrete levels in each channel to be used in histograms."},
    {"getSmoothingRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getSmoothingRadius, 0), "getSmoothingRadius() -> retval\n.   @brief Returns the kernel radius used for morphological operations"},
    {"getUpdateBackgroundModel", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_getUpdateBackgroundModel, 0), "getUpdateBackgroundModel() -> retval\n.   @brief Returns the status of background model update"},
    {"setBackgroundPrior", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setBackgroundPrior, 0), "setBackgroundPrior(bgprior) -> None\n.   @brief Sets the prior probability that each individual pixel is a background pixel."},
    {"setDecisionThreshold", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setDecisionThreshold, 0), "setDecisionThreshold(thresh) -> None\n.   @brief Sets the value of decision threshold."},
    {"setDefaultLearningRate", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setDefaultLearningRate, 0), "setDefaultLearningRate(lr) -> None\n.   @brief Sets the learning rate of the algorithm."},
    {"setMaxFeatures", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setMaxFeatures, 0), "setMaxFeatures(maxFeatures) -> None\n.   @brief Sets total number of distinct colors to maintain in histogram."},
    {"setMaxVal", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setMaxVal, 0), "setMaxVal(val) -> None\n.   @brief Sets the maximum value taken on by pixels in image sequence."},
    {"setMinVal", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setMinVal, 0), "setMinVal(val) -> None\n.   @brief Sets the minimum value taken on by pixels in image sequence."},
    {"setNumFrames", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setNumFrames, 0), "setNumFrames(nframes) -> None\n.   @brief Sets the number of frames used to initialize background model."},
    {"setQuantizationLevels", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setQuantizationLevels, 0), "setQuantizationLevels(nlevels) -> None\n.   @brief Sets the parameter used for quantization of color-space"},
    {"setSmoothingRadius", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setSmoothingRadius, 0), "setSmoothingRadius(radius) -> None\n.   @brief Sets the kernel radius used for morphological operations"},
    {"setUpdateBackgroundModel", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGMG_setUpdateBackgroundModel, 0), "setUpdateBackgroundModel(update) -> None\n.   @brief Sets the status of background model update"},

    {NULL,          NULL}
};

static void pyopencv_bgsegm_BackgroundSubtractorGMG_specials(void)
{
    pyopencv_bgsegm_BackgroundSubtractorGMG_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_bgsegm_BackgroundSubtractorGMG_Type.tp_dealloc = pyopencv_bgsegm_BackgroundSubtractorGMG_dealloc;
    pyopencv_bgsegm_BackgroundSubtractorGMG_Type.tp_repr = pyopencv_bgsegm_BackgroundSubtractorGMG_repr;
    pyopencv_bgsegm_BackgroundSubtractorGMG_Type.tp_getset = pyopencv_bgsegm_BackgroundSubtractorGMG_getseters;
    pyopencv_bgsegm_BackgroundSubtractorGMG_Type.tp_init = (initproc)0;
    pyopencv_bgsegm_BackgroundSubtractorGMG_Type.tp_methods = pyopencv_bgsegm_BackgroundSubtractorGMG_methods;
}

static PyObject* pyopencv_bgsegm_BackgroundSubtractorCNT_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_BackgroundSubtractorCNT %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_BackgroundSubtractorCNT_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_apply(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_fgmask = NULL;
    Mat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:bgsegm_BackgroundSubtractorCNT.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_fgmask = NULL;
    UMat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:bgsegm_BackgroundSubtractorCNT.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getBackgroundImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    {
    PyObject* pyobj_backgroundImage = NULL;
    Mat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bgsegm_BackgroundSubtractorCNT.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_backgroundImage = NULL;
    UMat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bgsegm_BackgroundSubtractorCNT.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getIsParallel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getIsParallel());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getMaxPixelStability(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMaxPixelStability());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getMinPixelStability(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getMinPixelStability());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getUseHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseHistory());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setIsParallel(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    bool value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:bgsegm_BackgroundSubtractorCNT.setIsParallel", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setIsParallel(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setMaxPixelStability(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    int value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorCNT.setMaxPixelStability", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setMaxPixelStability(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setMinPixelStability(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    int value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:bgsegm_BackgroundSubtractorCNT.setMinPixelStability", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setMinPixelStability(value));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setUseHistory(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorCNT* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorCNT_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorCNT*>(((pyopencv_bgsegm_BackgroundSubtractorCNT_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorCNT' or its derivative)");
    bool value=0;

    const char* keywords[] = { "value", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:bgsegm_BackgroundSubtractorCNT.setUseHistory", (char**)keywords, &value) )
    {
        ERRWRAP2(_self_->setUseHistory(value));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_bgsegm_BackgroundSubtractorCNT_methods[] =
{
    {"apply", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n."},
    {"getBackgroundImage", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getBackgroundImage, 0), "getBackgroundImage([, backgroundImage]) -> backgroundImage\n."},
    {"getIsParallel", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getIsParallel, 0), "getIsParallel() -> retval\n.   @brief Returns if we're parallelizing the algorithm."},
    {"getMaxPixelStability", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getMaxPixelStability, 0), "getMaxPixelStability() -> retval\n.   @brief Returns maximum allowed credit for a pixel in history."},
    {"getMinPixelStability", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getMinPixelStability, 0), "getMinPixelStability() -> retval\n.   @brief Returns number of frames with same pixel color to consider stable."},
    {"getUseHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_getUseHistory, 0), "getUseHistory() -> retval\n.   @brief Returns if we're giving a pixel credit for being stable for a long time."},
    {"setIsParallel", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setIsParallel, 0), "setIsParallel(value) -> None\n.   @brief Sets if we're parallelizing the algorithm."},
    {"setMaxPixelStability", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setMaxPixelStability, 0), "setMaxPixelStability(value) -> None\n.   @brief Sets the maximum allowed credit for a pixel in history."},
    {"setMinPixelStability", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setMinPixelStability, 0), "setMinPixelStability(value) -> None\n.   @brief Sets the number of frames with same pixel color to consider stable."},
    {"setUseHistory", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorCNT_setUseHistory, 0), "setUseHistory(value) -> None\n.   @brief Sets if we're giving a pixel credit for being stable for a long time."},

    {NULL,          NULL}
};

static void pyopencv_bgsegm_BackgroundSubtractorCNT_specials(void)
{
    pyopencv_bgsegm_BackgroundSubtractorCNT_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_bgsegm_BackgroundSubtractorCNT_Type.tp_dealloc = pyopencv_bgsegm_BackgroundSubtractorCNT_dealloc;
    pyopencv_bgsegm_BackgroundSubtractorCNT_Type.tp_repr = pyopencv_bgsegm_BackgroundSubtractorCNT_repr;
    pyopencv_bgsegm_BackgroundSubtractorCNT_Type.tp_getset = pyopencv_bgsegm_BackgroundSubtractorCNT_getseters;
    pyopencv_bgsegm_BackgroundSubtractorCNT_Type.tp_init = (initproc)0;
    pyopencv_bgsegm_BackgroundSubtractorCNT_Type.tp_methods = pyopencv_bgsegm_BackgroundSubtractorCNT_methods;
}

static PyObject* pyopencv_bgsegm_BackgroundSubtractorGSOC_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_BackgroundSubtractorGSOC %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_BackgroundSubtractorGSOC_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGSOC_apply(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGSOC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGSOC_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGSOC*>(((pyopencv_bgsegm_BackgroundSubtractorGSOC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGSOC' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_fgmask = NULL;
    Mat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:bgsegm_BackgroundSubtractorGSOC.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_fgmask = NULL;
    UMat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:bgsegm_BackgroundSubtractorGSOC.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGSOC_getBackgroundImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorGSOC* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorGSOC_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorGSOC*>(((pyopencv_bgsegm_BackgroundSubtractorGSOC_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorGSOC' or its derivative)");
    {
    PyObject* pyobj_backgroundImage = NULL;
    Mat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bgsegm_BackgroundSubtractorGSOC.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_backgroundImage = NULL;
    UMat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bgsegm_BackgroundSubtractorGSOC.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_bgsegm_BackgroundSubtractorGSOC_methods[] =
{
    {"apply", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGSOC_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n."},
    {"getBackgroundImage", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorGSOC_getBackgroundImage, 0), "getBackgroundImage([, backgroundImage]) -> backgroundImage\n."},

    {NULL,          NULL}
};

static void pyopencv_bgsegm_BackgroundSubtractorGSOC_specials(void)
{
    pyopencv_bgsegm_BackgroundSubtractorGSOC_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_bgsegm_BackgroundSubtractorGSOC_Type.tp_dealloc = pyopencv_bgsegm_BackgroundSubtractorGSOC_dealloc;
    pyopencv_bgsegm_BackgroundSubtractorGSOC_Type.tp_repr = pyopencv_bgsegm_BackgroundSubtractorGSOC_repr;
    pyopencv_bgsegm_BackgroundSubtractorGSOC_Type.tp_getset = pyopencv_bgsegm_BackgroundSubtractorGSOC_getseters;
    pyopencv_bgsegm_BackgroundSubtractorGSOC_Type.tp_init = (initproc)0;
    pyopencv_bgsegm_BackgroundSubtractorGSOC_Type.tp_methods = pyopencv_bgsegm_BackgroundSubtractorGSOC_methods;
}

static PyObject* pyopencv_bgsegm_BackgroundSubtractorLSBP_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_BackgroundSubtractorLSBP %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_BackgroundSubtractorLSBP_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorLSBP_apply(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorLSBP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorLSBP_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorLSBP*>(((pyopencv_bgsegm_BackgroundSubtractorLSBP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorLSBP' or its derivative)");
    {
    PyObject* pyobj_image = NULL;
    Mat image;
    PyObject* pyobj_fgmask = NULL;
    Mat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:bgsegm_BackgroundSubtractorLSBP.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_image = NULL;
    UMat image;
    PyObject* pyobj_fgmask = NULL;
    UMat fgmask;
    double learningRate=-1;

    const char* keywords[] = { "image", "fgmask", "learningRate", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|Od:bgsegm_BackgroundSubtractorLSBP.apply", (char**)keywords, &pyobj_image, &pyobj_fgmask, &learningRate) &&
        pyopencv_to(pyobj_image, image, ArgInfo("image", 0)) &&
        pyopencv_to(pyobj_fgmask, fgmask, ArgInfo("fgmask", 1)) )
    {
        ERRWRAP2(_self_->apply(image, fgmask, learningRate));
        return pyopencv_from(fgmask);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorLSBP_getBackgroundImage(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::BackgroundSubtractorLSBP* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_BackgroundSubtractorLSBP_Type))
        _self_ = dynamic_cast<cv::bgsegm::BackgroundSubtractorLSBP*>(((pyopencv_bgsegm_BackgroundSubtractorLSBP_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_BackgroundSubtractorLSBP' or its derivative)");
    {
    PyObject* pyobj_backgroundImage = NULL;
    Mat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bgsegm_BackgroundSubtractorLSBP.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_backgroundImage = NULL;
    UMat backgroundImage;

    const char* keywords[] = { "backgroundImage", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:bgsegm_BackgroundSubtractorLSBP.getBackgroundImage", (char**)keywords, &pyobj_backgroundImage) &&
        pyopencv_to(pyobj_backgroundImage, backgroundImage, ArgInfo("backgroundImage", 1)) )
    {
        ERRWRAP2(_self_->getBackgroundImage(backgroundImage));
        return pyopencv_from(backgroundImage);
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_bgsegm_BackgroundSubtractorLSBP_methods[] =
{
    {"apply", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorLSBP_apply, 0), "apply(image[, fgmask[, learningRate]]) -> fgmask\n."},
    {"getBackgroundImage", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_BackgroundSubtractorLSBP_getBackgroundImage, 0), "getBackgroundImage([, backgroundImage]) -> backgroundImage\n."},

    {NULL,          NULL}
};

static void pyopencv_bgsegm_BackgroundSubtractorLSBP_specials(void)
{
    pyopencv_bgsegm_BackgroundSubtractorLSBP_Type.tp_base = &pyopencv_BackgroundSubtractor_Type;
    pyopencv_bgsegm_BackgroundSubtractorLSBP_Type.tp_dealloc = pyopencv_bgsegm_BackgroundSubtractorLSBP_dealloc;
    pyopencv_bgsegm_BackgroundSubtractorLSBP_Type.tp_repr = pyopencv_bgsegm_BackgroundSubtractorLSBP_repr;
    pyopencv_bgsegm_BackgroundSubtractorLSBP_Type.tp_getset = pyopencv_bgsegm_BackgroundSubtractorLSBP_getseters;
    pyopencv_bgsegm_BackgroundSubtractorLSBP_Type.tp_init = (initproc)0;
    pyopencv_bgsegm_BackgroundSubtractorLSBP_Type.tp_methods = pyopencv_bgsegm_BackgroundSubtractorLSBP_methods;
}

static PyObject* pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_BackgroundSubtractorLSBPDesc %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_specials(void)
{
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type.tp_base = NULL;
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type.tp_dealloc = pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_dealloc;
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type.tp_repr = pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_repr;
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type.tp_getset = pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_getseters;
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type.tp_init = (initproc)0;
    pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_Type.tp_methods = pyopencv_bgsegm_BackgroundSubtractorLSBPDesc_methods;
}

static PyObject* pyopencv_bgsegm_SyntheticSequenceGenerator_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<bgsegm_SyntheticSequenceGenerator %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_bgsegm_SyntheticSequenceGenerator_getseters[] =
{
    {NULL}  /* Sentinel */
};

static int pyopencv_cv_bgsegm_bgsegm_SyntheticSequenceGenerator_SyntheticSequenceGenerator(pyopencv_bgsegm_SyntheticSequenceGenerator_t* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    {
    PyObject* pyobj_background = NULL;
    Mat background;
    PyObject* pyobj_object = NULL;
    Mat object;
    double amplitude=0;
    double wavelength=0;
    double wavespeed=0;
    double objspeed=0;

    const char* keywords[] = { "background", "object", "amplitude", "wavelength", "wavespeed", "objspeed", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOdddd:SyntheticSequenceGenerator", (char**)keywords, &pyobj_background, &pyobj_object, &amplitude, &wavelength, &wavespeed, &objspeed) &&
        pyopencv_to(pyobj_background, background, ArgInfo("background", 0)) &&
        pyopencv_to(pyobj_object, object, ArgInfo("object", 0)) )
    {
        new (&(self->v)) Ptr<cv::bgsegm::SyntheticSequenceGenerator>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::bgsegm::SyntheticSequenceGenerator(background, object, amplitude, wavelength, wavespeed, objspeed)));
        return 0;
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_background = NULL;
    UMat background;
    PyObject* pyobj_object = NULL;
    UMat object;
    double amplitude=0;
    double wavelength=0;
    double wavespeed=0;
    double objspeed=0;

    const char* keywords[] = { "background", "object", "amplitude", "wavelength", "wavespeed", "objspeed", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOdddd:SyntheticSequenceGenerator", (char**)keywords, &pyobj_background, &pyobj_object, &amplitude, &wavelength, &wavespeed, &objspeed) &&
        pyopencv_to(pyobj_background, background, ArgInfo("background", 0)) &&
        pyopencv_to(pyobj_object, object, ArgInfo("object", 0)) )
    {
        new (&(self->v)) Ptr<cv::bgsegm::SyntheticSequenceGenerator>(); // init Ptr with placement new
        if(self) ERRWRAP2(self->v.reset(new cv::bgsegm::SyntheticSequenceGenerator(background, object, amplitude, wavelength, wavespeed, objspeed)));
        return 0;
    }
    }

    return -1;
}

static PyObject* pyopencv_cv_bgsegm_bgsegm_SyntheticSequenceGenerator_getNextFrame(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::bgsegm;

    cv::bgsegm::SyntheticSequenceGenerator* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_bgsegm_SyntheticSequenceGenerator_Type))
        _self_ = dynamic_cast<cv::bgsegm::SyntheticSequenceGenerator*>(((pyopencv_bgsegm_SyntheticSequenceGenerator_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'bgsegm_SyntheticSequenceGenerator' or its derivative)");
    {
    PyObject* pyobj_frame = NULL;
    Mat frame;
    PyObject* pyobj_gtMask = NULL;
    Mat gtMask;

    const char* keywords[] = { "frame", "gtMask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:bgsegm_SyntheticSequenceGenerator.getNextFrame", (char**)keywords, &pyobj_frame, &pyobj_gtMask) &&
        pyopencv_to(pyobj_frame, frame, ArgInfo("frame", 1)) &&
        pyopencv_to(pyobj_gtMask, gtMask, ArgInfo("gtMask", 1)) )
    {
        ERRWRAP2(_self_->getNextFrame(frame, gtMask));
        return Py_BuildValue("(NN)", pyopencv_from(frame), pyopencv_from(gtMask));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_frame = NULL;
    UMat frame;
    PyObject* pyobj_gtMask = NULL;
    UMat gtMask;

    const char* keywords[] = { "frame", "gtMask", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|OO:bgsegm_SyntheticSequenceGenerator.getNextFrame", (char**)keywords, &pyobj_frame, &pyobj_gtMask) &&
        pyopencv_to(pyobj_frame, frame, ArgInfo("frame", 1)) &&
        pyopencv_to(pyobj_gtMask, gtMask, ArgInfo("gtMask", 1)) )
    {
        ERRWRAP2(_self_->getNextFrame(frame, gtMask));
        return Py_BuildValue("(NN)", pyopencv_from(frame), pyopencv_from(gtMask));
    }
    }

    return NULL;
}



static PyMethodDef pyopencv_bgsegm_SyntheticSequenceGenerator_methods[] =
{
    {"getNextFrame", CV_PY_FN_WITH_KW_(pyopencv_cv_bgsegm_bgsegm_SyntheticSequenceGenerator_getNextFrame, 0), "getNextFrame([, frame[, gtMask]]) -> frame, gtMask\n.   @brief Obtain the next frame in the sequence.\n.   \n.   @param frame Output frame.\n.   @param gtMask Output ground-truth (reference) segmentation mask object/background."},

    {NULL,          NULL}
};

static void pyopencv_bgsegm_SyntheticSequenceGenerator_specials(void)
{
    pyopencv_bgsegm_SyntheticSequenceGenerator_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_bgsegm_SyntheticSequenceGenerator_Type.tp_dealloc = pyopencv_bgsegm_SyntheticSequenceGenerator_dealloc;
    pyopencv_bgsegm_SyntheticSequenceGenerator_Type.tp_repr = pyopencv_bgsegm_SyntheticSequenceGenerator_repr;
    pyopencv_bgsegm_SyntheticSequenceGenerator_Type.tp_getset = pyopencv_bgsegm_SyntheticSequenceGenerator_getseters;
    pyopencv_bgsegm_SyntheticSequenceGenerator_Type.tp_init = (initproc)pyopencv_cv_bgsegm_bgsegm_SyntheticSequenceGenerator_SyntheticSequenceGenerator;
    pyopencv_bgsegm_SyntheticSequenceGenerator_Type.tp_methods = pyopencv_bgsegm_SyntheticSequenceGenerator_methods;
}

static PyObject* pyopencv_optflow_VariationalRefinement_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_VariationalRefinement %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_VariationalRefinement_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_calcUV(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    {
    PyObject* pyobj_I0 = NULL;
    Mat I0;
    PyObject* pyobj_I1 = NULL;
    Mat I1;
    PyObject* pyobj_flow_u = NULL;
    Mat flow_u;
    PyObject* pyobj_flow_v = NULL;
    Mat flow_v;

    const char* keywords[] = { "I0", "I1", "flow_u", "flow_v", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:optflow_VariationalRefinement.calcUV", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow_u, &pyobj_flow_v) &&
        pyopencv_to(pyobj_I0, I0, ArgInfo("I0", 0)) &&
        pyopencv_to(pyobj_I1, I1, ArgInfo("I1", 0)) &&
        pyopencv_to(pyobj_flow_u, flow_u, ArgInfo("flow_u", 1)) &&
        pyopencv_to(pyobj_flow_v, flow_v, ArgInfo("flow_v", 1)) )
    {
        ERRWRAP2(_self_->calcUV(I0, I1, flow_u, flow_v));
        return Py_BuildValue("(NN)", pyopencv_from(flow_u), pyopencv_from(flow_v));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_I0 = NULL;
    UMat I0;
    PyObject* pyobj_I1 = NULL;
    UMat I1;
    PyObject* pyobj_flow_u = NULL;
    UMat flow_u;
    PyObject* pyobj_flow_v = NULL;
    UMat flow_v;

    const char* keywords[] = { "I0", "I1", "flow_u", "flow_v", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "OOOO:optflow_VariationalRefinement.calcUV", (char**)keywords, &pyobj_I0, &pyobj_I1, &pyobj_flow_u, &pyobj_flow_v) &&
        pyopencv_to(pyobj_I0, I0, ArgInfo("I0", 0)) &&
        pyopencv_to(pyobj_I1, I1, ArgInfo("I1", 0)) &&
        pyopencv_to(pyobj_flow_u, flow_u, ArgInfo("flow_u", 1)) &&
        pyopencv_to(pyobj_flow_v, flow_v, ArgInfo("flow_v", 1)) )
    {
        ERRWRAP2(_self_->calcUV(I0, I1, flow_u, flow_v));
        return Py_BuildValue("(NN)", pyopencv_from(flow_u), pyopencv_from(flow_v));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_getAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getAlpha());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_getDelta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getDelta());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_getFixedPointIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFixedPointIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_getGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGamma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_getOmega(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getOmega());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_getSorIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getSorIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_setAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_VariationalRefinement.setAlpha", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setAlpha(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_setDelta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_VariationalRefinement.setDelta", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setDelta(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_setFixedPointIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_VariationalRefinement.setFixedPointIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setFixedPointIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_setGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_VariationalRefinement.setGamma", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_setOmega(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_VariationalRefinement.setOmega", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setOmega(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_VariationalRefinement_setSorIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::VariationalRefinement* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_VariationalRefinement_Type))
        _self_ = dynamic_cast<cv::optflow::VariationalRefinement*>(((pyopencv_optflow_VariationalRefinement_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_VariationalRefinement' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_VariationalRefinement.setSorIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setSorIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_optflow_VariationalRefinement_methods[] =
{
    {"calcUV", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_calcUV, 0), "calcUV(I0, I1, flow_u, flow_v) -> flow_u, flow_v\n.   @brief @ref calc function overload to handle separate horizontal (u) and vertical (v) flow components\n.   (to avoid extra splits/merges)"},
    {"getAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_getAlpha, 0), "getAlpha() -> retval\n.   @brief Weight of the smoothness term\n.   @see setAlpha"},
    {"getDelta", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_getDelta, 0), "getDelta() -> retval\n.   @brief Weight of the color constancy term\n.   @see setDelta"},
    {"getFixedPointIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_getFixedPointIterations, 0), "getFixedPointIterations() -> retval\n.   @brief Number of outer (fixed-point) iterations in the minimization procedure.\n.   @see setFixedPointIterations"},
    {"getGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_getGamma, 0), "getGamma() -> retval\n.   @brief Weight of the gradient constancy term\n.   @see setGamma"},
    {"getOmega", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_getOmega, 0), "getOmega() -> retval\n.   @brief Relaxation factor in SOR\n.   @see setOmega"},
    {"getSorIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_getSorIterations, 0), "getSorIterations() -> retval\n.   @brief Number of inner successive over-relaxation (SOR) iterations\n.   in the minimization procedure to solve the respective linear system.\n.   @see setSorIterations"},
    {"setAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_setAlpha, 0), "setAlpha(val) -> None\n.   @copybrief getAlpha @see getAlpha"},
    {"setDelta", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_setDelta, 0), "setDelta(val) -> None\n.   @copybrief getDelta @see getDelta"},
    {"setFixedPointIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_setFixedPointIterations, 0), "setFixedPointIterations(val) -> None\n.   @copybrief getFixedPointIterations @see getFixedPointIterations"},
    {"setGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_setGamma, 0), "setGamma(val) -> None\n.   @copybrief getGamma @see getGamma"},
    {"setOmega", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_setOmega, 0), "setOmega(val) -> None\n.   @copybrief getOmega @see getOmega"},
    {"setSorIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_VariationalRefinement_setSorIterations, 0), "setSorIterations(val) -> None\n.   @copybrief getSorIterations @see getSorIterations"},

    {NULL,          NULL}
};

static void pyopencv_optflow_VariationalRefinement_specials(void)
{
    pyopencv_optflow_VariationalRefinement_Type.tp_base = &pyopencv_DenseOpticalFlow_Type;
    pyopencv_optflow_VariationalRefinement_Type.tp_dealloc = pyopencv_optflow_VariationalRefinement_dealloc;
    pyopencv_optflow_VariationalRefinement_Type.tp_repr = pyopencv_optflow_VariationalRefinement_repr;
    pyopencv_optflow_VariationalRefinement_Type.tp_getset = pyopencv_optflow_VariationalRefinement_getseters;
    pyopencv_optflow_VariationalRefinement_Type.tp_init = (initproc)0;
    pyopencv_optflow_VariationalRefinement_Type.tp_methods = pyopencv_optflow_VariationalRefinement_methods;
}

static PyObject* pyopencv_optflow_DISOpticalFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_DISOpticalFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_DISOpticalFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getFinestScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getFinestScale());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getGradientDescentIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getGradientDescentIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getPatchSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPatchSize());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getPatchStride(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getPatchStride());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getUseMeanNormalization(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseMeanNormalization());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getUseSpatialPropagation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getUseSpatialPropagation());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVariationalRefinementAlpha());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementDelta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVariationalRefinementDelta());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    float retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVariationalRefinementGamma());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->getVariationalRefinementIterations());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setFinestScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_DISOpticalFlow.setFinestScale", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setFinestScale(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setGradientDescentIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_DISOpticalFlow.setGradientDescentIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setGradientDescentIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setPatchSize(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_DISOpticalFlow.setPatchSize", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setPatchSize(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setPatchStride(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_DISOpticalFlow.setPatchStride", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setPatchStride(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setUseMeanNormalization(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:optflow_DISOpticalFlow.setUseMeanNormalization", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setUseMeanNormalization(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setUseSpatialPropagation(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    bool val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:optflow_DISOpticalFlow.setUseSpatialPropagation", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setUseSpatialPropagation(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementAlpha(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_DISOpticalFlow.setVariationalRefinementAlpha", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setVariationalRefinementAlpha(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementDelta(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_DISOpticalFlow.setVariationalRefinementDelta", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setVariationalRefinementDelta(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementGamma(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    float val=0.f;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "f:optflow_DISOpticalFlow.setVariationalRefinementGamma", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setVariationalRefinementGamma(val));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementIterations(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv::optflow;

    cv::optflow::DISOpticalFlow* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_optflow_DISOpticalFlow_Type))
        _self_ = dynamic_cast<cv::optflow::DISOpticalFlow*>(((pyopencv_optflow_DISOpticalFlow_t*)self)->v.get());
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'optflow_DISOpticalFlow' or its derivative)");
    int val=0;

    const char* keywords[] = { "val", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "i:optflow_DISOpticalFlow.setVariationalRefinementIterations", (char**)keywords, &val) )
    {
        ERRWRAP2(_self_->setVariationalRefinementIterations(val));
        Py_RETURN_NONE;
    }

    return NULL;
}



static PyMethodDef pyopencv_optflow_DISOpticalFlow_methods[] =
{
    {"getFinestScale", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getFinestScale, 0), "getFinestScale() -> retval\n.   @brief Finest level of the Gaussian pyramid on which the flow is computed (zero level\n.   corresponds to the original image resolution). The final flow is obtained by bilinear upscaling.\n.   @see setFinestScale"},
    {"getGradientDescentIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getGradientDescentIterations, 0), "getGradientDescentIterations() -> retval\n.   @brief Maximum number of gradient descent iterations in the patch inverse search stage. Higher values\n.   may improve quality in some cases.\n.   @see setGradientDescentIterations"},
    {"getPatchSize", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getPatchSize, 0), "getPatchSize() -> retval\n.   @brief Size of an image patch for matching (in pixels). Normally, default 8x8 patches work well\n.   enough in most cases.\n.   @see setPatchSize"},
    {"getPatchStride", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getPatchStride, 0), "getPatchStride() -> retval\n.   @brief Stride between neighbor patches. Must be less than patch size. Lower values correspond\n.   to higher flow quality.\n.   @see setPatchStride"},
    {"getUseMeanNormalization", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getUseMeanNormalization, 0), "getUseMeanNormalization() -> retval\n.   @brief Whether to use mean-normalization of patches when computing patch distance. It is turned on\n.   by default as it typically provides a noticeable quality boost because of increased robustness to\n.   illumination variations. Turn it off if you are certain that your sequence doesn't contain any changes\n.   in illumination.\n.   @see setUseMeanNormalization"},
    {"getUseSpatialPropagation", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getUseSpatialPropagation, 0), "getUseSpatialPropagation() -> retval\n.   @brief Whether to use spatial propagation of good optical flow vectors. This option is turned on by\n.   default, as it tends to work better on average and can sometimes help recover from major errors\n.   introduced by the coarse-to-fine scheme employed by the DIS optical flow algorithm. Turning this\n.   option off can make the output flow field a bit smoother, however.\n.   @see setUseSpatialPropagation"},
    {"getVariationalRefinementAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementAlpha, 0), "getVariationalRefinementAlpha() -> retval\n.   @brief Weight of the smoothness term\n.   @see setVariationalRefinementAlpha"},
    {"getVariationalRefinementDelta", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementDelta, 0), "getVariationalRefinementDelta() -> retval\n.   @brief Weight of the color constancy term\n.   @see setVariationalRefinementDelta"},
    {"getVariationalRefinementGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementGamma, 0), "getVariationalRefinementGamma() -> retval\n.   @brief Weight of the gradient constancy term\n.   @see setVariationalRefinementGamma"},
    {"getVariationalRefinementIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_getVariationalRefinementIterations, 0), "getVariationalRefinementIterations() -> retval\n.   @brief Number of fixed point iterations of variational refinement per scale. Set to zero to\n.   disable variational refinement completely. Higher values will typically result in more smooth and\n.   high-quality flow.\n.   @see setGradientDescentIterations"},
    {"setFinestScale", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setFinestScale, 0), "setFinestScale(val) -> None\n.   @copybrief getFinestScale @see getFinestScale"},
    {"setGradientDescentIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setGradientDescentIterations, 0), "setGradientDescentIterations(val) -> None\n.   @copybrief getGradientDescentIterations @see getGradientDescentIterations"},
    {"setPatchSize", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setPatchSize, 0), "setPatchSize(val) -> None\n.   @copybrief getPatchSize @see getPatchSize"},
    {"setPatchStride", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setPatchStride, 0), "setPatchStride(val) -> None\n.   @copybrief getPatchStride @see getPatchStride"},
    {"setUseMeanNormalization", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setUseMeanNormalization, 0), "setUseMeanNormalization(val) -> None\n.   @copybrief getUseMeanNormalization @see getUseMeanNormalization"},
    {"setUseSpatialPropagation", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setUseSpatialPropagation, 0), "setUseSpatialPropagation(val) -> None\n.   @copybrief getUseSpatialPropagation @see getUseSpatialPropagation"},
    {"setVariationalRefinementAlpha", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementAlpha, 0), "setVariationalRefinementAlpha(val) -> None\n.   @copybrief getVariationalRefinementAlpha @see getVariationalRefinementAlpha"},
    {"setVariationalRefinementDelta", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementDelta, 0), "setVariationalRefinementDelta(val) -> None\n.   @copybrief getVariationalRefinementDelta @see getVariationalRefinementDelta"},
    {"setVariationalRefinementGamma", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementGamma, 0), "setVariationalRefinementGamma(val) -> None\n.   @copybrief getVariationalRefinementGamma @see getVariationalRefinementGamma"},
    {"setVariationalRefinementIterations", CV_PY_FN_WITH_KW_(pyopencv_cv_optflow_optflow_DISOpticalFlow_setVariationalRefinementIterations, 0), "setVariationalRefinementIterations(val) -> None\n.   @copybrief getGradientDescentIterations @see getGradientDescentIterations"},

    {NULL,          NULL}
};

static void pyopencv_optflow_DISOpticalFlow_specials(void)
{
    pyopencv_optflow_DISOpticalFlow_Type.tp_base = &pyopencv_DenseOpticalFlow_Type;
    pyopencv_optflow_DISOpticalFlow_Type.tp_dealloc = pyopencv_optflow_DISOpticalFlow_dealloc;
    pyopencv_optflow_DISOpticalFlow_Type.tp_repr = pyopencv_optflow_DISOpticalFlow_repr;
    pyopencv_optflow_DISOpticalFlow_Type.tp_getset = pyopencv_optflow_DISOpticalFlow_getseters;
    pyopencv_optflow_DISOpticalFlow_Type.tp_init = (initproc)0;
    pyopencv_optflow_DISOpticalFlow_Type.tp_methods = pyopencv_optflow_DISOpticalFlow_methods;
}

static PyObject* pyopencv_optflow_PCAPrior_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_PCAPrior %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_PCAPrior_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_PCAPrior_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_PCAPrior_specials(void)
{
    pyopencv_optflow_PCAPrior_Type.tp_base = NULL;
    pyopencv_optflow_PCAPrior_Type.tp_dealloc = pyopencv_optflow_PCAPrior_dealloc;
    pyopencv_optflow_PCAPrior_Type.tp_repr = pyopencv_optflow_PCAPrior_repr;
    pyopencv_optflow_PCAPrior_Type.tp_getset = pyopencv_optflow_PCAPrior_getseters;
    pyopencv_optflow_PCAPrior_Type.tp_init = (initproc)0;
    pyopencv_optflow_PCAPrior_Type.tp_methods = pyopencv_optflow_PCAPrior_methods;
}

static PyObject* pyopencv_optflow_OpticalFlowPCAFlow_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_OpticalFlowPCAFlow %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_OpticalFlowPCAFlow_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_OpticalFlowPCAFlow_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_OpticalFlowPCAFlow_specials(void)
{
    pyopencv_optflow_OpticalFlowPCAFlow_Type.tp_base = &pyopencv_DenseOpticalFlow_Type;
    pyopencv_optflow_OpticalFlowPCAFlow_Type.tp_dealloc = pyopencv_optflow_OpticalFlowPCAFlow_dealloc;
    pyopencv_optflow_OpticalFlowPCAFlow_Type.tp_repr = pyopencv_optflow_OpticalFlowPCAFlow_repr;
    pyopencv_optflow_OpticalFlowPCAFlow_Type.tp_getset = pyopencv_optflow_OpticalFlowPCAFlow_getseters;
    pyopencv_optflow_OpticalFlowPCAFlow_Type.tp_init = (initproc)0;
    pyopencv_optflow_OpticalFlowPCAFlow_Type.tp_methods = pyopencv_optflow_OpticalFlowPCAFlow_methods;
}

static PyObject* pyopencv_optflow_GPCPatchDescriptor_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_GPCPatchDescriptor %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_GPCPatchDescriptor_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_GPCPatchDescriptor_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_GPCPatchDescriptor_specials(void)
{
    pyopencv_optflow_GPCPatchDescriptor_Type.tp_base = NULL;
    pyopencv_optflow_GPCPatchDescriptor_Type.tp_dealloc = pyopencv_optflow_GPCPatchDescriptor_dealloc;
    pyopencv_optflow_GPCPatchDescriptor_Type.tp_repr = pyopencv_optflow_GPCPatchDescriptor_repr;
    pyopencv_optflow_GPCPatchDescriptor_Type.tp_getset = pyopencv_optflow_GPCPatchDescriptor_getseters;
    pyopencv_optflow_GPCPatchDescriptor_Type.tp_init = (initproc)0;
    pyopencv_optflow_GPCPatchDescriptor_Type.tp_methods = pyopencv_optflow_GPCPatchDescriptor_methods;
}

static PyObject* pyopencv_optflow_GPCPatchSample_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_GPCPatchSample %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_GPCPatchSample_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_GPCPatchSample_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_GPCPatchSample_specials(void)
{
    pyopencv_optflow_GPCPatchSample_Type.tp_base = NULL;
    pyopencv_optflow_GPCPatchSample_Type.tp_dealloc = pyopencv_optflow_GPCPatchSample_dealloc;
    pyopencv_optflow_GPCPatchSample_Type.tp_repr = pyopencv_optflow_GPCPatchSample_repr;
    pyopencv_optflow_GPCPatchSample_Type.tp_getset = pyopencv_optflow_GPCPatchSample_getseters;
    pyopencv_optflow_GPCPatchSample_Type.tp_init = (initproc)0;
    pyopencv_optflow_GPCPatchSample_Type.tp_methods = pyopencv_optflow_GPCPatchSample_methods;
}

static PyObject* pyopencv_optflow_GPCTrainingSamples_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_GPCTrainingSamples %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_GPCTrainingSamples_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_GPCTrainingSamples_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_GPCTrainingSamples_specials(void)
{
    pyopencv_optflow_GPCTrainingSamples_Type.tp_base = NULL;
    pyopencv_optflow_GPCTrainingSamples_Type.tp_dealloc = pyopencv_optflow_GPCTrainingSamples_dealloc;
    pyopencv_optflow_GPCTrainingSamples_Type.tp_repr = pyopencv_optflow_GPCTrainingSamples_repr;
    pyopencv_optflow_GPCTrainingSamples_Type.tp_getset = pyopencv_optflow_GPCTrainingSamples_getseters;
    pyopencv_optflow_GPCTrainingSamples_Type.tp_init = (initproc)0;
    pyopencv_optflow_GPCTrainingSamples_Type.tp_methods = pyopencv_optflow_GPCTrainingSamples_methods;
}

static PyObject* pyopencv_optflow_GPCTree_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_GPCTree %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_GPCTree_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_GPCTree_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_GPCTree_specials(void)
{
    pyopencv_optflow_GPCTree_Type.tp_base = &pyopencv_Algorithm_Type;
    pyopencv_optflow_GPCTree_Type.tp_dealloc = pyopencv_optflow_GPCTree_dealloc;
    pyopencv_optflow_GPCTree_Type.tp_repr = pyopencv_optflow_GPCTree_repr;
    pyopencv_optflow_GPCTree_Type.tp_getset = pyopencv_optflow_GPCTree_getseters;
    pyopencv_optflow_GPCTree_Type.tp_init = (initproc)0;
    pyopencv_optflow_GPCTree_Type.tp_methods = pyopencv_optflow_GPCTree_methods;
}

static PyObject* pyopencv_optflow_GPCDetails_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<optflow_GPCDetails %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_optflow_GPCDetails_getseters[] =
{
    {NULL}  /* Sentinel */
};



static PyMethodDef pyopencv_optflow_GPCDetails_methods[] =
{

    {NULL,          NULL}
};

static void pyopencv_optflow_GPCDetails_specials(void)
{
    pyopencv_optflow_GPCDetails_Type.tp_base = NULL;
    pyopencv_optflow_GPCDetails_Type.tp_dealloc = pyopencv_optflow_GPCDetails_dealloc;
    pyopencv_optflow_GPCDetails_Type.tp_repr = pyopencv_optflow_GPCDetails_repr;
    pyopencv_optflow_GPCDetails_Type.tp_getset = pyopencv_optflow_GPCDetails_getseters;
    pyopencv_optflow_GPCDetails_Type.tp_init = (initproc)0;
    pyopencv_optflow_GPCDetails_Type.tp_methods = pyopencv_optflow_GPCDetails_methods;
}

static PyObject* pyopencv_Stitcher_repr(PyObject* self)
{
    char str[1000];
    sprintf(str, "<Stitcher %p>", self);
    return PyString_FromString(str);
}



static PyGetSetDef pyopencv_Stitcher_getseters[] =
{
    {NULL}  /* Sentinel */
};

static PyObject* pyopencv_cv_Stitcher_composePanorama(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    {
    PyObject* pyobj_pano = NULL;
    Mat pano;
    Status retval;

    const char* keywords[] = { "pano", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:Stitcher.composePanorama", (char**)keywords, &pyobj_pano) &&
        pyopencv_to(pyobj_pano, pano, ArgInfo("pano", 1)) )
    {
        ERRWRAP2(retval = _self_->composePanorama(pano));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(pano));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_pano = NULL;
    UMat pano;
    Status retval;

    const char* keywords[] = { "pano", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "|O:Stitcher.composePanorama", (char**)keywords, &pyobj_pano) &&
        pyopencv_to(pyobj_pano, pano, ArgInfo("pano", 1)) )
    {
        ERRWRAP2(retval = _self_->composePanorama(pano));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(pano));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_compositingResol(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->compositingResol());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_estimateTransform(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    Status retval;

    const char* keywords[] = { "images", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Stitcher.estimateTransform", (char**)keywords, &pyobj_images) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) )
    {
        ERRWRAP2(retval = _self_->estimateTransform(images));
        return pyopencv_from(retval);
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    Status retval;

    const char* keywords[] = { "images", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O:Stitcher.estimateTransform", (char**)keywords, &pyobj_images) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) )
    {
        ERRWRAP2(retval = _self_->estimateTransform(images));
        return pyopencv_from(retval);
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_panoConfidenceThresh(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->panoConfidenceThresh());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_registrationResol(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->registrationResol());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_seamEstimationResol(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->seamEstimationResol());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_setCompositingResol(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double resol_mpx=0;

    const char* keywords[] = { "resol_mpx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:Stitcher.setCompositingResol", (char**)keywords, &resol_mpx) )
    {
        ERRWRAP2(_self_->setCompositingResol(resol_mpx));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_setPanoConfidenceThresh(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double conf_thresh=0;

    const char* keywords[] = { "conf_thresh", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:Stitcher.setPanoConfidenceThresh", (char**)keywords, &conf_thresh) )
    {
        ERRWRAP2(_self_->setPanoConfidenceThresh(conf_thresh));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_setRegistrationResol(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double resol_mpx=0;

    const char* keywords[] = { "resol_mpx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:Stitcher.setRegistrationResol", (char**)keywords, &resol_mpx) )
    {
        ERRWRAP2(_self_->setRegistrationResol(resol_mpx));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_setSeamEstimationResol(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double resol_mpx=0;

    const char* keywords[] = { "resol_mpx", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "d:Stitcher.setSeamEstimationResol", (char**)keywords, &resol_mpx) )
    {
        ERRWRAP2(_self_->setSeamEstimationResol(resol_mpx));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_setWaveCorrection(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    bool flag=0;

    const char* keywords[] = { "flag", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "b:Stitcher.setWaveCorrection", (char**)keywords, &flag) )
    {
        ERRWRAP2(_self_->setWaveCorrection(flag));
        Py_RETURN_NONE;
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_stitch(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_pano = NULL;
    Mat pano;
    Status retval;

    const char* keywords[] = { "images", "pano", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Stitcher.stitch", (char**)keywords, &pyobj_images, &pyobj_pano) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_pano, pano, ArgInfo("pano", 1)) )
    {
        ERRWRAP2(retval = _self_->stitch(images, pano));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(pano));
    }
    }
    PyErr_Clear();

    {
    PyObject* pyobj_images = NULL;
    vector_Mat images;
    PyObject* pyobj_pano = NULL;
    UMat pano;
    Status retval;

    const char* keywords[] = { "images", "pano", NULL };
    if( PyArg_ParseTupleAndKeywords(args, kw, "O|O:Stitcher.stitch", (char**)keywords, &pyobj_images, &pyobj_pano) &&
        pyopencv_to(pyobj_images, images, ArgInfo("images", 0)) &&
        pyopencv_to(pyobj_pano, pano, ArgInfo("pano", 1)) )
    {
        ERRWRAP2(retval = _self_->stitch(images, pano));
        return Py_BuildValue("(NN)", pyopencv_from(retval), pyopencv_from(pano));
    }
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_waveCorrection(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    bool retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->waveCorrection());
        return pyopencv_from(retval);
    }

    return NULL;
}

static PyObject* pyopencv_cv_Stitcher_workScale(PyObject* self, PyObject* args, PyObject* kw)
{
    using namespace cv;

    cv::Stitcher* _self_ = NULL;
    if(PyObject_TypeCheck(self, &pyopencv_Stitcher_Type))
        _self_ = ((pyopencv_Stitcher_t*)self)->v.get();
    if (_self_ == NULL)
        return failmsgp("Incorrect type of self (must be 'Stitcher' or its derivative)");
    double retval;

    if(PyObject_Size(args) == 0 && (kw == NULL || PyObject_Size(kw) == 0))
    {
        ERRWRAP2(retval = _self_->workScale());
        return pyopencv_from(retval);
    }

    return NULL;
}



static PyMethodDef pyopencv_Stitcher_methods[] =
{
    {"composePanorama", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_composePanorama, 0), "composePanorama([, pano]) -> retval, pano\n.   @overload"},
    {"compositingResol", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_compositingResol, 0), "compositingResol() -> retval\n."},
    {"estimateTransform", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_estimateTransform, 0), "estimateTransform(images) -> retval\n.   @overload"},
    {"panoConfidenceThresh", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_panoConfidenceThresh, 0), "panoConfidenceThresh() -> retval\n."},
    {"registrationResol", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_registrationResol, 0), "registrationResol() -> retval\n.   @brief Creates a Stitcher configured in one of the stitching modes.\n.   \n.   @param mode Scenario for stitcher operation. This is usually determined by source of images\n.   to stitch and their transformation. Default parameters will be chosen for operation in given\n.   scenario.\n.   @param try_use_gpu Flag indicating whether GPU should be used whenever it's possible.\n.   @return Stitcher class instance."},
    {"seamEstimationResol", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_seamEstimationResol, 0), "seamEstimationResol() -> retval\n."},
    {"setCompositingResol", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_setCompositingResol, 0), "setCompositingResol(resol_mpx) -> None\n."},
    {"setPanoConfidenceThresh", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_setPanoConfidenceThresh, 0), "setPanoConfidenceThresh(conf_thresh) -> None\n."},
    {"setRegistrationResol", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_setRegistrationResol, 0), "setRegistrationResol(resol_mpx) -> None\n."},
    {"setSeamEstimationResol", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_setSeamEstimationResol, 0), "setSeamEstimationResol(resol_mpx) -> None\n."},
    {"setWaveCorrection", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_setWaveCorrection, 0), "setWaveCorrection(flag) -> None\n."},
    {"stitch", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_stitch, 0), "stitch(images[, pano]) -> retval, pano\n.   @overload"},
    {"waveCorrection", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_waveCorrection, 0), "waveCorrection() -> retval\n."},
    {"workScale", CV_PY_FN_WITH_KW_(pyopencv_cv_Stitcher_workScale, 0), "workScale() -> retval\n.   @brief These functions try to stitch the given images.\n.   \n.   @param images Input images.\n.   @param rois Region of interest rectangles.\n.   @param pano Final pano.\n.   @return Status code."},

    {NULL,          NULL}
};

static void pyopencv_Stitcher_specials(void)
{
    pyopencv_Stitcher_Type.tp_base = NULL;
    pyopencv_Stitcher_Type.tp_dealloc = pyopencv_Stitcher_dealloc;
    pyopencv_Stitcher_Type.tp_repr = pyopencv_Stitcher_repr;
    pyopencv_Stitcher_Type.tp_getset = pyopencv_Stitcher_getseters;
    pyopencv_Stitcher_Type.tp_init = (initproc)0;
    pyopencv_Stitcher_Type.tp_methods = pyopencv_Stitcher_methods;
}
